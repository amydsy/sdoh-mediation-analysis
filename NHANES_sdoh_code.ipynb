{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6321aae-088c-49a4-aa8b-0d59f8b12f00",
   "metadata": {},
   "source": [
    "<h1>Data prep and merge</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c2fcda",
   "metadata": {},
   "source": [
    "<h2>Install required package/libraries and directory</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efe6e728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyreadstat in /Users/dengshuyue/amydsy/lib/python3.13/site-packages (1.3.0)\n",
      "Requirement already satisfied: pandas>=1.2.0 in /Users/dengshuyue/amydsy/lib/python3.13/site-packages (from pyreadstat) (2.3.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Users/dengshuyue/amydsy/lib/python3.13/site-packages (from pandas>=1.2.0->pyreadstat) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/dengshuyue/amydsy/lib/python3.13/site-packages (from pandas>=1.2.0->pyreadstat) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/dengshuyue/amydsy/lib/python3.13/site-packages (from pandas>=1.2.0->pyreadstat) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/dengshuyue/amydsy/lib/python3.13/site-packages (from pandas>=1.2.0->pyreadstat) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/dengshuyue/amydsy/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->pyreadstat) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: openpyxl in /Users/dengshuyue/amydsy/lib/python3.13/site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in /Users/dengshuyue/amydsy/lib/python3.13/site-packages (from openpyxl) (2.0.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# If not already installed, install pyreadstat (used to read .sas7bdat files)\n",
    "!pip install pyreadstat\n",
    "!pip install openpyxl\n",
    "\n",
    "# Import core packages\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import pyreadstat\n",
    "from scipy.stats import sem\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "#path \n",
    "output_path = \"/Users/dengshuyue/Desktop/SDOH/analysis/output/demo_summary.csv\"\n",
    "data_path = \"/Users/dengshuyue/Desktop/SDOH/analysis/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0095018c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<h2>Load and preview SAS dataset</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16e9a8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SEQN', 'SDDSRVYR', 'RIDSTATR', 'RIAGENDR', 'RIDAGEYR', 'RIDAGEMN', 'RIDRETH1', 'RIDRETH3', 'RIDEXMON', 'RIDEXAGM', 'DMQMILIZ', 'DMQADFC', 'DMDBORN4', 'DMDCITZN', 'DMDYRSUS', 'DMDEDUC3', 'DMDEDUC2', 'DMDMARTL', 'RIDEXPRG', 'SIALANG', 'SIAPROXY', 'SIAINTRP', 'FIALANG', 'FIAPROXY', 'FIAINTRP', 'MIALANG', 'MIAPROXY', 'MIAINTRP', 'AIALANGA', 'DMDHHSIZ', 'DMDFMSIZ', 'DMDHHSZA', 'DMDHHSZB', 'DMDHHSZE', 'DMDHRGND', 'DMDHRAGE', 'DMDHRBR4', 'DMDHREDU', 'DMDHRMAR', 'DMDHSEDU', 'WTINT2YR', 'WTMEC2YR', 'SDMVPSU', 'SDMVSTRA', 'INDHHIN2', 'INDFMIN2', 'INDFMPIR']\n"
     ]
    }
   ],
   "source": [
    "# Path to your .sas7bdat file\n",
    "file_path = \"/Users/dengshuyue/Desktop/SDOH/analysis/data/less_important/demo_i.sas7bdat\"\n",
    "\n",
    "# Load the dataset using pandas (requires pyreadstat)\n",
    "df = pd.read_sas(file_path, format=\"sas7bdat\")\n",
    "\n",
    "# Preview the data\n",
    "pd.set_option('display.max_columns', None)\n",
    "df.head()\n",
    "\n",
    "# List all column names\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4371d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SEQN', 'DAYS', 'DR12IFDC', 'WTDRD1', 'DR12DRST', 'SDDSRVYR', 'RIAGENDR', 'RIDAGEYR', 'RIDRETH1', 'DMDEDUC3', 'DMDEDUC2', 'INDFMPIR', 'DMDHREDU', 'WTINT2YR', 'WTMEC2YR', 'SDMVPSU', 'SDMVSTRA', 'DR12IKC2', 'CYCLE', 'WTDR2D', 'DR12FS', 'DMDHREDZ', 'age', 'race', 'edu', 'pedu', 'Incm', 'incm2', 'include', 'Weight16a', 'cycles', 'sex', 'age1', 'age2', 'age3', 'race2', 'race3', 'race4', 'weekend', '_NAME_', '_LABEL_', 'DRDAY1', 'DRDAY2', 'tkal1', 'tkal2', 'Tcal', 'DR12DAY', 'kcal1', 'kcal2', 'kcal3', 'kcal4', 'kcal12', 'wt1', 'wt2', 'wt3', 'wt4', 'wt12', 'pcte1', 'pcte2', 'pcte3', 'pcte4', 'pcte12', 'pctg1', 'pctg2', 'pctg3', 'pctg4', 'pctg12', 'kcals2', 'kcals5', 'kcals6', 'kcals9', 'kcals13', 'kcals14', 'kcals17', 'kcals20', 'kcals21', 'kcals22', 'kcals23', 'kcals25', 'kcals28', 'kcals29', 'kcals33', 'kcals36', 'kcals39', 'kcals41', 'kcals3', 'kcals37', 'kcals38', 'kcals40', 'kcals42', 'kcals1', 'kcals10', 'kcals16', 'kcals24', 'kcals15', 'kcals18', 'kcals7', 'kcals8', 'kcals19', 'kcals46', 'kcals44', 'kcals45', 'kcals12', 'kcals27', 'kcals34', 'kcals30', 'kcals26', 'kcals43', 'kcals214', 'kcals215', 'kcals32', 'kcals31', 'wts2', 'wts5', 'wts6', 'wts9', 'wts13', 'wts14', 'wts17', 'wts20', 'wts21', 'wts22', 'wts23', 'wts25', 'wts28', 'wts29', 'wts33', 'wts36', 'wts39', 'wts41', 'wts3', 'wts37', 'wts38', 'wts40', 'wts42', 'wts1', 'wts10', 'wts16', 'wts24', 'wts15', 'wts18', 'wts7', 'wts8', 'wts19', 'wts46', 'wts44', 'wts45', 'wts12', 'wts27', 'wts34', 'wts30', 'wts26', 'wts43', 'wts214', 'wts215', 'wts32', 'wts31', 'pctes2', 'pctes5', 'pctes6', 'pctes9', 'pctes13', 'pctes14', 'pctes17', 'pctes20', 'pctes21', 'pctes22', 'pctes23', 'pctes25', 'pctes28', 'pctes29', 'pctes33', 'pctes36', 'pctes39', 'pctes41', 'pctes3', 'pctes37', 'pctes38', 'pctes40', 'pctes42', 'pctes1', 'pctes10', 'pctes16', 'pctes24', 'pctes15', 'pctes18', 'pctes7', 'pctes8', 'pctes19', 'pctes46', 'pctes44', 'pctes45', 'pctes12', 'pctes27', 'pctes34', 'pctes30', 'pctes26', 'pctes43', 'pctes214', 'pctes215', 'pctes32', 'pctes31', 'pctgs2', 'pctgs5', 'pctgs6', 'pctgs9', 'pctgs13', 'pctgs14', 'pctgs17', 'pctgs20', 'pctgs21', 'pctgs22', 'pctgs23', 'pctgs25', 'pctgs28', 'pctgs29', 'pctgs33', 'pctgs36', 'pctgs39', 'pctgs41', 'pctgs3', 'pctgs37', 'pctgs38', 'pctgs40', 'pctgs42', 'pctgs1', 'pctgs10', 'pctgs16', 'pctgs24', 'pctgs15', 'pctgs18', 'pctgs7', 'pctgs8', 'pctgs19', 'pctgs46', 'pctgs44', 'pctgs45', 'pctgs12', 'pctgs27', 'pctgs34', 'pctgs30', 'pctgs26', 'pctgs43', 'pctgs214', 'pctgs215', 'pctgs32', 'pctgs31', 'kcals4', 'kcals11', 'kcals35', 'kcals47', 'kcals48', 'kcals49', 'kcals50', 'kcals51', 'pctgs4', 'pctgs11', 'pctgs35', 'pctgs47', 'pctgs48', 'pctgs49', 'pctgs50', 'pctgs51', 'pctes4', 'pctes11', 'pctes35', 'pctes47', 'pctes48', 'pctes49', 'pctes50', 'pctes51', 'wts4', 'wts11', 'wts35', 'wts47', 'wts48', 'wts49', 'wts50', 'wts51', 'Pctes1a', 'Pctes6a', 'Pctgs1a', 'Pctgs6a', 'Pctgs10b', 'Pctgs112', 'Kcals1a', 'Kcals6a', 'Kcals10b', 'Kcals112', 'Kcals10a', 'Pctes10a', 'wts10a', 'Pctgs10a', 'Pctes10b', 'Pctes112', 'Kcals2a', 'Pctes2a', 'wts2a', 'Pctgs2a', 'Kcals13a', 'Pctes13a', 'wts13a', 'Pctgs13a', 'Pctes23a', 'Kcals23a', 'Pctgs23a', 'Kcals38a', 'Pctes38a', 'wts38a', 'Pctgs38a', 'Pctes25a', 'Pctgs25a', 'Kcals25a', 'Pctes24a', 'Pctgs24a', 'Kcals24a', 'Pctes30a', 'Pctes30b', 'Pctgs30a', 'Pctgs30b', 'Kcals30a', 'Kcals30b', 'Pctes37b', 'pctes44a']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Path to your .sas7bdat file\n",
    "file_path = \"/Users/dengshuyue/Desktop/SDOH/analysis/data/less_important/gg.sas7bdat\"\n",
    "\n",
    "# Load the dataset using pandas (requires pyreadstat)\n",
    "df = pd.read_sas(file_path, format=\"sas7bdat\")\n",
    "\n",
    "# Preview the data\n",
    "pd.set_option('display.max_columns', None)\n",
    "df.head()\n",
    "\n",
    "# List all column names\n",
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbfd6ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "libname data \"C:\\Users\\lwang18\\Box\\Projects\\5_UPF_Mortality\\data\";\n",
      "*%let home=C:\\Users\\LWANG18\\Box\\Projects\\5_UPF_Mortality\\results_revision ; \n",
      "%let home= C:\\Users\\lwang18\\Box\\Projects\\Kroger project\\Data for analysis\\Scores;\n",
      " %let path= C:\\Users\\lwang18\\Box\\Projects\\Kroger project\\Data for analysis\\Scores;\n",
      "\n",
      "%let home= C:\\Users\\lwang18\\OneDrive - Tufts\\Desktop\\Projects\\Food Insecurity,;\n",
      "libname out \"C:\\Users\\lwang18\\OneDrive - Tufts\\Desktop\\Projects\\Food Insecurity,\";\n",
      "\n",
      "libname NHANES \"C:\\Users\\LWANG18\\Box\\NHANES_Lu\" ;\n",
      "\n",
      "/** main analysis **/\n",
      "\n",
      "%macro cox(data, dvar, evars, covars, death , out);\n",
      "ods select all ; \n",
      "ODS OUTPUT PARAMETERESTIMATES=r0; \n",
      "proc surveyphreg data=&data;\n",
      "\tstrata sdmvstra;\n",
      "\tcluster sdmvpsu;\n",
      "\tweight wt;\n",
      "\tclass    sex (ref=\"1\") race edu(ref=\"1\") smk  pir(ref=\"3\") SNAP(ref=\"0\") FS ins2(ref=\"1\") ins i_FCS_sdq hei2015q(ref=\"3\") marriage  hoq065/param=ref;\n",
      "\tmodel py*&death(0)= &dvar &covars /rl ties=breslow;\n",
      "run ;\n",
      "data r0 ; set r0 ; \n",
      "HRCL=compress(round(hazardratio,0.01)||\"(\"||round(HRLOWerCL,.01)||\",\" ||round(HRupperCL,0.01)||\")\")  ; \n",
      "outcome=\"&death.\" ;\n",
      "predictor=\"&dvar.\";\n",
      "explainvar=\"null\" ;\n",
      "model=\"&out.\"; \n",
      "run; \n",
      "proc append data=r0  base=&out force; run;\n",
      "\n",
      "%let i=1 ;\n",
      "%do %while(%scan(&evars,&i) ne ) ;\n",
      "%let evar=%scan(&evars,&i) ;\n",
      "\n",
      "ods select all ; \n",
      "ODS OUTPUT PARAMETERESTIMATES=r1; \n",
      "proc surveyphreg data=&data;\n",
      "\tstrata sdmvstra;\n",
      "\tcluster sdmvpsu;\n",
      "\tweight wt;\n",
      "\tclass    sex (ref=\"1\") race edu(ref=\"1\") smk  pir(ref=\"3\") SNAP(ref=\"0\") FS ins2(ref=\"1\") ins i_FCS_sdq hei2015q(ref=\"3\") marriage hoq065(ref=\"1\")/param=ref;\n",
      "\tmodel py*&death(0)= &dvar &evar &covars /rl ties=breslow;\n",
      "run ;\n",
      "\n",
      "data resc&death.&i ; \n",
      "set r1; \n",
      "HRCL=compress(round(hazardratio,0.01)||\"(\"||round(HRLOWerCL,.01)||\",\" ||round(HRupperCL,0.01)||\")\")  ; \n",
      "outcome=\"&death.\" ;\n",
      "predictor=\"&dvar.\";\n",
      "explainvar=\"&evar.\" ;\n",
      "model=\"&out.\"; \n",
      "run; \n",
      "proc append data=resc&death.&i  base=&out force; run;\n",
      " \n",
      "%let i=%eval(&i+1) ;\n",
      "%end ;\n",
      "%mend ; \n",
      "\n",
      " \n",
      "%macro cox1(data, dvar, evars, covars, death , out, label);\n",
      "OPTION SPOOL ; \n",
      "ods select all ; \n",
      "ODS OUTPUT PARAMETERESTIMATES=r1; \n",
      "proc surveyphreg data=&data;\n",
      "\tstrata sdmvstra;\n",
      "\tcluster sdmvpsu;\n",
      "\tweight wt;\n",
      "\tclass    sex (ref=\"1\") race edu(ref=\"1\") smk  pir(ref=\"3\") SNAP(ref=\"0\") FS ins ins2(ref=\"1\") i_FCS_sdq hei2015q(ref=\"3\") /param=ref;\n",
      "\tmodel py*&death(0)= &dvar &evars &covars /rl ties=breslow;\n",
      "run ;\n",
      "\n",
      "data resc&death ; \n",
      "set r1; \n",
      "HRCL=compress(round(hazardratio,0.01)||\"(\"||round(HRLOWerCL,.01)||\",\" ||round(HRupperCL,0.01)||\")\")  ; \n",
      "outcome=\"&death.\" ;\n",
      "predictor=\"&dvar.\";\n",
      "explainvar=&label ; \n",
      "model=\"&out.\"; \n",
      "run; \n",
      "proc append data=resc&death  base=&out force; run;\n",
      "%mend ; \n",
      "\n",
      "*%cox(score_mort1,pir, fs  i_FCS_sdq SNAP ins ins2, ridageyr sex race   , mortstat, out_model0 );\n",
      "%cox(score_mort,pir,  hei2015q i_FCS_sdq fs SNAP ins ins2 marriage hoq065 unemployment unemployment2  ,  ridageyr sex race  edu , mortstat, out_model1 );\n",
      "%cox1(score_mort,pir, i_FCS_sdq fs , ridageyr sex race  edu , mortstat, out_model1 , \"comb1\");\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq SNAP ins, ridageyr sex race  edu , mortstat, out_model1 , \"all1\");\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq SNAP ins2, ridageyr sex race  edu , mortstat, out_model1 , \"all2\");\n",
      "\n",
      "/**age, sex, race, edu, ins as the base model*/\n",
      "%cox(score_mort,pir, hei2015q i_FCS_sdq fs  SNAP marriage hoq065 unemployment unemployment2,  ridageyr sex race  edu ins , mortstat, out_model11 );  \n",
      "%cox1(score_mort,pir, fs i_FCS_sdq, ridageyr sex race  edu ins, mortstat, out_model11 , \"comb1\");\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq SNAP ins, ridageyr sex race  edu ins, mortstat, out_model11 , \"all1\");\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq SNAP ins2, ridageyr sex race  edu ins, mortstat, out_model11 , \"all2\");\n",
      "\n",
      "/**age, sex, race, edu, lifestyles as the base model*/\n",
      "%cox(score_mort,pir, hei2015q i_FCS_sdq fs  SNAP ins ins2 marriage hoq065 unemployment unemployment2, ridageyr sex race  edu smk\tmet_hr perE_alco , mortstat, out_model2 );\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq, ridageyr sex race  edu smk\tmet_hr perE_alco, mortstat, out_model2 , \"comb1\");\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq SNAP ins, ridageyr sex race  edu smk\tmet_hr perE_alco, mortstat, out_model2 , \"all1\");\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq SNAP ins2, ridageyr sex race  edu smk\tmet_hr perE_alco, mortstat, out_model2 , \"all2\");\n",
      "\n",
      "/**age, sex, race, edu, lifestyles, insurance as the base model*/\n",
      "%cox(score_mort,pir, hei2015q i_FCS_sdq fs  SNAP marriage hoq065 unemployment unemployment2, ridageyr sex race  edu smk\tmet_hr perE_alco ins , mortstat, out_model21 );\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq, ridageyr sex race  edu smk\tmet_hr perE_alco ins, mortstat, out_model21 , \"comb1\");\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq SNAP ins, ridageyr sex race  edu smk\tmet_hr perE_alco ins, mortstat, out_model21 , \"all1\");\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq SNAP ins2, ridageyr sex race  edu smk\tmet_hr perE_alco ins, mortstat, out_model21 , \"all2\");\n",
      "\n",
      "\n",
      "/**age, sex, race, edu, lifestyles, baseline health as the base model*/\n",
      "%cox(score_mort,pir, hei2015q i_FCS_sdq fs  SNAP ins ins2 marriage hoq065 unemployment unemployment2, ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl , mortstat, out_model3 );\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq, ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl , mortstat, out_model3 , \"comb1\");\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq SNAP ins, ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl , mortstat, out_model3 , \"all1\");\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq SNAP ins2, ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl , mortstat, out_model3 , \"all2\");\n",
      "\n",
      "\n",
      "/**age, sex, race, edu, lifestyles, baseline health, insurance, as the base model*/\n",
      "%cox(score_mort,pir, hei2015q i_FCS_sdq fs  SNAP marriage hoq065 unemployment unemployment2, ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl ins, mortstat , out_model31 );\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq, ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl ins, mortstat, out_model31 , \"comb1\");\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq SNAP ins, ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl ins, mortstat, out_model31 , \"all1\");\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq SNAP ins2, ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl ins, mortstat, out_model31 , \"all2\");\n",
      "\n",
      "\n",
      "data out.out_models_pir ; set out_model1 out_model11 out_model2 out_model21 out_model3 out_model31; run; \n",
      "\n",
      "\n",
      "\n",
      "PROC expORT data= WORK.out_models_pir \n",
      "            outFILE= \"&home\\allmodelspir.xlsx\" \n",
      "            DBMS=xlsx REPLACE;\n",
      " *    GETNAMES=YES;\n",
      "RUN;\n",
      "\n",
      "/***Predictor=FS **/\n",
      "\n",
      "*%cox(score_mort1,pir, fs  i_FCS_sdq SNAP ins ins2, ridageyr sex race   , mortstat, out_model0 );\n",
      "%cox(score_mort,fs, hei2015q i_FCS_sdq pir SNAP ins ins2 marriage hoq065 unemployment unemployment2,  ridageyr sex race  edu , mortstat, out_model1a );\n",
      "\n",
      "%cox(score_mort,fs, hei2015q i_FCS_sdq, ridageyr sex race edu ins marriage unemployment2 hoq065 pir snap , mortstat, out_model11a );  \n",
      "\n",
      "%cox(score_mort,fs, hei2015q i_FCS_sdq pir SNAP ins ins2 marriage hoq065 unemployment unemployment2, ridageyr sex race edu smk met_hr perE_alco , mortstat, out_model2a );  \n",
      "\n",
      "%cox(score_mort,fs, hei2015q i_FCS_sdq, ridageyr sex race edu smk met_hr perE_alco ins marriage unemployment2 hoq065 pir snap , mortstat, out_model21a);  \n",
      "/**age, sex, race, edu, lifestyles, baseline health, insurance, as the base model*/\n",
      "\n",
      "%cox(score_mort,fs, hei2015q i_FCS_sdq pir  SNAP ins ins2 marriage hoq065 unemployment unemployment2, ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl , mortstat , out_model3a );\n",
      "\n",
      "%cox1(score_mort,fs, hei2015q pir  SNAP ins marriage hoq065 unemployment , ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl , mortstat, out_model4 , \"all1\");\n",
      "\n",
      "\n",
      "data out_models_fs ; set out_model1a out_model11a out_model2a out_model21a  out_model3a out_model4; run; \n",
      "\n",
      "\n",
      "PROC expORT data= WORK.out_models_fs \n",
      "            outFILE= \"&home\\allmodelsfs.xlsx\" \n",
      "            DBMS=xlsx REPLACE;\n",
      " *    GETNAMES=YES;\n",
      "RUN;\n",
      "\n",
      "\n",
      "%cox(score_mort, hei2015q, fs pir SNAP ins ins2 marriage hoq065 unemployment unemployment2,  ridageyr sex race  edu , mortstat, out_model1b );\n",
      "\n",
      "%cox(score_mort, hei2015q, fs pir SNAP ins ins2 marriage hoq065 unemployment unemployment2, ridageyr sex race edu smk met_hr perE_alco , mortstat, out_model2b );  \n",
      "\n",
      "%cox(score_mort, hei2015q, fs pir  SNAP ins ins2 marriage hoq065 unemployment unemployment2, ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl , mortstat , out_model3b );\n",
      "\n",
      "%cox1(score_mort, hei2015q, fs pir  SNAP ins  , ridageyr sex race  edu smk\tmet_hr perE_alco , mortstat, out_model2b , \"all1\");\n",
      "\n",
      "\n",
      "%cox1(score_mort, hei2015q, fs pir  SNAP ins marriage hoq065 unemployment , ridageyr sex race  edu smk\tmet_hr perE_alco , mortstat, out_model2b , \"all2\");\n",
      "\n",
      "%cox1(score_mort, hei2015q, fs pir  SNAP ins  , ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl , mortstat, out_model3b , \"all1\");\n",
      "\n",
      "%cox1(score_mort, hei2015q, fs pir  SNAP ins marriage hoq065 unemployment , ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl , mortstat, out_model3b , \"all2\");\n",
      "\n",
      "%cox1(score_mort, hei2015q, fs pir  SNAP ins marriage hoq065 unemployment , ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl , mortstat, out_model3b , \"all3\");\n",
      "\n",
      "\n",
      "data data.out_models_hei ; set out_model1b out_model2b out_model3b; run; \n",
      "\n",
      "\n",
      "PROC expORT data= WORK.out_models_hei \n",
      "            outFILE= \"&home\\allmodelshei.xlsx\" \n",
      "            DBMS=xlsx REPLACE;\n",
      " *    GETNAMES=YES;\n",
      "RUN;\n",
      "\n",
      "\n",
      "%cox(score_mort, SNAP, hei2015q fs pir ins ins2 marriage hoq065 unemployment unemployment2,  ridageyr sex race  edu , mortstat, out_model1c );\n",
      "\n",
      "%cox(score_mort, SNAP, hei2015q fs pir ins ins2 marriage hoq065 unemployment unemployment2, ridageyr sex race edu smk met_hr perE_alco , mortstat, out_model2c );  \n",
      "\n",
      "%cox(score_mort, SNAP, hei2015q fs pir  ins ins2 marriage hoq065 unemployment unemployment2, ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl , mortstat , out_model3c );\n",
      "\n",
      "%cox1(score_mort, SNAP, hei2015q fs pir  ins  , ridageyr sex race  edu smk\tmet_hr perE_alco , mortstat, out_model2c , \"all1\");\n",
      "\n",
      "%cox1(score_mort, SNAP, hei2015q fs pir  ins marriage hoq065 unemployment , ridageyr sex race  edu smk\tmet_hr perE_alco , mortstat, out_model2c , \"all2\");\n",
      "\n",
      "%cox1(score_mort, SNAP, hei2015q fs pir  ins  , ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl , mortstat, out_model3c , \"all1\");\n",
      "\n",
      "%cox1(score_mort, SNAP, hei2015q fs pir  ins marriage hoq065 unemployment , ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl , mortstat, out_model3c , \"all2\");\n",
      "\n",
      "%cox1(score_mort, SNAP, hei2015q fs pir  ins marriage hoq065 unemployment2 , ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl , mortstat, out_model3c , \"all3\");\n",
      "\n",
      "\n",
      "data data.out_models_SNAP ; set out_model1c out_model2c out_model3c; run; \n",
      "\n",
      "\n",
      "PROC expORT data= data.out_models_snap \n",
      "            outFILE= \"&home\\allmodelssnap.xlsx\" \n",
      "            DBMS=xlsx REPLACE;\n",
      " *    GETNAMES=YES;\n",
      "RUN;\n",
      "\n",
      "\n",
      "\n",
      "/*PROC expORT data= WORK.out_model0*/\n",
      "/*            outFILE= \"&home\\model0.xlsx\" */\n",
      "/*            DBMS=xlsx REPLACE;*/\n",
      "/* *    GETNAMES=YES;*/\n",
      "/*RUN;*/\n",
      "/*PROC expORT data= WORK.out_model1*/\n",
      "/*            outFILE= \"&home\\model1.xlsx\" */\n",
      "/*            DBMS=xlsx REPLACE;*/\n",
      "/* *    GETNAMES=YES;*/\n",
      "/*RUN;*/\n",
      "/*PROC expORT data= WORK.out_model2*/\n",
      "/*            outFILE= \"&home\\model2.xlsx\" */\n",
      "/*            DBMS=xlsx REPLACE;*/\n",
      "/* *    GETNAMES=YES;*/\n",
      "/*RUN;*/\n",
      "/**/\n",
      "/*PROC expORT data= WORK.out_model3*/\n",
      "/*            outFILE= \"&home\\model3.xlsx\" */\n",
      "/*            DBMS=xlsx REPLACE;*/\n",
      "/* *    GETNAMES=YES;*/\n",
      "/*RUN;*/\n",
      "\n",
      "\n",
      "******************************************************************************************************;\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to your .sas script file (SAS code, not dataset)\n",
    "file_path = \"/Users/dengshuyue/Desktop/SDOH/analysis/code/code_lu/Analysis1_COX_allcause.sas\"\n",
    "\n",
    "# Load the SAS script as plain text\n",
    "with open(file_path, \"r\") as file:\n",
    "    sas_code = file.read()\n",
    "\n",
    "# Optionally preview the first 30 lines\n",
    "sas_code_lines = sas_code.splitlines()\n",
    "for line in sas_code_lines[:300]:\n",
    "    print(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ae37456-384b-43ef-a397-231c5e43cf66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sample size: 149,358\n",
      "Age range: 0.0 to 85.0 years\n"
     ]
    }
   ],
   "source": [
    "# Display total number of rows (i.e., participants) in the dataset\n",
    "print(f\"Total sample size: {df.shape[0]:,}\")\n",
    "\n",
    "# Check the min and max age\n",
    "min_age = df['RIDAGEYR'].min()\n",
    "max_age = df['RIDAGEYR'].max()\n",
    "print(f\"Age range: {min_age} to {max_age} years\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0354e411-4071-4d2d-b78c-fca5f733ca15",
   "metadata": {},
   "source": [
    "<h2>Merge datasets step-by-step and track sample size</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea763e8-7ab0-4d7e-b17a-5afda4e7847b",
   "metadata": {},
   "source": [
    "<h3>Merge demo</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b6b695-bbdb-43e3-8433-c03f275f7770",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7fe3423d-97e9-4c13-bcab-ae3de85d819b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total rows: 101316\n",
      "✅ Unique SEQN values: 101316\n"
     ]
    }
   ],
   "source": [
    "# %% Step 1: Set file path for NHANES demographic data\n",
    "folder_path = \"/Users/dengshuyue/Desktop/SDOH/analysis/data/less_important\"\n",
    "\n",
    "\n",
    "# Step 2: NHANES demographic files to include (1999–2018 cycles)\n",
    "demo_files = [\n",
    "    \"demo\",    # 1999–2000\n",
    "    \"demo_b\",  # 2001–2002\n",
    "    \"demo_c\",  # 2003–2004\n",
    "    \"demo_d\",  # 2005–2006\n",
    "    \"demo_e\",  # 2007–2008\n",
    "    \"demo_f\",  # 2009–2010\n",
    "    \"demo_g\",  # 2011–2012\n",
    "    \"demo_h\",  # 2013–2014\n",
    "    \"demo_i\",  # 2015–2016\n",
    "    \"demo_j\"   # 2017–2018\n",
    "]\n",
    "\n",
    "\n",
    "# Step 3: Load and append files if they exist\n",
    "demo_dfs = []\n",
    "for f in demo_files:\n",
    "    file_path = os.path.join(folder_path, f\"{f}.sas7bdat\")\n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_sas(file_path)\n",
    "        demo_dfs.append(df)\n",
    "    else:\n",
    "        print(f\"⚠️ File not found: {file_path}\")\n",
    "\n",
    "# Step 4: Combine all loaded demographic datasets\n",
    "demoall = pd.concat(demo_dfs, ignore_index=True).copy()\n",
    "\n",
    "# Step 5: Filter to adults age 20+\n",
    "# demoall = demoall[demoall[\"RIDAGEYR\"] >= 20]\n",
    "\n",
    "# Step 6: Recode marital status from DMDMARTL\n",
    "conditions = [\n",
    "    demoall['DMDMARTL'].isin([1, 6]),  # Married or living with partner\n",
    "    demoall['DMDMARTL'].isin([3, 4]),  # Widowed or divorced\n",
    "    demoall['DMDMARTL'] == 2,          # Never married\n",
    "    demoall['DMDMARTL'] == 5           # Separated\n",
    "]\n",
    "choices = [1, 2, 3, 4]\n",
    "demoall['marriage'] = np.select(conditions, choices, default=pd.NA)\n",
    "\n",
    "# Step 7: Keep only relevant variables\n",
    "# demoall = demoall[['SEQN', 'marriage', 'SDDSRVYR']]\n",
    "\n",
    "demoall = demoall[['SEQN', 'RIDAGEYR', 'marriage', 'SDDSRVYR']]\n",
    "\n",
    "\n",
    "# Step 8: Summary\n",
    "print(f\"✅ Total rows: {demoall.shape[0]}\") # (age 20+)\n",
    "print(f\"✅ Unique SEQN values: {demoall['SEQN'].nunique()}\")\n",
    "\n",
    "# Optional: Preview first few rows\n",
    "# print(demoall.head())\n",
    "\n",
    "# Step 9: Save cleaned demoall for reuse\n",
    "demoall.to_pickle(\"/Users/dengshuyue/Desktop/SDOH/analysis/data/demoall.pkl\")  # Fast & preserves types\n",
    "# Or as CSV if needed:\n",
    "demoall.to_csv(\"/Users/dengshuyue/Desktop/SDOH/analysis/data/demoall.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "67dbb5db-41cc-4099-b9a8-991b09188d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Readable cycles:\n",
      "SDDSRVYR\n",
      "1999–2000    4877\n",
      "2001–2002    5405\n",
      "2003–2004    5031\n",
      "2005–2006    4977\n",
      "2007–2008    5926\n",
      "2009–2010    6201\n",
      "2011–2012    5546\n",
      "2013–2014    5757\n",
      "2015–2016    5701\n",
      "2017–2018    5524\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "# --- Paths -------------------------------------------------------\n",
    "root = Path(\"/Users/dengshuyue/Desktop/SDOH/analysis\")\n",
    "mort_path = root / \"data\" / \"less_important\" / \"mortality9918.sas7bdat\"\n",
    "\n",
    "# --- Load mortality (1999–2018) --------------------------------\n",
    "mort = pd.read_sas(mort_path, format=\"sas7bdat\", encoding=\"latin1\")\n",
    "mort.columns = mort.columns.str.upper()\n",
    "\n",
    "keep_cols = [c for c in mort.columns if c in {\n",
    "    \"SEQN\",\"MORTSTAT\",\"PERMTH_INT\",\"PERMTH_EXM\",\"UCOD_LEADING\",\"DIABETES\",\"HYPERTEN\",\"DODQTR\",\"DODYEAR\"\n",
    "}]\n",
    "if keep_cols:\n",
    "    mort = mort[keep_cols]\n",
    "\n",
    "# --- 1) Ensure demoall columns & types --------------------------\n",
    "demoall.columns = demoall.columns.str.upper()\n",
    "if \"SDDSRVYR\" not in demoall.columns:\n",
    "    raise ValueError(\"SDDSRVYR not found in demoall. Ensure 1999–2018 DEMO files include it.\")\n",
    "# Sometimes SDDSRVYR is object/string; coerce to numeric\n",
    "demoall[\"SDDSRVYR\"] = pd.to_numeric(demoall[\"SDDSRVYR\"], errors=\"coerce\")\n",
    "\n",
    "# --- 2) Merge age + cycle into mortality ------------------------\n",
    "mort_with_demo = mort.merge(\n",
    "    demoall[[\"SEQN\", \"RIDAGEYR\", \"SDDSRVYR\"]],\n",
    "    on=\"SEQN\", how=\"left\"\n",
    ")\n",
    "\n",
    "# --- 3) Filter adults (≥20 years) -------------------------------\n",
    "# mort_with_demo = mort_with_demo.dropna(subset=[\"RIDAGEYR\"])\n",
    "# mort_with_demo = mort_with_demo[mort_with_demo[\"RIDAGEYR\"] >= 20]\n",
    "\n",
    "# --- 4) Cycle counts among adults 20+ ---------------------------\n",
    "# cycle_counts = mort_with_demo[\"SDDSRVYR\"].value_counts(dropna=False).sort_index()\n",
    "# print(\"📊 NHANES cycles in mortality data (age 20+):\")\n",
    "# print(cycle_counts)\n",
    "# print(f\"Total valid mortality count (age 20+): {int(cycle_counts.sum())}\")\n",
    "\n",
    "# Optional: map cycle codes to years for readability\n",
    "cycle_map = {\n",
    "    1: \"1999–2000\", 2: \"2001–2002\", 3: \"2003–2004\",\n",
    "    4: \"2005–2006\", 5: \"2007–2008\", 6: \"2009–2010\",\n",
    "    7: \"2011–2012\", 8: \"2013–2014\", 9: \"2015–2016\",\n",
    "    10: \"2017–2018\"\n",
    "}\n",
    "print(\"\\nReadable cycles:\")\n",
    "print(cycle_counts.rename(index=cycle_map))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "db406c70-4024-4bb8-8285-6a1a259361d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 2.0\n",
      "Min: 102956.0\n"
     ]
    }
   ],
   "source": [
    "# print(mort_with_demo[\"SDDSRVYR\"].min())\n",
    "# print(mort_with_demo[\"SDDSRVYR\"].max())\n",
    "\n",
    "# SEQN \n",
    "print(\"Min:\", mort_with_demo[\"SEQN\"].min())\n",
    "print(\"Min:\", mort_with_demo[\"SEQN\"].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef85735d-9830-43be-9b91-9088064ac93e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b98ffb6b-3b13-4779-bf96-3905542bd0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mort SEQN range: 2.0 102956.0\n",
      "demoall SEQN range: 1.0 102956.0\n",
      "Number of overlapping SEQNs: 59064\n",
      "Example overlapping IDs: [2.0, 5.0, 6.0, 7.0, 10.0, 12.0, 13.0, 14.0, 15.0, 16.0]\n"
     ]
    }
   ],
   "source": [
    "# Make sure both SEQN are numeric\n",
    "mort[\"SEQN\"] = pd.to_numeric(mort[\"SEQN\"], errors=\"coerce\")\n",
    "demoall[\"SEQN\"] = pd.to_numeric(demoall[\"SEQN\"], errors=\"coerce\")\n",
    "\n",
    "# Check min/max ranges\n",
    "print(\"mort SEQN range:\", mort[\"SEQN\"].min(), mort[\"SEQN\"].max())\n",
    "print(\"demoall SEQN range:\", demoall[\"SEQN\"].min(), demoall[\"SEQN\"].max())\n",
    "\n",
    "# Check overlap\n",
    "overlap = set(mort[\"SEQN\"]).intersection(set(demoall[\"SEQN\"]))\n",
    "print(\"Number of overlapping SEQNs:\", len(overlap))\n",
    "\n",
    "# If very small, print a sample\n",
    "if len(overlap) > 0:\n",
    "    print(\"Example overlapping IDs:\", list(overlap)[:10])\n",
    "else:\n",
    "    print(\"⚠️ No overlap found — SEQNs don't match between mort and demoall\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0586a5b3-55b1-4425-9fd9-79f6e0fe2f46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfddd67f-1244-4cdb-a854-574da5a53dab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa310ca0-7085-418a-bef3-271d7eefb856",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dcb1804b-1f7d-46f5-a98a-19d6534ba831",
   "metadata": {},
   "source": [
    "<h3>Preprocess SDOH</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4bd52bac-c8dd-497b-b42f-fc2d95baa41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Employment data (OCQ) ---\n",
    "ocq = pd.read_sas(f\"{folder_path}/ocq.sas7bdat\")\n",
    "\n",
    "ocq['employ'] = np.nan\n",
    "ocq.loc[ocq['OCD150'] == 1, 'employ'] = 1\n",
    "ocq.loc[(ocq['OCD150'] == 3) | (ocq['OCQ380'] == 5), 'employ'] = 2\n",
    "ocq.loc[ocq['OCQ380'] == 3, 'employ'] = 3\n",
    "ocq.loc[ocq['OCQ380'].isin([4, 6]), 'employ'] = 4\n",
    "ocq.loc[ocq['OCQ380'].isin([1, 2, 7]), 'employ'] = 5\n",
    "ocq['unemployment'] = (ocq['employ'] == 2).astype(int)\n",
    "ocq = ocq[['SEQN', 'employ', 'unemployment']]\n",
    "\n",
    "# --- Housing data (HOQ) ---\n",
    "hoq = pd.read_sas(f\"{folder_path}/hoq.sas7bdat\")\n",
    "hoq.loc[hoq['HOQ065'].isin([7, 9]), 'HOQ065'] = np.nan\n",
    "hoq = hoq[['SEQN', 'HOD050', 'HOQ065']]\n",
    "\n",
    "# --- Health insurance (HIQs) ---\n",
    "hiqs = pd.read_sas(f\"{folder_path}/hiqs.sas7bdat\")\n",
    "ins = pd.DataFrame({'SEQN': hiqs['SEQN']})\n",
    "ins['ins'] = np.nan\n",
    "\n",
    "# Private insurance\n",
    "ins.loc[(hiqs['HIQ031A'] == 14) | (hiqs['HID030A'] == 1), 'ins'] = 1\n",
    "\n",
    "# Medicare\n",
    "ins.loc[\n",
    "    ((hiqs['HIQ031B'] == 15) & (hiqs['HIQ031D'] != 17) & (hiqs['HIQ031E'] != 18)) |\n",
    "    ((hiqs['HID030B'] == 1) & (hiqs['HID030C'] != 1)),\n",
    "    'ins'\n",
    "] = 2\n",
    "\n",
    "# Medicaid\n",
    "ins.loc[\n",
    "    (((hiqs['HIQ031D'] == 17) | (hiqs['HIQ031E'] == 18)) & (hiqs['HIQ031B'] != 15)) |\n",
    "    ((hiqs['HID030B'] != 1) & (hiqs['HID030C'] == 1)),\n",
    "    'ins'\n",
    "] = 3\n",
    "ins.loc[\n",
    "    ((hiqs['HIQ031B'] == 15) & (hiqs['HIQ031D'] == 17)) |\n",
    "    ((hiqs['HID030B'] == 1) & (hiqs['HID030C'] == 1)),\n",
    "    'ins'\n",
    "] = 3\n",
    "\n",
    "# Other insurance\n",
    "ins.loc[\n",
    "    (hiqs[['HIQ031C', 'HIQ031F', 'HIQ031G', 'HIQ031H', 'HIQ031I']].eq(1).any(axis=1)) |\n",
    "    (hiqs['HID030D'] == 1),\n",
    "    'ins'\n",
    "] = 5\n",
    "\n",
    "# No insurance\n",
    "ins.loc[(hiqs['HIQ011'] == 2) | (hiqs['HID010'] == 2), 'ins'] = 0\n",
    "\n",
    "# --- SNAP and Food Security (FSQS) ---\n",
    "fsqs = pd.read_sas(f\"{folder_path}/fsqs.sas7bdat\")\n",
    "snap = pd.DataFrame({'SEQN': fsqs['SEQN'], 'FSDHH': fsqs['FSDHH']})\n",
    "\n",
    "snap['SNAP'] = np.nan\n",
    "snap.loc[fsqs['FSQ165'] == 2, 'SNAP'] = 0\n",
    "snap.loc[fsqs['FSQ012'] == 1, 'SNAP'] = 1\n",
    "snap.loc[fsqs['FSQ012'] == 2, 'SNAP'] = 0\n",
    "snap.loc[fsqs['FSQ171'] == 1, 'SNAP'] = 1\n",
    "snap.loc[fsqs['FSQ171'] == 2, 'SNAP'] = 0\n",
    "snap.loc[fsqs['FSD170N'] >= 1, 'SNAP'] = 1\n",
    "snap.loc[fsqs['FSQ170'] == 1, 'SNAP'] = 1\n",
    "snap.loc[(fsqs['FSQ170'] == 2) & (fsqs['FSD170N'] < 1), 'SNAP'] = 0\n",
    "snap.loc[fsqs['FSD200'] == 1, 'SNAP'] = 1\n",
    "\n",
    "# Food Security\n",
    "snap['FS'] = np.nan\n",
    "snap.loc[fsqs['FSDHH'].isin([1, 2]), 'FS'] = 1\n",
    "snap.loc[fsqs['FSDHH'] > 2, 'FS'] = 0\n",
    "\n",
    "# Keep only final columns\n",
    "snap = snap[['SEQN', 'SNAP', 'FSDHH', 'FS']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f88a2a78-49cc-4ade-ac8e-eae71ea69aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         21005.0\n",
      "1         21006.0\n",
      "2         21008.0\n",
      "3         21009.0\n",
      "4         21010.0\n",
      "           ...   \n",
      "50909    102950.0\n",
      "50910    102952.0\n",
      "50911    102953.0\n",
      "50912    102954.0\n",
      "50913    102956.0\n",
      "Name: SEQN, Length: 50914, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(ocq[\"SEQN\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c6f152e3-60c4-4836-a8ec-d078b1ad09b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0              1.0\n",
      "1              2.0\n",
      "2              3.0\n",
      "3              4.0\n",
      "4              5.0\n",
      "            ...   \n",
      "101311    102952.0\n",
      "101312    102953.0\n",
      "101313    102954.0\n",
      "101314    102955.0\n",
      "101315    102956.0\n",
      "Name: SEQN, Length: 101316, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(demoall[\"SEQN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2760d5c0-e576-4dcd-93bc-1dd406304d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14 matching file(s):\n",
      "\n",
      "[MOVE] /Users/dengshuyue/Desktop/SDOH/analysis/data/less_important/demo_d.sas7bdat  ->  /Users/dengshuyue/Desktop/SDOH/analysis/data/nhanes_by_module/demographic/demo_d.sas7bdat\n",
      "[MOVE] /Users/dengshuyue/Desktop/SDOH/analysis/data/less_important/demo_e.sas7bdat  ->  /Users/dengshuyue/Desktop/SDOH/analysis/data/nhanes_by_module/demographic/demo_e.sas7bdat\n",
      "[MOVE] /Users/dengshuyue/Desktop/SDOH/analysis/data/less_important/demo.sas7bdat  ->  /Users/dengshuyue/Desktop/SDOH/analysis/data/nhanes_by_module/demographic/demo.sas7bdat\n",
      "[MOVE] /Users/dengshuyue/Desktop/SDOH/analysis/data/less_important/demo_c.sas7bdat  ->  /Users/dengshuyue/Desktop/SDOH/analysis/data/nhanes_by_module/demographic/demo_c.sas7bdat\n",
      "[MOVE] /Users/dengshuyue/Desktop/SDOH/analysis/data/less_important/demo_b.sas7bdat  ->  /Users/dengshuyue/Desktop/SDOH/analysis/data/nhanes_by_module/demographic/demo_b.sas7bdat\n",
      "[MOVE] /Users/dengshuyue/Desktop/SDOH/analysis/data/less_important/demo_i.sas7bdat  ->  /Users/dengshuyue/Desktop/SDOH/analysis/data/nhanes_by_module/demographic/demo_i.sas7bdat\n",
      "[MOVE] /Users/dengshuyue/Desktop/SDOH/analysis/data/less_important/demo_h.sas7bdat  ->  /Users/dengshuyue/Desktop/SDOH/analysis/data/nhanes_by_module/demographic/demo_h.sas7bdat\n",
      "[MOVE] /Users/dengshuyue/Desktop/SDOH/analysis/data/less_important/ocq.sas7bdat  ->  /Users/dengshuyue/Desktop/SDOH/analysis/data/nhanes_by_module/ocq/ocq.sas7bdat\n",
      "[MOVE] /Users/dengshuyue/Desktop/SDOH/analysis/data/less_important/demo_g.sas7bdat  ->  /Users/dengshuyue/Desktop/SDOH/analysis/data/nhanes_by_module/demographic/demo_g.sas7bdat\n",
      "[MOVE] /Users/dengshuyue/Desktop/SDOH/analysis/data/less_important/demo_f.sas7bdat  ->  /Users/dengshuyue/Desktop/SDOH/analysis/data/nhanes_by_module/demographic/demo_f.sas7bdat\n",
      "[MOVE] /Users/dengshuyue/Desktop/SDOH/analysis/data/less_important/hoq.sas7bdat  ->  /Users/dengshuyue/Desktop/SDOH/analysis/data/nhanes_by_module/hoq/hoq.sas7bdat\n",
      "[MOVE] /Users/dengshuyue/Desktop/SDOH/analysis/data/less_important/fsqs.sas7bdat  ->  /Users/dengshuyue/Desktop/SDOH/analysis/data/nhanes_by_module/fsq/fsqs.sas7bdat\n",
      "[MOVE] /Users/dengshuyue/Desktop/SDOH/analysis/data/less_important/hiqs.sas7bdat  ->  /Users/dengshuyue/Desktop/SDOH/analysis/data/nhanes_by_module/hiq/hiqs.sas7bdat\n",
      "[MOVE] /Users/dengshuyue/Desktop/SDOH/analysis/data/less_important/demo_j.sas7bdat  ->  /Users/dengshuyue/Desktop/SDOH/analysis/data/nhanes_by_module/demographic/demo_j.sas7bdat\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe8ce36-fac2-4b81-ba8f-b52357bf3d9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a8699e-a22a-426e-86c3-dc10f015d272",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70952887-d00a-4352-a60a-5e961026d804",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bcd2d863-6034-4a11-8471-e670a8424b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# -----------------------------\n",
    "# Step 0: Setup (NEW PATHS)\n",
    "# -----------------------------\n",
    "base_module_path = Path(\"/Users/dengshuyue/Desktop/SDOH/analysis/data/nhanes_by_module\")\n",
    "\n",
    "ocq_path = base_module_path / \"ocq\" / \"ocq.sas7bdat\"     # main OCQ (2003–2018)\n",
    "hoq_path = base_module_path / \"hoq\" / \"hoq.sas7bdat\"     # main HOQ (2003–2018)\n",
    "hiq_path = base_module_path / \"hiq\" / \"hiqs.sas7bdat\"    # insurance (HIQS for later years)\n",
    "fsq_path = base_module_path / \"fsq\" / \"fsqs.sas7bdat\"    # food security (FSQS for later years)\n",
    "\n",
    "# Step 1: Load demoall (unchanged)\n",
    "demoall = pd.read_pickle(\"/Users/dengshuyue/Desktop/SDOH/analysis/data/demoall.pkl\")\n",
    "if \"SEQN\" not in demoall.columns or \"RIDAGEYR\" not in demoall.columns:\n",
    "    raise ValueError(\"demoall is missing SEQN or RIDAGEYR\")\n",
    "\n",
    "# Step 2: Filter demoall to adults (age ≥ 20)\n",
    "age_df = demoall[[\"SEQN\", \"RIDAGEYR\"]].dropna()\n",
    "age_df = age_df[age_df[\"RIDAGEYR\"] >= 20]\n",
    "\n",
    "# Helper: Filter any df to age ≥ 20 using demoall\n",
    "def filter_adults(df):\n",
    "    return df.merge(age_df, on=\"SEQN\", how=\"inner\")\n",
    "\n",
    "# -----------------------------\n",
    "# Step 3: Employment (OCQ)\n",
    "# -----------------------------\n",
    "if not ocq_path.exists():\n",
    "    raise FileNotFoundError(f\"OCQ file not found at: {ocq_path}\")\n",
    "\n",
    "ocq = filter_adults(pd.read_sas(str(ocq_path), format=\"sas7bdat\", encoding=\"latin1\"))\n",
    "ocq['employ'] = np.nan\n",
    "ocq.loc[ocq['OCD150'] == 1, 'employ'] = 1\n",
    "ocq.loc[(ocq['OCD150'] == 3) | (ocq['OCQ380'] == 5), 'employ'] = 2\n",
    "ocq.loc[ocq['OCQ380'] == 3, 'employ'] = 3\n",
    "ocq.loc[ocq['OCQ380'].isin([4, 6]), 'employ'] = 4\n",
    "ocq.loc[ocq['OCQ380'].isin([1, 2, 7]), 'employ'] = 5\n",
    "ocq['unemployment'] = (ocq['employ'] == 2).astype(int)\n",
    "ocq = ocq[['SEQN', 'employ', 'unemployment']]\n",
    "\n",
    "# -----------------------------\n",
    "# Step 4: Housing (HOQ)\n",
    "# -----------------------------\n",
    "if not hoq_path.exists():\n",
    "    raise FileNotFoundError(f\"HOQ file not found at: {hoq_path}\")\n",
    "\n",
    "hoq = filter_adults(pd.read_sas(str(hoq_path), format=\"sas7bdat\", encoding=\"latin1\"))\n",
    "if \"HOQ065\" in hoq.columns:\n",
    "    hoq.loc[hoq['HOQ065'].isin([7, 9]), 'HOQ065'] = np.nan\n",
    "keep_hoq = [c for c in ['SEQN', 'HOD050', 'HOQ065'] if c in hoq.columns]\n",
    "hoq = hoq[keep_hoq]\n",
    "\n",
    "# -----------------------------\n",
    "# Step 5: Insurance (HIQS)\n",
    "# -----------------------------\n",
    "if not hiq_path.exists():\n",
    "    raise FileNotFoundError(f\"HIQS file not found at: {hiq_path}\")\n",
    "\n",
    "hiqs = filter_adults(pd.read_sas(str(hiq_path), format=\"sas7bdat\", encoding=\"latin1\"))\n",
    "ins = pd.DataFrame({'SEQN': hiqs['SEQN']})\n",
    "ins['ins'] = np.nan\n",
    "\n",
    "# Private\n",
    "if 'HIQ031A' in hiqs: ins.loc[(hiqs['HIQ031A'] == 14), 'ins'] = 1\n",
    "if 'HID030A' in hiqs: ins.loc[(hiqs['HID030A'] == 1), 'ins'] = 1\n",
    "\n",
    "# Medicare\n",
    "cond_med = False\n",
    "if set(['HIQ031B','HIQ031D','HIQ031E']).issubset(hiqs.columns):\n",
    "    cond_med = ((hiqs['HIQ031B'] == 15) & (hiqs['HIQ031D'] != 17) & (hiqs['HIQ031E'] != 18))\n",
    "if set(['HID030B','HID030C']).issubset(hiqs.columns):\n",
    "    cond_med = cond_med | ((hiqs['HID030B'] == 1) & (hiqs['HID030C'] != 1)) if isinstance(cond_med, pd.Series) else ((hiqs['HID030B'] == 1) & (hiqs['HID030C'] != 1))\n",
    "ins.loc[cond_med, 'ins'] = 2\n",
    "\n",
    "# Medicaid\n",
    "cond_mcaid = False\n",
    "if set(['HIQ031B','HIQ031D','HIQ031E']).issubset(hiqs.columns):\n",
    "    cond_mcaid = (((hiqs['HIQ031D'] == 17) | (hiqs['HIQ031E'] == 18)) & (hiqs['HIQ031B'] != 15))\n",
    "if set(['HID030B','HID030C']).issubset(hiqs.columns):\n",
    "    cond_mcaid = cond_mcaid | ((hiqs['HID030B'] != 1) & (hiqs['HID030C'] == 1)) if isinstance(cond_mcaid, pd.Series) else ((hiqs['HID030B'] != 1) & (hiqs['HID030C'] == 1))\n",
    "ins.loc[cond_mcaid, 'ins'] = 3\n",
    "\n",
    "# Medicaid (both present)\n",
    "if set(['HIQ031B','HIQ031D']).issubset(hiqs.columns):\n",
    "    ins.loc[((hiqs['HIQ031B'] == 15) & (hiqs['HIQ031D'] == 17)), 'ins'] = 3\n",
    "if set(['HID030B','HID030C']).issubset(hiqs.columns):\n",
    "    ins.loc[((hiqs['HID030B'] == 1) & (hiqs['HID030C'] == 1)), 'ins'] = 3\n",
    "\n",
    "# Other insurance\n",
    "cols_other = [c for c in ['HIQ031C','HIQ031F','HIQ031G','HIQ031H','HIQ031I'] if c in hiqs.columns]\n",
    "cond_other = hiqs[cols_other].eq(1).any(axis=1) if cols_other else False\n",
    "if 'HID030D' in hiqs:\n",
    "    cond_other = cond_other | (hiqs['HID030D'] == 1) if isinstance(cond_other, pd.Series) else (hiqs['HID030D'] == 1)\n",
    "ins.loc[cond_other, 'ins'] = 5\n",
    "\n",
    "# No insurance\n",
    "conds_none = []\n",
    "if 'HIQ011' in hiqs: conds_none.append(hiqs['HIQ011'] == 2)\n",
    "if 'HID010' in hiqs: conds_none.append(hiqs['HID010'] == 2)\n",
    "if conds_none:\n",
    "    ins.loc[np.logical_or.reduce(conds_none), 'ins'] = 0\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Step 6: SNAP & Food Security (FSQS)\n",
    "# -----------------------------\n",
    "if not fsq_path.exists():\n",
    "    raise FileNotFoundError(f\"FSQS file not found: {fsq_path}\")\n",
    "\n",
    "fsqs = filter_adults(pd.read_sas(str(fsq_path), format=\"sas7bdat\", encoding=\"latin1\"))\n",
    "snap = pd.DataFrame({'SEQN': fsqs['SEQN']})\n",
    "if 'FSDHH' in fsqs: snap['FSDHH'] = fsqs['FSDHH']\n",
    "\n",
    "snap['SNAP'] = np.nan\n",
    "if 'FSQ165' in fsqs: snap.loc[fsqs['FSQ165'] == 2, 'SNAP'] = 0\n",
    "if 'FSQ012' in fsqs:\n",
    "    snap.loc[fsqs['FSQ012'] == 1, 'SNAP'] = 1\n",
    "    snap.loc[fsqs['FSQ012'] == 2, 'SNAP'] = 0\n",
    "if 'FSQ171' in fsqs:\n",
    "    snap.loc[fsqs['FSQ171'] == 1, 'SNAP'] = 1\n",
    "    snap.loc[fsqs['FSQ171'] == 2, 'SNAP'] = 0\n",
    "if 'FSD170N' in fsqs: snap.loc[fsqs['FSD170N'] >= 1, 'SNAP'] = 1\n",
    "if 'FSQ170' in fsqs:\n",
    "    snap.loc[fsqs['FSQ170'] == 1, 'SNAP'] = 1\n",
    "    snap.loc[(fsqs['FSQ170'] == 2) & (fsqs.get('FSD170N', pd.Series(index=fsqs.index)) < 1), 'SNAP'] = 0\n",
    "if 'FSD200' in fsqs: snap.loc[fsqs['FSD200'] == 1, 'SNAP'] = 1\n",
    "\n",
    "snap['FS'] = np.nan\n",
    "if 'FSDHH' in fsqs:\n",
    "    snap.loc[fsqs['FSDHH'].isin([1, 2]), 'FS'] = 1\n",
    "    snap.loc[fsqs['FSDHH'] > 2, 'FS'] = 0\n",
    "\n",
    "snap = snap[['SEQN', 'SNAP', 'FSDHH', 'FS']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "11e5ca16-ea4a-4e9b-a81c-fe4264eea952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         21009.0\n",
      "1         21010.0\n",
      "2         21012.0\n",
      "3         21015.0\n",
      "4         21017.0\n",
      "           ...   \n",
      "44785    102950.0\n",
      "44786    102952.0\n",
      "44787    102953.0\n",
      "44788    102954.0\n",
      "44789    102956.0\n",
      "Name: SEQN, Length: 44790, dtype: float64\n",
      "0         21009.0\n",
      "1         21010.0\n",
      "2         21012.0\n",
      "3         21015.0\n",
      "4         21017.0\n",
      "           ...   \n",
      "44785    102950.0\n",
      "44786    102952.0\n",
      "44787    102953.0\n",
      "44788    102954.0\n",
      "44789    102956.0\n",
      "Name: SEQN, Length: 44790, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# check\n",
    "print(hiqs[\"SEQN\"])\n",
    "\n",
    "print(snap[\"SEQN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f90b8c-dca4-4bab-ba33-40d1a0a02ca8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f05d68dc-726b-4398-86ee-e8db7b1d908b",
   "metadata": {},
   "source": [
    "<h4>Add previous years SDOH</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3e5560aa-f09e-4990-a208-792e509c387d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Loaded OCQ.xpt (1999–2000)\n",
      "Shape: (7295, 36)\n",
      "SEQN min: 2.0 SEQN max: 9965.0\n",
      "Unique SEQN: 7295\n",
      "First 5 SEQN: [2.0, 5.0, 6.0, 7.0, 8.0]\n",
      "\n",
      "✅ Loaded OCQ_B.xpt (2001–2002)\n",
      "Shape: (6634, 25)\n",
      "SEQN min: 9966.0 SEQN max: 21004.0\n",
      "Unique SEQN: 6634\n",
      "First 5 SEQN: [9966.0, 9967.0, 9968.0, 9969.0, 9970.0]\n",
      "\n",
      "✅ Loaded OCQ_C.xpt (2003–2004)\n",
      "Shape: (6213, 25)\n",
      "SEQN min: 21005.0 SEQN max: 31125.0\n",
      "Unique SEQN: 6213\n",
      "First 5 SEQN: [21005.0, 21006.0, 21008.0, 21009.0, 21010.0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# merge previous 99-03 ir 99-01  first check their SEQN\n",
    "\n",
    "import pyreadstat\n",
    "from pathlib import Path\n",
    "\n",
    "ocq_dir = Path(\"/Users/dengshuyue/Desktop/SDOH/analysis/data/nhanes_by_module/ocq\")\n",
    "\n",
    "files = [\n",
    "    (\"OCQ.xpt\",   \"1999–2000\"),\n",
    "    (\"OCQ_B.xpt\", \"2001–2002\"),\n",
    "    (\"OCQ_C.xpt\", \"2003–2004\"),\n",
    "]\n",
    "\n",
    "for fname, years in files:\n",
    "    path = ocq_dir / fname\n",
    "    if not path.exists():\n",
    "        print(f\"⚠️ File not found: {path}\")\n",
    "        continue\n",
    "    \n",
    "    df, meta = pyreadstat.read_xport(path)\n",
    "    df.columns = df.columns.str.upper()\n",
    "    \n",
    "    print(f\"\\n✅ Loaded {fname} ({years})\")\n",
    "    print(\"Shape:\", df.shape)\n",
    "    print(\"SEQN min:\", df['SEQN'].min(), \"SEQN max:\", df['SEQN'].max())\n",
    "    print(\"Unique SEQN:\", df['SEQN'].nunique())\n",
    "    print(\"First 5 SEQN:\", df['SEQN'].head().tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "93108342-9861-42c3-bb91-6f1c7e437e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final OCQ shape: (58719, 4)\n",
      "SEQN range: 2 → 102956\n",
      "Cycles present:\n",
      " SDDSRVYR\n",
      "1    7295\n",
      "2    6634\n",
      "Name: count, dtype: Int64\n"
     ]
    }
   ],
   "source": [
    "# first start with ocq \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pyreadstat\n",
    "\n",
    "# --- Paths ---\n",
    "ocq_dir = Path(\"/Users/dengshuyue/Desktop/SDOH/analysis/data/nhanes_by_module/ocq\")\n",
    "ocq_main_path = ocq_dir / \"ocq.sas7bdat\"                 # 2003–2018 consolidated\n",
    "ocq_early_paths = [(ocq_dir/\"OCQ.xpt\", 1), (ocq_dir/\"OCQ_B.xpt\", 2)]  # 1999–2002\n",
    "\n",
    "def recode_employment(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = df.columns.str.upper()\n",
    "    df[\"EMPLOY\"] = np.nan\n",
    "    if \"OCD150\" in df:\n",
    "        df.loc[df[\"OCD150\"] == 1, \"EMPLOY\"] = 1\n",
    "        df.loc[df[\"OCD150\"] == 3, \"EMPLOY\"] = 2\n",
    "    if \"OCQ380\" in df:\n",
    "        df.loc[df[\"OCQ380\"] == 5, \"EMPLOY\"] = 2\n",
    "        df.loc[df[\"OCQ380\"] == 3, \"EMPLOY\"] = 3\n",
    "        df.loc[df[\"OCQ380\"].isin([4, 6]), \"EMPLOY\"] = 4\n",
    "        df.loc[df[\"OCQ380\"].isin([1, 2, 7]), \"EMPLOY\"] = 5\n",
    "    df[\"UNEMPLOYMENT\"] = (df[\"EMPLOY\"] == 2).astype(int)\n",
    "    keep = [c for c in [\"SEQN\",\"EMPLOY\",\"UNEMPLOYMENT\",\"SDDSRVYR\"] if c in df.columns]\n",
    "    return df[keep]\n",
    "\n",
    "# --- A) Read early cycles (1999–2002) ---\n",
    "early_parts = []\n",
    "for p, cyc in ocq_early_paths:\n",
    "    if p.exists():\n",
    "        df, _ = pyreadstat.read_xport(str(p))\n",
    "        df.columns = df.columns.str.upper()\n",
    "        df[\"SDDSRVYR\"] = cyc\n",
    "        early_parts.append(recode_employment(df))\n",
    "    else:\n",
    "        print(f\"⚠️ Missing early OCQ file: {p}\")\n",
    "ocq_early = pd.concat(early_parts, ignore_index=True) if early_parts else pd.DataFrame()\n",
    "\n",
    "# --- B) Read main consolidated OCQ (2003–2018) from the SAME folder ---\n",
    "if not ocq_main_path.exists():\n",
    "    raise FileNotFoundError(f\"Main OCQ not found: {ocq_main_path}\")\n",
    "ocq_main_raw = pd.read_sas(str(ocq_main_path), format=\"sas7bdat\", encoding=\"latin1\")\n",
    "ocq_main_raw.columns = ocq_main_raw.columns.str.upper()\n",
    "\n",
    "# Optional adult filter if you have it defined\n",
    "try:\n",
    "    ocq_main_raw = filter_adults(ocq_main_raw)\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "# If SDDSRVYR isn't present in the main file, you can skip it or infer later via SEQN.\n",
    "ocq_main = recode_employment(ocq_main_raw)\n",
    "\n",
    "# --- C) Combine to final OCQ (1999–2018) ---\n",
    "ocq = pd.concat([ocq_early, ocq_main], ignore_index=True)\n",
    "\n",
    "# Hygiene\n",
    "ocq[\"SEQN\"] = pd.to_numeric(ocq[\"SEQN\"], errors=\"coerce\").astype(\"Int64\")\n",
    "if \"SDDSRVYR\" in ocq.columns:\n",
    "    ocq[\"SDDSRVYR\"] = pd.to_numeric(ocq[\"SDDSRVYR\"], errors=\"coerce\").astype(\"Int64\")\n",
    "ocq = ocq.dropna(subset=[\"SEQN\"]).drop_duplicates(subset=[\"SEQN\"])\n",
    "\n",
    "# --- D) Quick checks ---\n",
    "print(\"Final OCQ shape:\", ocq.shape)\n",
    "print(\"SEQN range:\", ocq[\"SEQN\"].min(), \"→\", ocq[\"SEQN\"].max())\n",
    "if \"SDDSRVYR\" in ocq.columns:\n",
    "    print(\"Cycles present:\\n\", ocq[\"SDDSRVYR\"].value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cdb51458-37b6-431b-a9eb-ba225dc6e232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final HOQ shape: (65794, 4)\n",
      "SEQN range: 1 → 102956\n",
      "Cycles present:\n",
      " SDDSRVYR\n",
      "1999–2000     9965\n",
      "2001–2002    11039\n",
      "Name: count, dtype: Int64\n"
     ]
    }
   ],
   "source": [
    "# hoq (housing)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Paths ---\n",
    "hoq_dir = Path(\"/Users/dengshuyue/Desktop/SDOH/analysis/data/nhanes_by_module/hoq\")\n",
    "hoq_main_path = hoq_dir / \"hoq.sas7bdat\"  # your 2003–2018 consolidated file\n",
    "early_candidates = [\n",
    "    (hoq_dir / \"HOQ.xpt\",   1),  # 1999–2000\n",
    "    (hoq_dir / \"HOQ_B.xpt\", 2),  # 2001–2002\n",
    "    # if you happen to have SAS versions instead of XPTs, we’ll try those too:\n",
    "    (hoq_dir / \"hoq.sas7bdat\",   1),  # fallback for 99–00 (rare)\n",
    "    (hoq_dir / \"hoq_b.sas7bdat\", 2),\n",
    "]\n",
    "\n",
    "def read_any(path: Path) -> pd.DataFrame:\n",
    "    if path.suffix.lower() == \".xpt\":\n",
    "        import pyreadstat\n",
    "        df, _ = pyreadstat.read_xport(str(path))\n",
    "    elif path.suffix.lower() == \".sas7bdat\":\n",
    "        df = pd.read_sas(str(path), format=\"sas7bdat\", encoding=\"latin1\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file: {path}\")\n",
    "    df.columns = df.columns.str.upper()\n",
    "    return df\n",
    "\n",
    "def preprocess_hoq(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    # Clean HOQ065: set 7/9 to NaN where present\n",
    "    if \"HOQ065\" in df.columns:\n",
    "        df.loc[df[\"HOQ065\"].isin([7, 9]), \"HOQ065\"] = np.nan\n",
    "    # Keep the core columns that exist\n",
    "    keep = [c for c in [\"SEQN\", \"HOD050\", \"HOQ065\", \"SDDSRVYR\"] if c in df.columns]\n",
    "    return df[keep]\n",
    "\n",
    "# --- A) Early cycles (99–02) ---\n",
    "early_parts = []\n",
    "seen_cycles = set()\n",
    "for p, cyc in early_candidates:\n",
    "    if p.exists() and cyc not in seen_cycles:\n",
    "        df = read_any(p)\n",
    "        df[\"SDDSRVYR\"] = cyc\n",
    "        early_parts.append(preprocess_hoq(df))\n",
    "        seen_cycles.add(cyc)\n",
    "hoq_early = pd.concat(early_parts, ignore_index=True) if early_parts else pd.DataFrame()\n",
    "\n",
    "# --- B) Main consolidated HOQ (2003–2018) ---\n",
    "if not hoq_main_path.exists():\n",
    "    raise FileNotFoundError(f\"Main HOQ not found: {hoq_main_path}\")\n",
    "hoq_main_raw = read_any(hoq_main_path)\n",
    "# (Optional) If you have an adult filter function:\n",
    "try:\n",
    "    hoq_main_raw = filter_adults(hoq_main_raw)  # your function, if defined\n",
    "except NameError:\n",
    "    pass\n",
    "hoq_main = preprocess_hoq(hoq_main_raw)\n",
    "\n",
    "# --- C) Combine to final HOQ (1999–2018) ---\n",
    "hoq_all = pd.concat([hoq_early, hoq_main], ignore_index=True)\n",
    "\n",
    "# Hygiene\n",
    "if \"SEQN\" in hoq_all:\n",
    "    hoq_all[\"SEQN\"] = pd.to_numeric(hoq_all[\"SEQN\"], errors=\"coerce\").astype(\"Int64\")\n",
    "if \"SDDSRVYR\" in hoq_all:\n",
    "    hoq_all[\"SDDSRVYR\"] = pd.to_numeric(hoq_all[\"SDDSRVYR\"], errors=\"coerce\").astype(\"Int64\")\n",
    "hoq_all = hoq_all.dropna(subset=[\"SEQN\"]).drop_duplicates(subset=[\"SEQN\"])\n",
    "\n",
    "# --- D) Quick checks ---\n",
    "print(\"Final HOQ shape:\", hoq_all.shape)\n",
    "print(\"SEQN range:\", hoq_all[\"SEQN\"].min(), \"→\", hoq_all[\"SEQN\"].max())\n",
    "if \"SDDSRVYR\" in hoq_all:\n",
    "    cycle_map = {1:\"1999–2000\",2:\"2001–2002\",3:\"2003–2004\",4:\"2005–2006\",5:\"2007–2008\",\n",
    "                 6:\"2009–2010\",7:\"2011–2012\",8:\"2013–2014\",9:\"2015–2016\",10:\"2017–2018\"}\n",
    "    counts = hoq_all[\"SDDSRVYR\"].value_counts().sort_index()\n",
    "    print(\"Cycles present:\\n\", counts.rename(index=cycle_map))\n",
    "\n",
    "# (Optional) Save for reuse\n",
    "hoq_all.to_csv(\"/Users/dengshuyue/Desktop/SDOH/analysis/data/nhanes_by_module/hoq/hoq_99_18.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "627089f5-c29e-46b2-8a3d-d9bcf5e96738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insurance table shape: (55081, 3)\n",
      "Cycles present:\n",
      " SDDSRVYR\n",
      "1    4880\n",
      "2    5411\n",
      "Name: count, dtype: Int64\n"
     ]
    }
   ],
   "source": [
    "# hiqs \n",
    "\n",
    "# -----------------------------\n",
    "# Insurance (HIQ/HIQS) — add 1999–2002\n",
    "# -----------------------------\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "base_module_path = Path(\"/Users/dengshuyue/Desktop/SDOH/analysis/data/nhanes_by_module\")\n",
    "hiq_dir = base_module_path / \"hiq\"\n",
    "\n",
    "# Main consolidated later years (HIQS for 2003–2018)\n",
    "hiq_main_path = hiq_dir / \"hiqs.sas7bdat\"\n",
    "\n",
    "# Early cycles (prefer XPT; fallback to sas7bdat if that’s what you have)\n",
    "early_candidates = [\n",
    "    (hiq_dir / \"HIQ.xpt\",       1),  # 1999–2000\n",
    "    (hiq_dir / \"HIQ_B.xpt\",     2),  # 2001–2002\n",
    "    (hiq_dir / \"hiq.sas7bdat\",  1),  # fallback (rare)\n",
    "    (hiq_dir / \"hiq_b.sas7bdat\",2),\n",
    "]\n",
    "\n",
    "def read_any(p: Path) -> pd.DataFrame:\n",
    "    if p.suffix.lower() == \".xpt\":\n",
    "        import pyreadstat\n",
    "        df, _ = pyreadstat.read_xport(str(p))\n",
    "    elif p.suffix.lower() == \".sas7bdat\":\n",
    "        df = pd.read_sas(str(p), format=\"sas7bdat\", encoding=\"latin1\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file: {p}\")\n",
    "    df.columns = df.columns.str.upper()\n",
    "    return df\n",
    "\n",
    "# A) Early cycles (99–02)\n",
    "early_parts, seen = [], set()\n",
    "for p, cyc in early_candidates:\n",
    "    if p.exists() and cyc not in seen:\n",
    "        df = read_any(p)\n",
    "        df[\"SDDSRVYR\"] = cyc\n",
    "        early_parts.append(df)\n",
    "        seen.add(cyc)\n",
    "hiq_early_raw = pd.concat(early_parts, ignore_index=True) if early_parts else pd.DataFrame()\n",
    "\n",
    "# B) Main consolidated (03–18)\n",
    "if not hiq_main_path.exists():\n",
    "    raise FileNotFoundError(f\"HIQS file not found at: {hiq_main_path}\")\n",
    "hiq_main_raw = read_any(hiq_main_path)\n",
    "\n",
    "# C) Filter to adults using your helper\n",
    "hiq_early = filter_adults(hiq_early_raw) if not hiq_early_raw.empty else hiq_early_raw\n",
    "hiqs       = filter_adults(hiq_main_raw)\n",
    "\n",
    "# D) Stack early + main\n",
    "hiq_all = pd.concat([hiq_early, hiqs], ignore_index=True)\n",
    "hiq_all.columns = hiq_all.columns.str.upper()\n",
    "\n",
    "# E) Compute insurance 'ins' on the combined data (guards for varying columns)\n",
    "ins = pd.DataFrame({\"SEQN\": hiq_all[\"SEQN\"]})\n",
    "if \"SDDSRVYR\" in hiq_all.columns:\n",
    "    ins[\"SDDSRVYR\"] = hiq_all[\"SDDSRVYR\"]\n",
    "ins[\"ins\"] = np.nan\n",
    "\n",
    "# Private\n",
    "if \"HIQ031A\" in hiq_all: ins.loc[hiq_all[\"HIQ031A\"] == 14, \"ins\"] = 1\n",
    "if \"HID030A\" in hiq_all: ins.loc[hiq_all[\"HID030A\"] == 1,  \"ins\"] = 1\n",
    "\n",
    "# Medicare\n",
    "cond_med = False\n",
    "if {\"HIQ031B\",\"HIQ031D\",\"HIQ031E\"}.issubset(hiq_all.columns):\n",
    "    cond_med = (hiq_all[\"HIQ031B\"] == 15) & (hiq_all[\"HIQ031D\"] != 17) & (hiq_all[\"HIQ031E\"] != 18)\n",
    "if {\"HID030B\",\"HID030C\"}.issubset(hiq_all.columns):\n",
    "    cond_med = cond_med | ((hiq_all[\"HID030B\"] == 1) & (hiq_all[\"HID030C\"] != 1)) if isinstance(cond_med, pd.Series) else ((hiq_all[\"HID030B\"] == 1) & (hiq_all[\"HID030C\"] != 1))\n",
    "ins.loc[cond_med, \"ins\"] = 2\n",
    "\n",
    "# Medicaid\n",
    "cond_mcaid = False\n",
    "if {\"HIQ031B\",\"HIQ031D\",\"HIQ031E\"}.issubset(hiq_all.columns):\n",
    "    cond_mcaid = (((hiq_all[\"HIQ031D\"] == 17) | (hiq_all[\"HIQ031E\"] == 18)) & (hiq_all[\"HIQ031B\"] != 15))\n",
    "if {\"HID030B\",\"HID030C\"}.issubset(hiq_all.columns):\n",
    "    cond_mcaid = cond_mcaid | ((hiq_all[\"HID030B\"] != 1) & (hiq_all[\"HID030C\"] == 1)) if isinstance(cond_mcaid, pd.Series) else ((hiq_all[\"HID030B\"] != 1) & (hiq_all[\"HID030C\"] == 1))\n",
    "ins.loc[cond_mcaid, \"ins\"] = 3\n",
    "\n",
    "# Medicaid when both present\n",
    "if {\"HIQ031B\",\"HIQ031D\"}.issubset(hiq_all.columns):\n",
    "    ins.loc[(hiq_all[\"HIQ031B\"] == 15) & (hiq_all[\"HIQ031D\"] == 17), \"ins\"] = 3\n",
    "if {\"HID030B\",\"HID030C\"}.issubset(hiq_all.columns):\n",
    "    ins.loc[(hiq_all[\"HID030B\"] == 1) & (hiq_all[\"HID030C\"] == 1), \"ins\"] = 3\n",
    "\n",
    "# Other insurance\n",
    "other_cols = [c for c in [\"HIQ031C\",\"HIQ031F\",\"HIQ031G\",\"HIQ031H\",\"HIQ031I\"] if c in hiq_all.columns]\n",
    "cond_other = hiq_all[other_cols].eq(1).any(axis=1) if other_cols else False\n",
    "if \"HID030D\" in hiq_all:\n",
    "    cond_other = cond_other | (hiq_all[\"HID030D\"] == 1) if isinstance(cond_other, pd.Series) else (hiq_all[\"HID030D\"] == 1)\n",
    "ins.loc[cond_other, \"ins\"] = 5\n",
    "\n",
    "# No insurance\n",
    "conds_none = []\n",
    "if \"HIQ011\" in hiq_all: conds_none.append(hiq_all[\"HIQ011\"] == 2)\n",
    "if \"HID010\" in hiq_all: conds_none.append(hiq_all[\"HID010\"] == 2)\n",
    "if conds_none:\n",
    "    ins.loc[np.logical_or.reduce(conds_none), \"ins\"] = 0\n",
    "\n",
    "# Tidy up\n",
    "ins[\"SEQN\"] = pd.to_numeric(ins[\"SEQN\"], errors=\"coerce\").astype(\"Int64\")\n",
    "if \"SDDSRVYR\" in ins.columns:\n",
    "    ins[\"SDDSRVYR\"] = pd.to_numeric(ins[\"SDDSRVYR\"], errors=\"coerce\").astype(\"Int64\")\n",
    "ins = ins.dropna(subset=[\"SEQN\"]).drop_duplicates(subset=[\"SEQN\"])\n",
    "\n",
    "print(\"Insurance table shape:\", ins.shape)\n",
    "if \"SDDSRVYR\" in ins.columns:\n",
    "    print(\"Cycles present:\\n\", ins[\"SDDSRVYR\"].value_counts().sort_index())\n",
    "\n",
    "# print(ins[\"SEQN\"])\n",
    "\n",
    "# (Optional) Save for reuse\n",
    "ins.to_csv(\"/Users/dengshuyue/Desktop/SDOH/analysis/data/nhanes_by_module/hiq/ins_99_18.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "be2337dc-d28f-4be7-9912-c6dba7fdae00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNAP table shape: (55081, 5)\n",
      "Cycles present:\n",
      " SDDSRVYR\n",
      "1    4880\n",
      "2    5411\n",
      "Name: count, dtype: Int64\n",
      "0             2\n",
      "1             5\n",
      "2             7\n",
      "3            10\n",
      "4            12\n",
      "          ...  \n",
      "55076    102950\n",
      "55077    102952\n",
      "55078    102953\n",
      "55079    102954\n",
      "55080    102956\n",
      "Name: SEQN, Length: 55081, dtype: Int64\n"
     ]
    }
   ],
   "source": [
    "# fsq\n",
    "# -----------------------------\n",
    "# Step 6: SNAP & Food Security (FSQ/FSQS) — add 1999–2002\n",
    "# -----------------------------\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "base_module_path = Path(\"/Users/dengshuyue/Desktop/SDOH/analysis/data/nhanes_by_module\")\n",
    "fsq_dir = base_module_path / \"fsq\"\n",
    "\n",
    "# Main later-years file (FSQS for 2003–2018)\n",
    "fsq_main_path = fsq_dir / \"fsqs.sas7bdat\"\n",
    "\n",
    "# Early cycles (prefer XPT; fallback to sas7bdat if that’s what you have)\n",
    "early_candidates = [\n",
    "    (fsq_dir / \"FSQ.xpt\",         1),  # 1999–2000\n",
    "    (fsq_dir / \"FSQ_B.xpt\",       2),  # 2001–2002\n",
    "    (fsq_dir / \"fsq.sas7bdat\",    1),  # fallback (rare)\n",
    "    (fsq_dir / \"fsq_b.sas7bdat\",  2),\n",
    "]\n",
    "\n",
    "def read_any(p: Path) -> pd.DataFrame:\n",
    "    if p.suffix.lower() == \".xpt\":\n",
    "        import pyreadstat\n",
    "        df, _ = pyreadstat.read_xport(str(p))\n",
    "    elif p.suffix.lower() == \".sas7bdat\":\n",
    "        df = pd.read_sas(str(p), format=\"sas7bdat\", encoding=\"latin1\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file: {p}\")\n",
    "    df.columns = df.columns.str.upper()\n",
    "    return df\n",
    "\n",
    "# A) Early cycles (99–02)\n",
    "early_parts, seen = [], set()\n",
    "for p, cyc in early_candidates:\n",
    "    if p.exists() and cyc not in seen:\n",
    "        df = read_any(p)\n",
    "        df[\"SDDSRVYR\"] = cyc\n",
    "        early_parts.append(df)\n",
    "        seen.add(cyc)\n",
    "fsq_early_raw = pd.concat(early_parts, ignore_index=True) if early_parts else pd.DataFrame()\n",
    "\n",
    "# B) Main consolidated (03–18)\n",
    "if not fsq_main_path.exists():\n",
    "    raise FileNotFoundError(f\"FSQS file not found: {fsq_main_path}\")\n",
    "fsq_main_raw = read_any(fsq_main_path)\n",
    "\n",
    "# C) Filter to adults using your helper\n",
    "fsq_early = filter_adults(fsq_early_raw) if not fsq_early_raw.empty else fsq_early_raw\n",
    "fsqs       = filter_adults(fsq_main_raw)\n",
    "\n",
    "# D) Stack early + main\n",
    "fsq_all = pd.concat([fsq_early, fsqs], ignore_index=True)\n",
    "fsq_all.columns = fsq_all.columns.str.upper()\n",
    "\n",
    "# E) Build SNAP/FS outputs (guards for varying columns)\n",
    "snap = pd.DataFrame({\"SEQN\": fsq_all[\"SEQN\"]})\n",
    "if \"SDDSRVYR\" in fsq_all: snap[\"SDDSRVYR\"] = fsq_all[\"SDDSRVYR\"]\n",
    "if \"FSDHH\" in fsq_all:   snap[\"FSDHH\"] = fsq_all[\"FSDHH\"]\n",
    "\n",
    "snap[\"SNAP\"] = np.nan\n",
    "if \"FSQ165\" in fsq_all: snap.loc[fsq_all[\"FSQ165\"] == 2, \"SNAP\"] = 0\n",
    "if \"FSQ012\" in fsq_all:\n",
    "    snap.loc[fsq_all[\"FSQ012\"] == 1, \"SNAP\"] = 1\n",
    "    snap.loc[fsq_all[\"FSQ012\"] == 2, \"SNAP\"] = 0\n",
    "if \"FSQ171\" in fsq_all:\n",
    "    snap.loc[fsq_all[\"FSQ171\"] == 1, \"SNAP\"] = 1\n",
    "    snap.loc[fsq_all[\"FSQ171\"] == 2, \"SNAP\"] = 0\n",
    "if \"FSD170N\" in fsq_all: snap.loc[fsq_all[\"FSD170N\"] >= 1, \"SNAP\"] = 1\n",
    "if \"FSQ170\" in fsq_all:\n",
    "    snap.loc[fsq_all[\"FSQ170\"] == 1, \"SNAP\"] = 1\n",
    "    snap.loc[(fsq_all[\"FSQ170\"] == 2) & (fsq_all.get(\"FSD170N\", pd.Series(index=fsq_all.index)) < 1), \"SNAP\"] = 0\n",
    "if \"FSD200\" in fsq_all: snap.loc[fsq_all[\"FSD200\"] == 1, \"SNAP\"] = 1\n",
    "\n",
    "snap[\"FS\"] = np.nan\n",
    "if \"FSDHH\" in fsq_all:\n",
    "    snap.loc[fsq_all[\"FSDHH\"].isin([1, 2]), \"FS\"] = 1\n",
    "    snap.loc[fsq_all[\"FSDHH\"] > 2, \"FS\"] = 0\n",
    "\n",
    "# Final columns\n",
    "snap = snap[[c for c in [\"SEQN\", \"SNAP\", \"FSDHH\", \"FS\", \"SDDSRVYR\"] if c in snap.columns]]\n",
    "\n",
    "# Hygiene\n",
    "snap[\"SEQN\"] = pd.to_numeric(snap[\"SEQN\"], errors=\"coerce\").astype(\"Int64\")\n",
    "if \"SDDSRVYR\" in snap.columns:\n",
    "    snap[\"SDDSRVYR\"] = pd.to_numeric(snap[\"SDDSRVYR\"], errors=\"coerce\").astype(\"Int64\")\n",
    "snap = snap.dropna(subset=[\"SEQN\"]).drop_duplicates(subset=[\"SEQN\"])\n",
    "\n",
    "# Quick checks\n",
    "print(\"SNAP table shape:\", snap.shape)\n",
    "if \"SDDSRVYR\" in snap.columns:\n",
    "    print(\"Cycles present:\\n\", snap[\"SDDSRVYR\"].value_counts().sort_index())\n",
    "\n",
    "print(snap[\"SEQN\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cde0c3-85aa-437a-84fc-158958d2bf1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a507cb5-b7ba-4bab-baa2-f17f890d434b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c183089-7261-488d-b971-8f72648c5434",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6115a7-51db-4778-bf42-805f814bc90d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e034b887-1b61-4c4c-acf1-3eece86e054c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1620f9b-fb3b-4864-b5cb-aee223d496e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fe7017-9d2b-4e1a-9e73-b409c115b9fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39615767-a602-4e6a-8728-05d792d852c0",
   "metadata": {},
   "source": [
    "<h3> HEI? COV </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "340baed0-47e0-430c-a6d9-6f7949971219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores2:\n",
      "  Rows: 39746\n",
      "  Unique SEQN: 39746\n",
      "----------------------------------------\n",
      "covar:\n",
      "  Rows: 101316\n",
      "  Unique SEQN: 101316\n",
      "----------------------------------------\n",
      "covariates1:\n",
      "  Rows: 39262\n",
      "  Unique SEQN: 39262\n",
      "----------------------------------------\n",
      "dietwt:\n",
      "  Rows: 88413\n",
      "  Unique SEQN: 88413\n",
      "----------------------------------------\n",
      "mort:\n",
      "  Rows: 59064\n",
      "  Unique SEQN: 59064\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Read dietary score data\n",
    "scores = pd.read_excel(os.path.join(folder_path, \"i.scores.xlsx\"), engine=\"openpyxl\")\n",
    "\n",
    "# Rename columns to match desired output\n",
    "scores = scores.rename(columns={\n",
    "    \"seqn\": \"SEQN\",\n",
    "    \"i.FCS\": \"i_FCS\",\n",
    "    \"i.optup\": \"i_optup\",  # keep lowercase here first\n",
    "    \"i.HSR\": \"i_HSR\",\n",
    "    \"i.nutri\": \"i_nutri\"\n",
    "})\n",
    "\n",
    "# Then copy and rename for output\n",
    "scores2 = scores[[\"SEQN\", \"i_FCS\", \"i_optup\", \"i_HSR\", \"i_nutri\"]].copy()\n",
    "scores2 = scores2.rename(columns={\"i_optup\": \"i_Optup\"})\n",
    "scores2 = scores2.sort_values(\"SEQN\")\n",
    "\n",
    "\n",
    "# Step 2: Read covariates from Lu paper\n",
    "covar = pd.read_sas(os.path.join(folder_path, \"covar.sas7bdat\"), format=\"sas7bdat\")\n",
    "covar = covar.rename(columns=str.upper)  # make all column names uppercase to match SAS style\n",
    "# filter available variables only\n",
    "covar_vars = [\"SEQN\", \"RIDAGEYR\", \"SEX\", \"RACE\", \"EDU\", \"INDFMPIR\", \"SMK_AVG\", \"SMK_PAST\",\n",
    "              \"SMK\", \"ALCG2\", \"HEI2015_TOTAL_SCORE\", \"DIABE\"]\n",
    "covar = covar[[col for col in covar_vars if col in covar.columns]].copy()\n",
    "covar = covar.sort_values(\"SEQN\")\n",
    "\n",
    "# Step 3: Read covariates from Meghan paper\n",
    "covariates1_raw = pd.read_csv(os.path.join(folder_path, \"covariates.csv\"))\n",
    "covariates1 = covariates1_raw.rename(columns={\"seqn\": \"SEQN\"})\n",
    "covariates_vars = [\"SEQN\", \"sdmvpsu\", \"sdmvstra\", \"met_hr\", \"perE_alco\", \"dm_self\",\n",
    "                   \"tchol\", \"hdl\", \"ldl\", \"tg\", \"bmi\", \"CVD\", \"dm_rx\", \"chol_rx\",\n",
    "                   \"angina_rx\", \"lung_disease\", \"angina\", \"hba1c\", \"sbp\", \"dbp\", \"cancer\"]\n",
    "covariates1 = covariates1[[col for col in covariates_vars if col in covariates1.columns]].copy()\n",
    "covariates1 = covariates1.sort_values(\"SEQN\")\n",
    "\n",
    "# Step 4: Read dietary weight data (filter DAYS == 1)\n",
    "dietwt = pd.read_sas(os.path.join(folder_path, \"gg.sas7bdat\"), format=\"sas7bdat\")\n",
    "\n",
    "# Check for expected columns\n",
    "required_cols = [\"SEQN\", \"DAYS\", \"WTDRD1\", \"WTDR2D\", \"DR12DRST\"]\n",
    "missing = [col for col in required_cols if col not in dietwt.columns]\n",
    "if missing:\n",
    "    print(f\"Warning: Missing columns from gg.sas7bdat: {missing}\")\n",
    "\n",
    "# Filter and select\n",
    "dietwt = dietwt[dietwt[\"DAYS\"] == 1][[\"SEQN\", \"WTDRD1\", \"WTDR2D\", \"DR12DRST\"]].copy()\n",
    "dietwt = dietwt.sort_values(\"SEQN\")\n",
    "\n",
    "\n",
    "# Step 5: Read mortality data\n",
    "mort = pd.read_sas(os.path.join(folder_path, \"mortality9918.sas7bdat\"), format=\"sas7bdat\")\n",
    "mort = mort.sort_values(\"SEQN\")\n",
    "\n",
    "def summarize_df(name, df):\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Rows: {df.shape[0]}\")\n",
    "    print(f\"  Unique SEQN: {df['SEQN'].nunique()}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "summarize_df(\"scores2\", scores2)\n",
    "summarize_df(\"covar\", covar)\n",
    "summarize_df(\"covariates1\", covariates1)\n",
    "summarize_df(\"dietwt\", dietwt)\n",
    "summarize_df(\"mort\", mort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "67d72dcb-97f9-4681-a81f-3176921845e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "i.scores.xlsx\n",
      "  Rows: 39746\n",
      "  Null SEQN: 0\n",
      "  dtype: Int64\n",
      "  Min / Max: 21009 / 102956\n",
      "  Unique SEQN: 39746  |  Duplicates: 0\n",
      "  Head SEQN: [np.int64(21009), np.int64(21010), np.int64(21012), np.int64(21015), np.int64(21017)]\n",
      "  Tail SEQN: [np.int64(102949), np.int64(102952), np.int64(102953), np.int64(102954), np.int64(102956)]\n",
      "\n",
      "covar.sas7bdat\n",
      "  Rows: 101316\n",
      "  Null SEQN: 0\n",
      "  dtype: Int64\n",
      "  Min / Max: 1 / 102956\n",
      "  Unique SEQN: 101316  |  Duplicates: 0\n",
      "  Head SEQN: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5)]\n",
      "  Tail SEQN: [np.int64(102952), np.int64(102953), np.int64(102954), np.int64(102955), np.int64(102956)]\n",
      "\n",
      "covariates.csv\n",
      "  Rows: 39262\n",
      "  Null SEQN: 0\n",
      "  dtype: Int64\n",
      "  Min / Max: 21009 / 102956\n",
      "  Unique SEQN: 39262  |  Duplicates: 0\n",
      "  Head SEQN: [np.int64(21009), np.int64(21010), np.int64(21012), np.int64(21015), np.int64(21017)]\n",
      "  Tail SEQN: [np.int64(102947), np.int64(102952), np.int64(102953), np.int64(102954), np.int64(102956)]\n",
      "\n",
      "gg.sas7bdat\n",
      "  Rows: 149358\n",
      "  Null SEQN: 0\n",
      "  dtype: Int64\n",
      "  Min / Max: 1 / 102956\n",
      "  Unique SEQN: 88469  |  Duplicates: 60889\n",
      "  Head SEQN: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5)]\n",
      "  Tail SEQN: [np.int64(102954), np.int64(102955), np.int64(102955), np.int64(102956), np.int64(102956)]\n",
      "\n",
      "mortality9918.sas7bdat\n",
      "  Rows: 59064\n",
      "  Null SEQN: 0\n",
      "  dtype: Int64\n",
      "  Min / Max: 2 / 102956\n",
      "  Unique SEQN: 59064  |  Duplicates: 0\n",
      "  Head SEQN: [np.int64(2), np.int64(5), np.int64(6), np.int64(7), np.int64(10)]\n",
      "  Tail SEQN: [np.int64(102950), np.int64(102952), np.int64(102953), np.int64(102954), np.int64(102956)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#  SEQN check — i.scores, covar, covariates, gg (dietwt), mortality9918\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Set your folder\n",
    "folder_path = \"/Users/dengshuyue/Desktop/SDOH/analysis/data/less_important\"\n",
    "\n",
    "def _standardize_seqn(df):\n",
    "    # make columns case-insensitive and return a Series of SEQN (nullable Int64)\n",
    "    df.columns = df.columns.str.upper()\n",
    "    if \"SEQN\" not in df.columns:\n",
    "        return None\n",
    "    s = pd.to_numeric(df[\"SEQN\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    return s\n",
    "\n",
    "def summarize_seqn(name, seqn):\n",
    "    print(f\"\\n{name}\")\n",
    "    if seqn is None:\n",
    "        print(\"  ❌ SEQN column not found\")\n",
    "        return\n",
    "    print(f\"  Rows: {len(seqn)}\")\n",
    "    print(f\"  Null SEQN: {seqn.isna().sum()}\")\n",
    "    if seqn.notna().any():\n",
    "        print(f\"  dtype: {seqn.dtype}\")\n",
    "        print(f\"  Min / Max: {seqn.min()} / {seqn.max()}\")\n",
    "        nunq = seqn.nunique(dropna=True)\n",
    "        dups = (seqn.notna().sum() - nunq)\n",
    "        print(f\"  Unique SEQN: {nunq}  |  Duplicates: {dups}\")\n",
    "        # small sample\n",
    "        print(f\"  Head SEQN: {list(seqn.dropna().head(5))}\")\n",
    "        print(f\"  Tail SEQN: {list(seqn.dropna().tail(5))}\")\n",
    "\n",
    "# 1) i.scores.xlsx\n",
    "try:\n",
    "    scores = pd.read_excel(os.path.join(folder_path, \"i.scores.xlsx\"), engine=\"openpyxl\")\n",
    "    # handle seqn casing\n",
    "    if \"seqn\" in scores.columns and \"SEQN\" not in scores.columns:\n",
    "        scores = scores.rename(columns={\"seqn\": \"SEQN\"})\n",
    "    summarize_seqn(\"i.scores.xlsx\", _standardize_seqn(scores))\n",
    "except Exception as e:\n",
    "    print(\"\\n[i.scores.xlsx] ⚠️\", e)\n",
    "\n",
    "# 2) covar.sas7bdat\n",
    "try:\n",
    "    covar = pd.read_sas(os.path.join(folder_path, \"covar.sas7bdat\"),\n",
    "                        format=\"sas7bdat\", encoding=\"latin1\")\n",
    "    summarize_seqn(\"covar.sas7bdat\", _standardize_seqn(covar))\n",
    "except Exception as e:\n",
    "    print(\"\\n[covar.sas7bdat] ⚠️\", e)\n",
    "\n",
    "# 3) covariates.csv\n",
    "try:\n",
    "    covariates1_raw = pd.read_csv(os.path.join(folder_path, \"covariates.csv\"))\n",
    "    if \"seqn\" in covariates1_raw.columns and \"SEQN\" not in covariates1_raw.columns:\n",
    "        covariates1_raw = covariates1_raw.rename(columns={\"seqn\": \"SEQN\"})\n",
    "    summarize_seqn(\"covariates.csv\", _standardize_seqn(covariates1_raw))\n",
    "except Exception as e:\n",
    "    print(\"\\n[covariates.csv] ⚠️\", e)\n",
    "\n",
    "# 4) gg.sas7bdat (diet weights)\n",
    "try:\n",
    "    dietwt = pd.read_sas(os.path.join(folder_path, \"gg.sas7bdat\"),\n",
    "                         format=\"sas7bdat\", encoding=\"latin1\")\n",
    "    summarize_seqn(\"gg.sas7bdat\", _standardize_seqn(dietwt))\n",
    "except Exception as e:\n",
    "    print(\"\\n[gg.sas7bdat] ⚠️\", e)\n",
    "\n",
    "# 5) mortality9918.sas7bdat\n",
    "try:\n",
    "    mort = pd.read_sas(os.path.join(folder_path, \"mortality9918.sas7bdat\"),\n",
    "                       format=\"sas7bdat\", encoding=\"latin1\")\n",
    "    summarize_seqn(\"mortality9918.sas7bdat\", _standardize_seqn(mort))\n",
    "except Exception as e:\n",
    "    print(\"\\n[mortality9918.sas7bdat] ⚠️\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bed3786-e96e-4a4d-9fca-c91f28013bd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38d3b04-7466-4e81-97eb-0d826de9ddde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔥🔥🔥🔥🔥🔥 NOW WORK AT HERE!!!!!!\n",
    "# try to extend covariates.csv to 99-18 currently is 03-18\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e182d4d0-64d5-48dd-bce6-d51a3d27cfdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b7fcd9-0865-465e-9d48-0c30172b3253",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e524830a-7e2b-47b2-829b-9b1940c9c224",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53125a08-a2c8-475c-8ee7-f3bcdab7a415",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3cf7aa-8db1-49bc-8c3e-e5afe1dae728",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc0d637-2830-4a1b-8545-59c9854101da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d208411a-510b-4c13-ad33-1ef1bd57eaff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Index(['SEQN', 'RIDAGEYR', 'marriage', 'SDDSRVYR'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"RIDAGEYR\" in demoall.columns)  # Should print True\n",
    "print(demoall.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "625ece01-37f0-44fd-a74e-84e4d729ad04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure SEQN is int in all datasets\n",
    "# for name, df in datasets.items():\n",
    "#    df[\"SEQN\"] = df[\"SEQN\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0044a5a0-83ab-48de-aa4f-dfb396953781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename RIDAGEYR in covar to avoid overwriting demoall's RIDAGEYR\n",
    "covar = covar.rename(columns={\"RIDAGEYR\": \"RIDAGEYR_covar\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5408feb0-87e0-4fa6-942e-a83fc0f4d79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "# ✅ Step 0: Merge all datasets by SEQN\n",
    "merge_list = [demoall, hoq_all, ocq, mort, snap, ins, covar, dietwt, scores2, covariates1]\n",
    "score_mort = reduce(lambda left, right: pd.merge(left, right, on=\"SEQN\", how=\"left\"), merge_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1b64dc0c-cb7b-4b56-b218-3a48a1a72d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Step 1: Filter to adults age ≥ 20\n",
    "if \"RIDAGEYR\" not in score_mort.columns:\n",
    "    raise ValueError(\"RIDAGEYR is missing after merge — check if demoall was merged correctly.\")\n",
    "score_mort = score_mort[score_mort[\"RIDAGEYR\"] >= 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4294a2ac-4a00-4e88-bd93-5b3084ebf2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Variable transformations\n",
    "score_mort[\"wt10\"] = score_mort[\"WTDRD1\"] / 10\n",
    "score_mort[\"wt\"] = score_mort[\"WTDR2D\"] / 8\n",
    "score_mort[\"i_FCS_sd\"] = score_mort[\"i_FCS\"] / 10.89\n",
    "score_mort[\"i_Optup_sd\"] = score_mort[\"i_Optup\"] / 8.17\n",
    "score_mort[\"i_nutri_sd\"] = -score_mort[\"i_nutri\"] / 3.17\n",
    "score_mort[\"i_HSR_sd\"] = score_mort[\"i_HSR\"] / 1.01\n",
    "score_mort[\"hei2015_sd\"] = score_mort[\"HEI2015_TOTAL_SCORE\"] / 13\n",
    "\n",
    "# Step 3: Recode death indicators\n",
    "for cause, code in {\n",
    "    \"death_heart\": \"001\", \"death_cancer\": \"002\", \"death_resp\": \"003\", \"Death_inj\": \"004\",\n",
    "    \"death_cerev\": \"005\", \"Death_alz\": \"006\", \"death_diabe\": \"007\",\n",
    "    \"Death_infl\": \"008\", \"Death_kid\": \"009\", \"death_other1\": \"010\"\n",
    "}.items():\n",
    "    score_mort[cause] = (score_mort[\"UCOD_LEADING\"] == code).astype(int)\n",
    "\n",
    "# Step 4: Composite categories\n",
    "score_mort[\"Death_other\"] = score_mort[[\"death_resp\", \"Death_inj\", \"Death_alz\", \"Death_infl\"]].sum(axis=1).clip(upper=1)\n",
    "score_mort[\"Death_oth2\"] = score_mort[[\"death_resp\", \"Death_inj\", \"Death_alz\", \"Death_infl\", \"death_other1\"]].sum(axis=1).clip(upper=1)\n",
    "score_mort[\"death_cvd\"] = score_mort[[\"death_heart\", \"death_cerev\"]].sum(axis=1).clip(upper=1)\n",
    "score_mort[\"death_cmd\"] = score_mort[[\"death_heart\", \"death_cerev\", \"death_diabe\"]].sum(axis=1).clip(upper=1)\n",
    "score_mort[\"death_cmdk\"] = score_mort[[\"death_heart\", \"death_cerev\", \"death_diabe\", \"Death_kid\"]].sum(axis=1).clip(upper=1)\n",
    "score_mort[\"death_cmdkh\"] = score_mort[\"death_cmdk\"]\n",
    "score_mort.loc[score_mort[\"DIABETES\"] == 1, \"death_cmdkh\"] = 1\n",
    "score_mort.loc[score_mort[\"HYPERTEN\"] == 1, \"death_cmdkh\"] = 1\n",
    "score_mort[\"death_cmd\"] = score_mort[\"death_cmd\"].fillna(0)\n",
    "score_mort.loc[score_mort[\"death_cmd\"] == 1, [\"Death_other\", \"Death_oth2\"]] = 0\n",
    "\n",
    "# Step 5: Multiple cause mortality\n",
    "score_mort[\"death_multi\"] = score_mort[\"MORTSTAT\"]\n",
    "score_mort.loc[score_mort[\"death_cmd\"] == 1, \"death_multi\"] = 1\n",
    "score_mort.loc[score_mort[\"death_cancer\"] == 1, \"death_multi\"] = 2\n",
    "score_mort.loc[score_mort[\"Death_oth2\"] == 1, \"death_multi\"] = 3\n",
    "\n",
    "# Step 6: Age & time vars\n",
    "score_mort[\"agesq\"] = score_mort[\"RIDAGEYR\"] ** 2\n",
    "score_mort[\"py\"] = score_mort[\"PERMTH_EXM\"] / 12\n",
    "score_mort[\"agestart\"] = score_mort[\"RIDAGEYR\"]\n",
    "score_mort[\"ageend\"] = score_mort[\"RIDAGEYR\"] + score_mort[\"py\"]\n",
    "\n",
    "# Step 7: Poverty\n",
    "score_mort[\"pir\"] = 5\n",
    "score_mort.loc[(score_mort[\"INDFMPIR\"] < 1.3) & (score_mort[\"INDFMPIR\"].notna()), \"pir\"] = 1\n",
    "score_mort.loc[(score_mort[\"INDFMPIR\"] >= 1.3), \"pir\"] = 2\n",
    "score_mort.loc[(score_mort[\"INDFMPIR\"] >= 3), \"pir\"] = 3\n",
    "\n",
    "# Step 8: Recode SNAP\n",
    "score_mort.loc[(score_mort[\"INDFMPIR\"].between(0, 1.3)) & (score_mort[\"SNAP\"] != 1), \"SNAP\"] = 2\n",
    "\n",
    "# Step 9: BMI categories\n",
    "score_mort[\"bmic\"] = pd.NA\n",
    "score_mort.loc[(score_mort[\"bmi\"] > 0) & (score_mort[\"bmi\"] < 18.5), \"bmic\"] = 0\n",
    "score_mort.loc[(score_mort[\"bmi\"] >= 18.5) & (score_mort[\"bmi\"] < 25), \"bmic\"] = 1\n",
    "score_mort.loc[(score_mort[\"bmi\"] >= 25), \"bmic\"] = 2\n",
    "score_mort.loc[(score_mort[\"bmi\"] >= 30), \"bmic\"] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "76734187-4ed6-487d-8c2a-aff415f549c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After ELIGSTAT & MORTSTAT inclusion: {1: 54945, 0: 136}\n",
      "Dropped at Step 2 (diet data or recall quality): 6288\n",
      "Dropped at Step 3 (missing FS/SNAP or pir=4): 9755\n",
      "Dropped at Step 4 (zero or negative WTDRD1): 0\n",
      "📊 Final include flag counts: {1: 38902, 3: 9755, 2: 6288, 0: 136}\n",
      " Final analytic sample size: 38902\n"
     ]
    }
   ],
   "source": [
    "# check drop by step\n",
    "\n",
    "# Step 1: Initial inclusion based on ELIGSTAT and MORTSTAT\n",
    "score_mort[\"include\"] = np.where(\n",
    "    (score_mort[\"ELIGSTAT\"] == 1) & (score_mort[\"MORTSTAT\"].notna()), 1, 0\n",
    ")\n",
    "print(f\"After ELIGSTAT & MORTSTAT inclusion: {score_mort['include'].value_counts().to_dict()}\")\n",
    "\n",
    "# Step 2: Exclude missing HEI, or DR12DRST > 1 \n",
    "step2_cond = (\n",
    "    (score_mort[\"include\"] == 1) &\n",
    "    (\n",
    "        score_mort[\"HEI2015_TOTAL_SCORE\"].isna() |\n",
    "        #(score_mort[\"WTDRD1\"] <= 0) |\n",
    "        (score_mort[\"DR12DRST\"] > 1)\n",
    "    )\n",
    ")\n",
    "print(f\"Dropped at Step 2 (diet data or recall quality): {step2_cond.sum()}\")\n",
    "score_mort.loc[step2_cond, \"include\"] = 2\n",
    "\n",
    "# Step 3: Exclude missing FS, SNAP, or pir == 4\n",
    "step3_cond = (\n",
    "    (score_mort[\"include\"] == 1) &\n",
    "    (\n",
    "        score_mort[\"FS\"].isna() |\n",
    "        score_mort[\"SNAP\"].isna() |\n",
    "        (score_mort[\"pir\"] == 4)\n",
    "    )\n",
    ")\n",
    "print(f\"Dropped at Step 3 (missing FS/SNAP or pir=4): {step3_cond.sum()}\")\n",
    "score_mort.loc[step3_cond, \"include\"] = 3\n",
    "\n",
    "# Step 4: Exclude if WTDRD1 <= 0 \n",
    "step4_cond = (score_mort[\"include\"] == 1) & (score_mort[\"WTDRD1\"] <= 0)\n",
    "print(f\"Dropped at Step 4 (zero or negative WTDRD1): {step4_cond.sum()}\")\n",
    "score_mort.loc[step4_cond, \"include\"] = 4\n",
    "\n",
    "# Final count of each inclusion code\n",
    "print(f\"📊 Final include flag counts: {score_mort['include'].value_counts().to_dict()}\")\n",
    "\n",
    "# Apply final filter\n",
    "score_mort = score_mort[score_mort[\"include\"] == 1].copy()\n",
    "print(f\" Final analytic sample size: {score_mort.shape[0]}\")\n",
    "\n",
    "\n",
    "# Inclusion flag\n",
    "score_mort[\"include\"] = np.where(\n",
    "    (score_mort[\"ELIGSTAT\"] == 1) & (score_mort[\"MORTSTAT\"].notna()), 1, 0\n",
    ")\n",
    "score_mort.loc[\n",
    "    (score_mort[\"include\"] == 1) &\n",
    "    (\n",
    "        (score_mort[\"HEI2015_TOTAL_SCORE\"].isna()) |\n",
    "        (score_mort[\"WTDRD1\"] <= 0) |\n",
    "        (score_mort[\"DR12DRST\"] > 1)\n",
    "    ),\n",
    "    \"include\"\n",
    "] = 2\n",
    "score_mort.loc[\n",
    "    (score_mort[\"include\"] == 1) &\n",
    "    (\n",
    "        score_mort[\"FS\"].isna() |\n",
    "        score_mort[\"SNAP\"].isna() |\n",
    "        (score_mort[\"pir\"] == 4)\n",
    "    ),\n",
    "    \"include\"\n",
    "] = 3\n",
    "score_mort.loc[\n",
    "    (score_mort[\"include\"] == 1) & (score_mort[\"WTDRD1\"] <= 0),\n",
    "    \"include\"\n",
    "] = 4\n",
    "\n",
    "# Insurance binary\n",
    "score_mort[\"ins2\"] = np.where(score_mort[\"ins\"] == 0, 0, 1)\n",
    "\n",
    "# Unemployment indicator\n",
    "score_mort[\"unemployment2\"] = np.where(score_mort[\"employ\"] > 1, 1, 0)\n",
    "\n",
    "# Final filter to keep only those with include == 1\n",
    "score_mort = score_mort[score_mort[\"include\"] == 1].copy()\n",
    "\n",
    "# Save final dataset\n",
    "score_mort.to_pickle(os.path.join(folder_path, \"SODH_diet_mort.pkl\"))  # You can also use .csv or .parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aefee5b-6b6d-4dbf-95b5-9d44af87bf02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41941bd4-a755-4636-b600-bb425a546832",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "299cbdb9-9f4f-45ee-9d9a-ac27efde94e1",
   "metadata": {},
   "source": [
    "<h3>check final merged data </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8cf05be6-2a0e-4022-909b-d08da3900193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total rows: 38902\n",
      " Unique SEQN values: 38902\n"
     ]
    }
   ],
   "source": [
    "SODH_diet_mort = pd.read_pickle(os.path.join(folder_path, \"SODH_diet_mort.pkl\"))\n",
    "\n",
    "score_mort.to_csv(\"/Users/dengshuyue/Desktop/SDOH/analysis/data/SODH_diet_mort_depr2.csv\", index=False)\n",
    "\n",
    "# Number of rows\n",
    "total_rows = SODH_diet_mort.shape[0]\n",
    "\n",
    "# Number of unique SEQN values\n",
    "unique_ids = SODH_diet_mort['SEQN'].nunique()\n",
    "\n",
    "print(f\" Total rows: {total_rows}\")\n",
    "print(f\" Unique SEQN values: {unique_ids}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bbb9ed4d-66f8-45f5-b203-21551fd964e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Age range: 20.0 to 85.0\n"
     ]
    }
   ],
   "source": [
    "# Check age range\n",
    "min_age = score_mort[\"RIDAGEYR\"].min()\n",
    "max_age = score_mort[\"RIDAGEYR\"].max()\n",
    "\n",
    "print(f\" Age range: {min_age} to {max_age}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "10fec0a9-0c67-4437-9fdd-5e8a1231e648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SEQN', 'RIDAGEYR', 'marriage', 'SDDSRVYR', 'HOD050', 'HOQ065', 'employ', 'unemployment', 'ELIGSTAT', 'MORTSTAT', 'UCOD_LEADING', 'DIABETES', 'HYPERTEN', 'PERMTH_INT', 'PERMTH_EXM', 'Death_heart', 'Death_cancer', 'Death_resp', 'Death_cerev', 'Death_diabe', 'Death_other', 'death_cvd', 'death_cmd', 'SNAP', 'FSDHH', 'FS', 'ins', 'RIDAGEYR_covar', 'SEX', 'RACE', 'EDU', 'INDFMPIR', 'SMK_AVG', 'SMK', 'ALCG2', 'HEI2015_TOTAL_SCORE', 'DIABE', 'WTDRD1', 'WTDR2D', 'DR12DRST', 'i_FCS', 'i_Optup', 'i_HSR', 'i_nutri', 'sdmvpsu', 'sdmvstra', 'met_hr', 'perE_alco', 'dm_self', 'tchol', 'hdl', 'ldl', 'tg', 'bmi', 'CVD', 'dm_rx', 'chol_rx', 'angina_rx', 'lung_disease', 'angina', 'hba1c', 'sbp', 'dbp', 'cancer', 'wt10', 'wt', 'i_FCS_sd', 'i_Optup_sd', 'i_nutri_sd', 'i_HSR_sd', 'hei2015_sd', 'death_heart', 'death_cancer', 'death_resp', 'Death_inj', 'death_cerev', 'Death_alz', 'death_diabe', 'Death_infl', 'Death_kid', 'death_other1', 'Death_oth2', 'death_cmdk', 'death_cmdkh', 'death_multi', 'agesq', 'py', 'agestart', 'ageend', 'pir', 'bmic', 'include', 'ins2', 'unemployment2']\n"
     ]
    }
   ],
   "source": [
    "# View column names\n",
    "print(SODH_diet_mort.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63bab96-6045-4562-bc25-ac6aff06a0a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b60085-19d6-4273-9e48-498e630c7202",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca94725-3dbf-4c0f-9cc9-41fbf49f1b01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29de9496-7e60-414d-8985-17c00eb4202e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6c6adb-c130-40fa-a2b4-ba60cecd8c0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9be492-74d7-4489-9878-da7d5c791395",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dbaaf864-de3a-448d-9ab4-fc3a7147a9f7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<h1>Demographic Table</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03524dc6",
   "metadata": {},
   "source": [
    "<h2>🧮 Generate Weighted Demographic Summary Table</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14613ecd-1872-4ca4-9324-cb10cda0b4ec",
   "metadata": {},
   "source": [
    "<h3>try demo all item </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "b650ef50-71f9-4e05-b072-1013f9c5fcbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Conversion complete: .pkl → .csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the .pkl file\n",
    "pkl_path = \"/Users/dengshuyue/Desktop/SDOH/analysis/data/SODH_diet_mort.pkl\"\n",
    "df = pd.read_pickle(pkl_path)\n",
    "\n",
    "# Save as .csv\n",
    "csv_path = \"/Users/dengshuyue/Desktop/SDOH/analysis/data/SODH_diet_mort.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(\"✅ Conversion complete: .pkl → .csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "9715d92a-e1be-4bea-8d14-63827a63de26",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'linearmodels.survey'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[279]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m counts.round(\u001b[32m0\u001b[39m).astype(\u001b[38;5;28mint\u001b[39m), (props * \u001b[32m100\u001b[39m).round(\u001b[32m1\u001b[39m)\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# === Use Taylor Linearization for SEs of continuous variables ===\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlinearmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msurvey\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SurveyDesign, SurveyMean\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msurvey_mean_se\u001b[39m(df, var, weight=\u001b[33m'\u001b[39m\u001b[33mwt10\u001b[39m\u001b[33m'\u001b[39m, strata=\u001b[33m'\u001b[39m\u001b[33msdmvstra\u001b[39m\u001b[33m'\u001b[39m, cluster=\u001b[33m'\u001b[39m\u001b[33msdmvpsu\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m     27\u001b[39m     \u001b[38;5;66;03m# Remove missing\u001b[39;00m\n\u001b[32m     28\u001b[39m     d = df[[var, weight, strata, cluster]].dropna()\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'linearmodels.survey'"
     ]
    }
   ],
   "source": [
    "# Python does not support full survey design (strata + PSU) out of the box\n",
    "# below code not working \n",
    "# === Load data ===\n",
    "data_path = \"/Users/dengshuyue/Desktop/SDOH/analysis/data/\"\n",
    "df = pd.read_pickle(os.path.join(data_path, \"SODH_diet_mort.pkl\"))\n",
    "\n",
    "# Drop rows missing survey design variables\n",
    "df = df.dropna(subset=['wt10', 'sdmvstra', 'sdmvpsu'])\n",
    "\n",
    "# === Weighted proportions for categorical variables ===\n",
    "def weighted_props(df, var, weight):\n",
    "    d = df[[var, weight]].dropna()\n",
    "    counts = d.groupby(var)[weight].sum()\n",
    "    total_weight = d[weight].sum()\n",
    "    props = (counts / total_weight).round(3)\n",
    "    return counts.round(0).astype(int), (props * 100).round(1)\n",
    "\n",
    "# === Use Taylor Linearization for SEs of continuous variables ===\n",
    "from linearmodels.survey import SurveyDesign, SurveyMean\n",
    "\n",
    "def survey_mean_se(df, var, weight='wt10', strata='sdmvstra', cluster='sdmvpsu'):\n",
    "    # Remove missing\n",
    "    d = df[[var, weight, strata, cluster]].dropna()\n",
    "\n",
    "    # Define survey design\n",
    "    design = sm.survey.SurveyDesign(\n",
    "        strata=d[strata],\n",
    "        cluster=d[cluster],\n",
    "        weights=d[weight],\n",
    "    )\n",
    "\n",
    "    # Fit the design-based estimator\n",
    "    survey_var = sm.survey.SurveyMean(d[var], design)\n",
    "    return float(survey_var.mean), float(survey_var.std)\n",
    "\n",
    "# === Variable dictionaries ===\n",
    "cat_vars = {\n",
    "    'SEX': {1: 'Male', 2: 'Female'},\n",
    "    'RACE': {1: 'Non-Hispanic White', 2: 'Non-Hispanic Black', 3: 'Hispanic', 4: 'Other'},\n",
    "    'EDU': {1: 'Less than high school', 2: 'High school or equivalent', 3: 'Some college', 4: 'College or above'},\n",
    "    'pir': {1: '<1.3', 2: '1.3~2.99', 3: '>=3'},\n",
    "    'SNAP': {0: 'Not participant', 1: 'Participant', 2: 'Income eligible non-participant'},\n",
    "    'SMK': {1: 'Nonsmokers', 2: 'Former smokers', 3: '<15 cigarettes/day', 4: '15-24.9 cigarettes/day', 5: '≥ 25 cigarettes/day'},\n",
    "    'ALCG2': {1: 'Nondrinkers', 2: 'Moderate drinker', 3: 'Heavy drinker', 4: 'Missing'},\n",
    "    'bmic': {1: 'BMI <18.5', 2: '18-24.9', 3: '25-29.9', 4: 'BMI ≥30'}\n",
    "}\n",
    "\n",
    "binary_vars = ['DIABETES', 'CVD', 'dm_rx', 'chol_rx', 'angina', 'cancer', 'lung_disease', 'MORTSTAT']\n",
    "cont_vars = ['RIDAGEYR', 'met_hr', 'bmi', 'hba1c', 'sbp', 'dbp', 'hdl', 'ldl', 'tg', 'HEI2015_TOTAL_SCORE']\n",
    "\n",
    "# === Generate summary ===\n",
    "rows = []\n",
    "\n",
    "# Categorical\n",
    "for var, mapping in cat_vars.items():\n",
    "    counts, props = weighted_props(df, var, 'wt10')\n",
    "    for code, label in mapping.items():\n",
    "        if code in counts.index:\n",
    "            rows.append({\n",
    "                \"Variable\": var,\n",
    "                \"Category\": label,\n",
    "                \"Overall\": f\"{counts[code]:,.0f} ({props[code]}%)\"\n",
    "            })\n",
    "\n",
    "# Binary (weighted %)\n",
    "for var in binary_vars:\n",
    "    val, se = survey_mean_se(df, var)\n",
    "    rows.append({\n",
    "        \"Variable\": var,\n",
    "        \"Category\": \"1\",\n",
    "        \"Overall\": f\"{val * 100:.1f}% ({se * 100:.1f})\"\n",
    "    })\n",
    "\n",
    "# Continuous (mean ± SE)\n",
    "for var in cont_vars:\n",
    "    mean, se = survey_mean_se(df, var)\n",
    "    rows.append({\n",
    "        \"Variable\": var,\n",
    "        \"Category\": \"\",\n",
    "        \"Overall\": f\"{mean:.2f} ({se:.2f})\"\n",
    "    })\n",
    "\n",
    "# === Create summary table ===\n",
    "demo_table = pd.DataFrame(rows)\n",
    "\n",
    "# === Save and preview ===\n",
    "output_path = os.path.join(data_path, \"demo_summary.csv\")\n",
    "demo_table.to_csv(output_path, index=False)\n",
    "\n",
    "# Show preview\n",
    "demo_table.head(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "01a00334-ef08-4a43-9926-9c7e2a0c18c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable</th>\n",
       "      <th>Category</th>\n",
       "      <th>Overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SEX</td>\n",
       "      <td>Male</td>\n",
       "      <td>82,147,536 (47.8%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SEX</td>\n",
       "      <td>Female</td>\n",
       "      <td>89,889,015 (52.2%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RACE</td>\n",
       "      <td>Non-Hispanic White</td>\n",
       "      <td>117,066,666 (68.0%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RACE</td>\n",
       "      <td>Non-Hispanic Black</td>\n",
       "      <td>19,270,383 (11.2%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RACE</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>23,166,597 (13.5%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RACE</td>\n",
       "      <td>Other</td>\n",
       "      <td>12,532,906 (7.3%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>EDU</td>\n",
       "      <td>Less than high school</td>\n",
       "      <td>27,365,333 (15.9%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>EDU</td>\n",
       "      <td>High school or equivalent</td>\n",
       "      <td>41,028,422 (23.9%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>EDU</td>\n",
       "      <td>Some college</td>\n",
       "      <td>54,476,932 (31.7%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>EDU</td>\n",
       "      <td>College or above</td>\n",
       "      <td>49,075,426 (28.5%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>pir</td>\n",
       "      <td>&lt;1.3</td>\n",
       "      <td>35,288,368 (20.5%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>pir</td>\n",
       "      <td>1.3~2.99</td>\n",
       "      <td>46,559,275 (27.1%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>pir</td>\n",
       "      <td>&gt;=3</td>\n",
       "      <td>80,918,834 (47.0%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>SNAP</td>\n",
       "      <td>Not participant</td>\n",
       "      <td>127,698,889 (74.2%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>SNAP</td>\n",
       "      <td>Participant</td>\n",
       "      <td>24,672,009 (14.3%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>SNAP</td>\n",
       "      <td>Income eligible non-participant</td>\n",
       "      <td>19,665,654 (11.4%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>SMK</td>\n",
       "      <td>Nonsmokers</td>\n",
       "      <td>93,221,980 (54.2%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>SMK</td>\n",
       "      <td>Former smokers</td>\n",
       "      <td>42,940,949 (25.0%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>SMK</td>\n",
       "      <td>&lt;15 cigarettes/day</td>\n",
       "      <td>21,030,364 (12.2%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>SMK</td>\n",
       "      <td>15-24.9 cigarettes/day</td>\n",
       "      <td>10,939,898 (6.4%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>SMK</td>\n",
       "      <td>≥ 25 cigarettes/day</td>\n",
       "      <td>3,836,110 (2.2%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ALCG2</td>\n",
       "      <td>Nondrinkers</td>\n",
       "      <td>69,265,625 (40.3%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ALCG2</td>\n",
       "      <td>Moderate drinker</td>\n",
       "      <td>80,514,401 (46.8%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ALCG2</td>\n",
       "      <td>Heavy drinker</td>\n",
       "      <td>13,782,467 (8.0%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ALCG2</td>\n",
       "      <td>Missing</td>\n",
       "      <td>8,474,059 (4.9%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>bmic</td>\n",
       "      <td>BMI &lt;18.5</td>\n",
       "      <td>2,736,947 (1.6%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>bmic</td>\n",
       "      <td>18-24.9</td>\n",
       "      <td>56,978,492 (33.1%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>bmic</td>\n",
       "      <td>25-29.9</td>\n",
       "      <td>63,146,301 (36.7%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>DIABETES</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>CVD</td>\n",
       "      <td>1</td>\n",
       "      <td>8.2%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>dm_rx</td>\n",
       "      <td>1</td>\n",
       "      <td>8.5%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>chol_rx</td>\n",
       "      <td>1</td>\n",
       "      <td>17.9%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>angina</td>\n",
       "      <td>1</td>\n",
       "      <td>4.9%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>cancer</td>\n",
       "      <td>1</td>\n",
       "      <td>10.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>lung_disease</td>\n",
       "      <td>1</td>\n",
       "      <td>19.3%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>MORTSTAT</td>\n",
       "      <td>1</td>\n",
       "      <td>9.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>RIDAGEYR</td>\n",
       "      <td></td>\n",
       "      <td>47.44 (0.13)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>met_hr</td>\n",
       "      <td></td>\n",
       "      <td>62.34 (0.76)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>bmi</td>\n",
       "      <td></td>\n",
       "      <td>28.94 (0.05)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>hba1c</td>\n",
       "      <td></td>\n",
       "      <td>5.60 (0.01)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>sbp</td>\n",
       "      <td></td>\n",
       "      <td>122.60 (0.13)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>dbp</td>\n",
       "      <td></td>\n",
       "      <td>70.61 (0.09)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>hdl</td>\n",
       "      <td></td>\n",
       "      <td>53.65 (0.12)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>ldl</td>\n",
       "      <td></td>\n",
       "      <td>117.40 (0.32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>tg</td>\n",
       "      <td></td>\n",
       "      <td>121.52 (0.78)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>HEI2015_TOTAL_SCORE</td>\n",
       "      <td></td>\n",
       "      <td>53.26 (0.10)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Variable                         Category              Overall\n",
       "0                   SEX                             Male   82,147,536 (47.8%)\n",
       "1                   SEX                           Female   89,889,015 (52.2%)\n",
       "2                  RACE               Non-Hispanic White  117,066,666 (68.0%)\n",
       "3                  RACE               Non-Hispanic Black   19,270,383 (11.2%)\n",
       "4                  RACE                         Hispanic   23,166,597 (13.5%)\n",
       "5                  RACE                            Other    12,532,906 (7.3%)\n",
       "6                   EDU            Less than high school   27,365,333 (15.9%)\n",
       "7                   EDU        High school or equivalent   41,028,422 (23.9%)\n",
       "8                   EDU                     Some college   54,476,932 (31.7%)\n",
       "9                   EDU                 College or above   49,075,426 (28.5%)\n",
       "10                  pir                             <1.3   35,288,368 (20.5%)\n",
       "11                  pir                         1.3~2.99   46,559,275 (27.1%)\n",
       "12                  pir                              >=3   80,918,834 (47.0%)\n",
       "13                 SNAP                  Not participant  127,698,889 (74.2%)\n",
       "14                 SNAP                      Participant   24,672,009 (14.3%)\n",
       "15                 SNAP  Income eligible non-participant   19,665,654 (11.4%)\n",
       "16                  SMK                       Nonsmokers   93,221,980 (54.2%)\n",
       "17                  SMK                   Former smokers   42,940,949 (25.0%)\n",
       "18                  SMK               <15 cigarettes/day   21,030,364 (12.2%)\n",
       "19                  SMK           15-24.9 cigarettes/day    10,939,898 (6.4%)\n",
       "20                  SMK              ≥ 25 cigarettes/day     3,836,110 (2.2%)\n",
       "21                ALCG2                      Nondrinkers   69,265,625 (40.3%)\n",
       "22                ALCG2                 Moderate drinker   80,514,401 (46.8%)\n",
       "23                ALCG2                    Heavy drinker    13,782,467 (8.0%)\n",
       "24                ALCG2                          Missing     8,474,059 (4.9%)\n",
       "25                 bmic                        BMI <18.5     2,736,947 (1.6%)\n",
       "26                 bmic                          18-24.9   56,978,492 (33.1%)\n",
       "27                 bmic                          25-29.9   63,146,301 (36.7%)\n",
       "28             DIABETES                                1                 1.0%\n",
       "29                  CVD                                1                 8.2%\n",
       "30                dm_rx                                1                 8.5%\n",
       "31              chol_rx                                1                17.9%\n",
       "32               angina                                1                 4.9%\n",
       "33               cancer                                1                10.1%\n",
       "34         lung_disease                                1                19.3%\n",
       "35             MORTSTAT                                1                 9.1%\n",
       "36             RIDAGEYR                                          47.44 (0.13)\n",
       "37               met_hr                                          62.34 (0.76)\n",
       "38                  bmi                                          28.94 (0.05)\n",
       "39                hba1c                                           5.60 (0.01)\n",
       "40                  sbp                                         122.60 (0.13)\n",
       "41                  dbp                                          70.61 (0.09)\n",
       "42                  hdl                                          53.65 (0.12)\n",
       "43                  ldl                                         117.40 (0.32)\n",
       "44                   tg                                         121.52 (0.78)\n",
       "45  HEI2015_TOTAL_SCORE                                          53.26 (0.10)"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# === Load data ===\n",
    "df = pd.read_pickle(os.path.join(data_path, \"SODH_diet_mort.pkl\"))\n",
    "\n",
    "# Drop missing survey design variables (weight pooled: WTDRD1)\n",
    "df = df.dropna(subset=['wt10', 'sdmvstra', 'sdmvpsu'])\n",
    "\n",
    "# === Helper functions ===\n",
    "def weighted_mean(x, w):\n",
    "    d = pd.DataFrame({'x': x, 'w': w}).dropna()\n",
    "    return np.sum(d.x * d.w) / np.sum(d.w)\n",
    "\n",
    "# not standard \n",
    "# def weighted_se(x, w):\n",
    "#    d = pd.DataFrame({'x': x, 'w': w}).dropna()\n",
    "#    mean = weighted_mean(d.x, d.w)\n",
    "#    return np.sqrt(np.sum(d.w * (d.x - mean)**2) / ((len(d) - 1) * np.sum(d.w) / len(d)))\n",
    "\n",
    "# Only acceptable if weights are uniform\n",
    "# def weighted_se(x, w):\n",
    "#    d = pd.DataFrame({'x': x, 'w': w}).dropna()\n",
    "#    mean = np.sum(d.x * d.w) / np.sum(d.w)\n",
    "#    var = np.sum(d.w * (d.x - mean)**2) / np.sum(d.w)\n",
    "#    se = np.sqrt(var / len(d))  # Approximate\n",
    "#    return se\n",
    "\n",
    "# 🔥 Check best practice for estimating SE with survey weights\n",
    "\n",
    "def weighted_se(x, w):\n",
    "    d = pd.DataFrame({'x': x, 'w': w}).dropna()\n",
    "    mean = np.sum(d.x * d.w) / np.sum(d.w)\n",
    "    eff_n = (np.sum(d.w))**2 / np.sum(d.w**2)  # Effective sample size\n",
    "    var = np.sum(d.w * (d.x - mean)**2) / np.sum(d.w)\n",
    "    se = np.sqrt(var / eff_n)\n",
    "    return se\n",
    "\n",
    "    \n",
    "def weighted_props(df, var, weight):\n",
    "    d = df[[var, weight]].dropna()\n",
    "    counts = d.groupby(var)[weight].sum()\n",
    "    total_weight = d[weight].sum()\n",
    "    props = (counts / total_weight).round(3)\n",
    "    return counts.round(0).astype(int), (props * 100).round(1)\n",
    "\n",
    "# === Categorical variables ===\n",
    "cat_vars = {\n",
    "    'SEX': {1: 'Male', 2: 'Female'},\n",
    "    'RACE': {1: 'Non-Hispanic White', 2: 'Non-Hispanic Black', 3: 'Hispanic', 4: 'Other'},\n",
    "    'EDU': {1: 'Less than high school', 2: 'High school or equivalent', 3: 'Some college', 4: 'College or above'},\n",
    "    'pir': {1: '<1.3', 2: '1.3~2.99', 3: '>=3'},\n",
    "    'SNAP': {0: 'Not participant', 1: 'Participant', 2: 'Income eligible non-participant'},\n",
    "    'SMK': {1: 'Nonsmokers', 2: 'Former smokers', 3: '<15 cigarettes/day', 4: '15-24.9 cigarettes/day', 5: '≥ 25 cigarettes/day'},\n",
    "    'ALCG2': {1: 'Nondrinkers', 2: 'Moderate drinker', 3: 'Heavy drinker', 4: 'Missing'},\n",
    "    'bmic': {1: 'BMI <18.5', 2: '18-24.9', 3: '25-29.9', 4: 'BMI ≥30'}\n",
    "}\n",
    "\n",
    "rows = []\n",
    "\n",
    "for var, mapping in cat_vars.items():\n",
    "    counts, props = weighted_props(df, var, 'wt10')\n",
    "    for code, label in mapping.items():\n",
    "        if code in counts.index:\n",
    "            rows.append({\n",
    "                \"Variable\": var,\n",
    "                \"Category\": label,\n",
    "                \"Overall\": f\"{counts[code]:,.0f} ({props[code]}%)\"\n",
    "            })\n",
    "\n",
    "# === Binary variables ===\n",
    "binary_vars = ['DIABETES', 'CVD', 'dm_rx', 'chol_rx', 'angina', 'cancer', 'lung_disease', 'MORTSTAT']\n",
    "for var in binary_vars:\n",
    "    val = weighted_mean(df[var].fillna(0), df['wt10']) * 100\n",
    "    rows.append({\n",
    "        \"Variable\": var,\n",
    "        \"Category\": \"1\",\n",
    "        \"Overall\": f\"{val:.1f}%\"\n",
    "    })\n",
    "\n",
    "# === Continuous variables ===\n",
    "cont_vars = ['RIDAGEYR', 'met_hr', 'bmi', 'hba1c', 'sbp', 'dbp', 'hdl', 'ldl', 'tg', 'HEI2015_TOTAL_SCORE']\n",
    "for var in cont_vars:\n",
    "    mean = weighted_mean(df[var], df['wt10'])\n",
    "    se = weighted_se(df[var], df['wt10'])\n",
    "    rows.append({\n",
    "        \"Variable\": var,\n",
    "        \"Category\": \"\",\n",
    "        \"Overall\": f\"{mean:.2f} ({se:.2f})\"\n",
    "    })\n",
    "\n",
    "# === Create summary table ===\n",
    "demo_table = pd.DataFrame(rows)\n",
    "\n",
    "# Save, preview\n",
    "\n",
    "# Optionally: save to CSV\n",
    "# demo_table.to_csv(\"demo_summary.csv\", index=False)\n",
    "demo_table.to_csv(output_path, index=False)\n",
    "\n",
    "demo_table.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "b12bfb40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted percentage deceased: 9.1%\n"
     ]
    }
   ],
   "source": [
    "# Clean data: drop rows with missing mortality status or weight\n",
    "mort_df = df[['MORTSTAT', 'WTDRD1']].dropna()\n",
    "\n",
    "# Calculate total weighted sum\n",
    "total_weight = mort_df['WTDRD1'].sum()\n",
    "\n",
    "# Weighted percentage of deceased (MORTSTAT == 1)\n",
    "dead_weight = mort_df.loc[mort_df['MORTSTAT'] == 1, 'WTDRD1'].sum()\n",
    "weighted_pct_dead = (dead_weight / total_weight) * 100\n",
    "\n",
    "print(f\"Weighted percentage deceased: {weighted_pct_dead:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eae26a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Male: 18845 (48.4%)\n",
      "Female: 20055 (51.6%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "38420"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define unweighted counts and proportions\n",
    "def unweighted_props(df, var):\n",
    "    counts = df[var].value_counts(dropna=False).sort_index()\n",
    "    props = counts / counts.sum() * 100\n",
    "    return counts, props.round(1)\n",
    "\n",
    "# Now use it for 'sex'\n",
    "counts, props = unweighted_props(df, 'sex')\n",
    "\n",
    "# Optionally map codes to labels\n",
    "sex_labels = {1: 'Male', 2: 'Female'}\n",
    "for val in counts.index:\n",
    "    label = sex_labels.get(val, val)\n",
    "    print(f\"{label}: {counts[val]} ({props[val]}%)\")\n",
    "18530+19890\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9eae91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c1062d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63741b2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ed50c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531b7645",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2440f2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/dengshuyue/Desktop/SDOH/analysis/code/Ref/2.Prepare data_master.sas', 'r', encoding='latin1') as f:\n",
    "    sas_code = f.read()\n",
    "\n",
    "print(sas_code[:20000])  # Preview the first X,000 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4e5a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/dengshuyue/Desktop/SDOH/analysis/code/Ref/2.1_Prepare data_covariates.sas', 'r', encoding='latin1') as f:\n",
    "    sas_code = f.read()\n",
    "\n",
    "print(sas_code[:20000])  # Preview the first X,000 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1742324",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/dengshuyue/Desktop/SDOH/analysis/code/Descriptions_1.sas', 'r', encoding='latin1') as f:\n",
    "    sas_code = f.read()\n",
    "\n",
    "print(sas_code[:20000])  # Preview the first 15,000 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9704426b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diabetes Identification Note:\n",
    "# The final diabetes indicator variable used in the analysis is 'diabe2'.\n",
    "# This composite variable identifies diabetes based on the following criteria:\n",
    "# - Self-reported physician diagnosis (DIQ010), current insulin use (DIQ050), or oral medication use (DIQ070)\n",
    "# - Prescription drug data indicating diabetes treatment (dm_rx2)\n",
    "# - Fasting glucose ≥ 126 mg/dL (glu_dm)\n",
    "# - OGTT ≥ 200 mg/dL (ogtt_dm)\n",
    "# - HbA1c ≥ 6.5% (hb_dm)\n",
    "# Any one of these criteria being met will set 'diabe2' = 1, otherwise 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4508de7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/dengshuyue/Desktop/SDOH/analysis/Analysis1_COX_allcause_bysub.sas', 'r', encoding='latin1') as f:\n",
    "    sas_code = f.read()\n",
    "\n",
    "print(sas_code[:10000])  # Preview the first 15,000 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bcc94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/dengshuyue/Desktop/SDOH/analysis/Analysis1_dataprep.sas', 'r', encoding='latin1') as f:\n",
    "    sas_code = f.read()\n",
    "\n",
    "print(sas_code[:20000])  # Preview the first 15,000 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58be0118",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab19022e-5d46-4163-88b5-58f23781e962",
   "metadata": {},
   "source": [
    "<h1>SDOH score</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e7542aa-9952-439b-afe6-f53d66af6b1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HOQ065</th>\n",
       "      <th>HOD050</th>\n",
       "      <th>EMPLOY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38895</th>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38896</th>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38897</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38898</th>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38899</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38900 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       HOQ065  HOD050  EMPLOY\n",
       "0         1.0     6.0     1.0\n",
       "1         1.0     3.0     2.0\n",
       "2         1.0     7.0     3.0\n",
       "3         1.0     6.0     3.0\n",
       "4         2.0     3.0     1.0\n",
       "...       ...     ...     ...\n",
       "38895     3.0     8.0     1.0\n",
       "38896     1.0     6.0     3.0\n",
       "38897     2.0     1.0     1.0\n",
       "38898     2.0     8.0     2.0\n",
       "38899     1.0     9.0     4.0\n",
       "\n",
       "[38900 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see housing and employ\n",
    "df[['HOQ065', 'HOD050', 'EMPLOY']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "508bb06c-1993-4a1f-ad30-df81712d139c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== HOQ065 Summary ===\n",
      "Value counts:\n",
      "HOQ065\n",
      "1.0    24162\n",
      "2.0    13779\n",
      "3.0      907\n",
      "NaN       52\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Proportions:\n",
      "HOQ065\n",
      "1.0    62.11\n",
      "2.0    35.42\n",
      "3.0     2.33\n",
      "NaN     0.13\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Missing values: 52\n",
      "\n",
      "=== HOD050 Summary ===\n",
      "Value counts:\n",
      "HOD050\n",
      "5.0      8034\n",
      "6.0      7469\n",
      "4.0      6416\n",
      "7.0      5175\n",
      "8.0      3352\n",
      "3.0      3193\n",
      "9.0      1908\n",
      "10.0     1122\n",
      "2.0       820\n",
      "11.0      476\n",
      "12.0      285\n",
      "1.0       281\n",
      "13.0      260\n",
      "999.0      43\n",
      "777.0      39\n",
      "NaN        27\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Proportions:\n",
      "HOD050\n",
      "5.0      20.65\n",
      "6.0      19.20\n",
      "4.0      16.49\n",
      "7.0      13.30\n",
      "8.0       8.62\n",
      "3.0       8.21\n",
      "9.0       4.90\n",
      "10.0      2.88\n",
      "2.0       2.11\n",
      "11.0      1.22\n",
      "12.0      0.73\n",
      "1.0       0.72\n",
      "13.0      0.67\n",
      "999.0     0.11\n",
      "777.0     0.10\n",
      "NaN       0.07\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Missing values: 27\n",
      "\n",
      "=== EMPLOY Summary ===\n",
      "Value counts:\n",
      "EMPLOY\n",
      "1.0    20703\n",
      "3.0     7765\n",
      "5.0     4323\n",
      "4.0     3634\n",
      "2.0     1594\n",
      "NaN      881\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Proportions:\n",
      "EMPLOY\n",
      "1.0    53.22\n",
      "3.0    19.96\n",
      "5.0    11.11\n",
      "4.0     9.34\n",
      "2.0     4.10\n",
      "NaN     2.26\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Missing values: 881\n"
     ]
    }
   ],
   "source": [
    "# List of selected SDOH-related variables\n",
    "selected_cols = ['HOQ065', 'HOD050', 'MOR']\n",
    "\n",
    "# Summary function for each variable\n",
    "for col in selected_cols:\n",
    "    print(f\"\\n=== {col} Summary ===\")\n",
    "    print(\"Value counts:\")\n",
    "    print(df[col].value_counts(dropna=False))\n",
    "    print(\"\\nProportions:\")\n",
    "    print((df[col].value_counts(normalize=True, dropna=False) * 100).round(2))\n",
    "    print(f\"\\nMissing values: {df[col].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97dc217-6b45-4c00-b48f-d933c0aab82f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
