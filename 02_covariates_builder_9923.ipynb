{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d162ff1",
   "metadata": {},
   "source": [
    "\n",
    "# NHANES 1999–2023 Covariates Builder (Revised)\n",
    "\n",
    "This notebook builds the following files into your `CONFIG.out_dir`:\n",
    "\n",
    "- `cov_smk_1999_2023.parquet`\n",
    "- `cov_alc_1999_2023.parquet`\n",
    "- `cov_pa_1999_2023.parquet`\n",
    "- `cov_bmx_1999_2023.parquet`\n",
    "- `cov_clinical_1999_2023.parquet`\n",
    "- `cov_household_1999_2023.parquet`\n",
    "- `cov_core_1999_2023.parquet`\n",
    "\n",
    "**Key fixes vs. your previous version**  \n",
    "- The PA builder writes only `SEQN, LTPA, METSCORE, IMP` → prevents `_X/_Y` duplicates in CORE.  \n",
    "- The CORE merge explicitly restricts columns from each piece, ensuring clean output.  \n",
    "- `METSCORE` now flows correctly into CORE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95d53c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 200)\n",
    "\n",
    "# --- Base project folder (adjust if needed) ---\n",
    "BASE = Path(\"/Users/dengshuyue/Desktop/SDOH/analysis\")\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    raw_dir: Path = BASE / \"data\"\n",
    "    interim_dir: Path = BASE / \"data\" / \"cov\"\n",
    "    out_dir: Path = BASE / \"output\"\n",
    "\n",
    "    # Preferred inputs (set demo_9923 to your stack)\n",
    "    demo_9923: Path = BASE / \"data\" / \"cov\" / \"demo9923.parquet\"\n",
    "    demo_9918: Optional[Path] = None\n",
    "\n",
    "    # Optional preferred sources (leave None if you want auto-discovery)\n",
    "    bmx_9923: Optional[Path] = None\n",
    "    smk_9918: Optional[Path] = None\n",
    "    smk_1923: Optional[Path] = None\n",
    "    pa_9918_imputed: Optional[Path] = None\n",
    "    pa_1923: Optional[Path] = None\n",
    "    clinical_9918: Optional[Path] = None\n",
    "    clinical_1923: Optional[Path] = None\n",
    "\n",
    "    # Output file names (fixed 1999–2023 names)\n",
    "    cov_smk: str       = \"cov_smk_1999_2023.parquet\"\n",
    "    cov_alc: str       = \"cov_alc_1999_2023.parquet\"\n",
    "    cov_pa: str        = \"cov_pa_1999_2023.parquet\"\n",
    "    cov_bmx: str       = \"cov_bmx_1999_2023.parquet\"\n",
    "    cov_clinical: str  = \"cov_clinical_1999_2023.parquet\"\n",
    "    cov_household: str = \"cov_household_1999_2023.parquet\"\n",
    "    cov_core: str      = \"cov_core_1999_2023.parquet\"\n",
    "\n",
    "CONFIG = Config()\n",
    "\n",
    "# --- helpers ---\n",
    "NHANES_MISS = {7, 9, 77, 99, 777, 999, 7777, 9999, 77777, 99999}\n",
    "\n",
    "def log(msg: str) -> None:\n",
    "    print(msg, flush=True)\n",
    "\n",
    "def ensure_dir(p: Path | str) -> None:\n",
    "    Path(p).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def upper_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    d = df.copy()\n",
    "    d.columns = [c.upper() for c in d.columns]\n",
    "    return d\n",
    "\n",
    "def nhanes_na(s: pd.Series) -> pd.Series:\n",
    "    return pd.to_numeric(s, errors=\"coerce\").mask(lambda x: x.isin(NHANES_MISS))\n",
    "\n",
    "def read_any(p: Path) -> pd.DataFrame:\n",
    "    p = Path(p)\n",
    "    return pd.read_parquet(p) if p.suffix.lower()==\".parquet\" else pd.read_csv(p, low_memory=False)\n",
    "\n",
    "def pick_first_existing(*cands: Optional[Path]) -> Optional[Path]:\n",
    "    for c in cands:\n",
    "        if c and Path(c).exists():\n",
    "            return Path(c)\n",
    "    return None\n",
    "\n",
    "def norm_seqn(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"SEQN\"] = pd.to_numeric(df[\"SEQN\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f724ce67-0240-4f1f-bd38-e7284d79d1d3",
   "metadata": {},
   "source": [
    "## SMK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97609322",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SMK_STATUS_CATS = pd.CategoricalDtype([\"NEVER\", \"FORMER\", \"CURRENT\"], ordered=True)\n",
    "\n",
    "def build_smk(cfg: Config = CONFIG) -> pd.DataFrame:\n",
    "    \"\"\"Build smoking covariates with tolerant inputs.\"\"\"\n",
    "    ensure_dir(cfg.out_dir)\n",
    "\n",
    "    def _read_any(p: Path) -> pd.DataFrame:\n",
    "        return pd.read_parquet(p) if str(p).endswith(\".parquet\") else pd.read_csv(p, low_memory=False)\n",
    "\n",
    "    # 1) Locate sources (prefer combined 99–23; else 99–18 (+ optional 19–23))\n",
    "    smk_9923 = pick_first_existing(\n",
    "        cfg.interim_dir / \"smk_9923.parquet\",\n",
    "        cfg.interim_dir / \"smk_9923.csv\",\n",
    "    )\n",
    "    if smk_9923:\n",
    "        smk = upper_df(_read_any(smk_9923))\n",
    "        src_msg = f\"using {smk_9923.name}\"\n",
    "    else:\n",
    "        p9918 = pick_first_existing(cfg.smk_9918, cfg.interim_dir / \"smk_9918.parquet\", cfg.interim_dir / \"smk_9918.csv\")\n",
    "        p1923 = pick_first_existing(cfg.smk_1923, cfg.interim_dir / \"smk_1923.parquet\", cfg.interim_dir / \"smk_1923.csv\")\n",
    "        if p9918 is None:\n",
    "            raise FileNotFoundError(\"Provide smk_9923 or smk_9918 (csv/parquet) under interim.\")\n",
    "        smk = upper_df(_read_any(p9918))\n",
    "        if p1923:\n",
    "            smk = pd.concat([smk, upper_df(_read_any(p1923))], ignore_index=True)\n",
    "            src_msg = f\"using {Path(p9918).name} + {Path(p1923).name}\"\n",
    "        else:\n",
    "            src_msg = f\"using {Path(p9918).name}\"\n",
    "\n",
    "    # 2) If standardized columns already exist, just use them\n",
    "    needed = {\"SEQN\", \"SMK_STATUS\", \"CIGS_PER_DAY\", \"PACK_YEARS\", \"FORMER_SMOKER\"}\n",
    "    have_std = needed.issubset(set(smk.columns))\n",
    "\n",
    "    if not have_std:\n",
    "        d = smk.copy()\n",
    "        for junk in [\"UNNAMED: 0\", \"INDEX\"]:\n",
    "            if junk in d.columns:\n",
    "                d = d.drop(columns=[junk])\n",
    "\n",
    "        out = pd.DataFrame({\"SEQN\": d[\"SEQN\"]})\n",
    "\n",
    "        # SMK_STATUS\n",
    "        if \"SMK_STATUS\" in d.columns:\n",
    "            smk_status = d[\"SMK_STATUS\"].astype(\"string\").str.strip().str.upper()\n",
    "        else:\n",
    "            smk_num = pd.to_numeric(d.get(\"SMK\"), errors=\"coerce\")\n",
    "            smk_status = smk_num.map({1: \"NEVER\", 2: \"FORMER\", 3: \"CURRENT\"}).astype(\"string\")\n",
    "            if smk_status.isna().all() and ((\"SMQ020\" in d.columns) or (\"SMQ040\" in d.columns)):\n",
    "                ever = d.get(\"SMQ020\")\n",
    "                if ever is not None:\n",
    "                    ever = pd.to_numeric(ever, errors=\"coerce\").replace({2: 0, 1: 1})\n",
    "                smq040 = pd.to_numeric(d.get(\"SMQ040\"), errors=\"coerce\")\n",
    "                smk_status = pd.Series(\"NEVER\", index=d.index, dtype=\"string\")\n",
    "                if ever is not None:\n",
    "                    smk_status = smk_status.mask(ever == 1, \"FORMER\")\n",
    "                if smq040 is not None:\n",
    "                    smk_status = smk_status.mask(smq040.isin([1, 2]), \"CURRENT\")\n",
    "                    smk_status = smk_status.mask(smq040 == 3, \"FORMER\")\n",
    "\n",
    "        out[\"SMK_STATUS\"] = smk_status.astype(\"string\").str.upper()\n",
    "\n",
    "        # CIGS_PER_DAY\n",
    "        cigs = pd.to_numeric(d.get(\"SMK_AVG\"), errors=\"coerce\")\n",
    "        if cigs.isna().all():\n",
    "            q = pd.to_numeric(d.get(\"SMQ050Q\"), errors=\"coerce\")\n",
    "            u = pd.to_numeric(d.get(\"SMQ050U\"), errors=\"coerce\")\n",
    "            if q is not None and u is not None:\n",
    "                cigs = q.where(u == 1).fillna((q / 7.0).where(u == 2)).fillna((q / 30.0).where(u == 3))\n",
    "        out[\"CIGS_PER_DAY\"] = cigs\n",
    "\n",
    "        # PACK_YEARS\n",
    "        pack_years = pd.to_numeric(d.get(\"PACK_YR\"), errors=\"coerce\")\n",
    "        if pack_years.isna().all():\n",
    "            years = pd.to_numeric(d.get(\"SMK_YR\"), errors=\"coerce\")\n",
    "            if years is None or years.isna().all():\n",
    "                years = pd.to_numeric(d.get(\"SMD030\"), errors=\"coerce\")\n",
    "            pack_years = (out[\"CIGS_PER_DAY\"] / 20.0) * years\n",
    "        out[\"PACK_YEARS\"] = pack_years\n",
    "\n",
    "        # FORMER_SMOKER\n",
    "        out[\"FORMER_SMOKER\"] = out[\"SMK_STATUS\"].eq(\"FORMER\").fillna(False).astype(\"int8\")\n",
    "\n",
    "        out.loc[out[\"CIGS_PER_DAY\"] < 0, \"CIGS_PER_DAY\"] = np.nan\n",
    "        out.loc[out[\"PACK_YEARS\"] < 0, \"PACK_YEARS\"] = np.nan\n",
    "\n",
    "        smk_std = out\n",
    "    else:\n",
    "        smk_std = smk[list(needed)].copy()\n",
    "        smk_std[\"SMK_STATUS\"] = smk_std[\"SMK_STATUS\"].astype(\"string\").str.upper()\n",
    "        smk_std[\"FORMER_SMOKER\"] = pd.to_numeric(smk_std[\"FORMER_SMOKER\"], errors=\"coerce\").fillna(0).astype(\"int8\")\n",
    "\n",
    "    smk_std = smk_std.drop_duplicates(\"SEQN\")\n",
    "    smk_std[\"SMK_STATUS\"] = smk_std[\"SMK_STATUS\"].astype(\"category\").cat.set_categories(SMK_STATUS_CATS.categories, ordered=True)\n",
    "    outp = cfg.out_dir / cfg.cov_smk\n",
    "    smk_std.to_parquet(outp, index=False)\n",
    "\n",
    "    miss_rate = smk_std[[\"SMK_STATUS\", \"CIGS_PER_DAY\", \"PACK_YEARS\"]].isna().mean().round(3).to_dict()\n",
    "    log(f\"✓ SMK → {outp} ({src_msg}); missing: {miss_rate}\")\n",
    "    return smk_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af488487",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------- Alcohol builder with optional CDC fetch (revised) ----------\n",
    "import pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def _read_any(p: Path) -> pd.DataFrame:\n",
    "    p = Path(p)\n",
    "    if p.suffix.lower() == \".parquet\":\n",
    "        return pd.read_parquet(p)\n",
    "    return pd.read_csv(p, low_memory=False)\n",
    "\n",
    "def _clean_num(s: pd.Series) -> pd.Series:\n",
    "    NH_MISS = {7, 9, 77, 99, 777, 999, 7777, 9999, 77777, 99999}\n",
    "    s = pd.to_numeric(s, errors=\"coerce\")\n",
    "    return s.mask(s.isin(NH_MISS))\n",
    "\n",
    "def _download(url: str, dest: Path, timeout=90):\n",
    "    import requests\n",
    "    dest.parent.mkdir(parents=True, exist_ok=True)\n",
    "    headers = {\"User-Agent\": \"nhanes-fetch/1.0\"}\n",
    "    with requests.get(url, headers=headers, stream=True, timeout=timeout) as r:\n",
    "        r.raise_for_status()\n",
    "        tmp = dest.with_suffix(dest.suffix + \".downloading\")\n",
    "        with open(tmp, \"wb\") as f:\n",
    "            for chunk in r.iter_content(1 << 15):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "        tmp.rename(dest)\n",
    "    return dest\n",
    "\n",
    "def _read_xpt(p: Path) -> pd.DataFrame:\n",
    "    try:\n",
    "        import pyreadstat\n",
    "        df, _ = pyreadstat.read_xport(p)\n",
    "    except Exception:\n",
    "        df = pd.read_sas(p, format=\"xport\")\n",
    "    df.columns = [c.upper() for c in df.columns]\n",
    "    return df\n",
    "\n",
    "def _ensure_demo_sex(cfg) -> pd.DataFrame:\n",
    "    demo_path = None\n",
    "    for cand in [\n",
    "        getattr(cfg, \"demo_9923\", None),\n",
    "        getattr(cfg, \"demo_9918\", None),\n",
    "        cfg.interim_dir / \"demo_9923.parquet\",\n",
    "        cfg.interim_dir / \"demo_9918.parquet\",\n",
    "    ]:\n",
    "        if cand and Path(cand).exists():\n",
    "            demo_path = Path(cand); break\n",
    "    if demo_path:\n",
    "        demo = _read_any(demo_path)\n",
    "        demo = upper_df(demo)\n",
    "        if \"RIAGENDR\" in demo.columns:\n",
    "            return demo[[\"SEQN\", \"RIAGENDR\"]].drop_duplicates(\"SEQN\")\n",
    "\n",
    "    DEMO_URLS = {\n",
    "        \"1999-2000\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/1999/DataFiles/DEMO.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/1999-2000/DEMO.XPT\",\n",
    "        ],\n",
    "        \"2001-2002\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2001/DataFiles/DEMO_B.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2001-2002/DEMO_B.XPT\",\n",
    "        ],\n",
    "        \"2003-2004\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2003/DataFiles/DEMO_C.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2003-2004/DEMO_C.XPT\",\n",
    "        ],\n",
    "        \"2005-2006\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2005/DataFiles/DEMO_D.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2005-2006/DEMO_D.XPT\",\n",
    "        ],\n",
    "        \"2007-2008\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2007/DataFiles/DEMO_E.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2007-2008/DEMO_E.XPT\",\n",
    "        ],\n",
    "        \"2009-2010\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2009/DataFiles/DEMO_F.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2009-2010/DEMO_F.XPT\",\n",
    "        ],\n",
    "        \"2011-2012\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2011/DataFiles/DEMO_G.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2011-2012/DEMO_G.XPT\",\n",
    "        ],\n",
    "        \"2013-2014\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2013/DataFiles/DEMO_H.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2013-2014/DEMO_H.XPT\",\n",
    "        ],\n",
    "        \"2015-2016\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2015/DataFiles/DEMO_I.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2015-2016/DEMO_I.XPT\",\n",
    "        ],\n",
    "        \"2017-2018\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2017/DataFiles/DEMO_J.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2017-2018/DEMO_J.XPT\",\n",
    "        ],\n",
    "        \"2017-March 2020 (pre-pandemic)\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2017/DataFiles/P_DEMO.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2017-2020/P_DEMO.XPT\",\n",
    "        ],\n",
    "        \"August 2021–August 2023\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2021/DataFiles/DEMO_L.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2021-2023/DEMO_L.XPT\",\n",
    "        ],\n",
    "    }\n",
    "    store = Path(cfg.interim_dir)\n",
    "    parts = []\n",
    "    for cyc, urls in DEMO_URLS.items():\n",
    "        got = None\n",
    "        for u in urls:\n",
    "            try:\n",
    "                dst = store / Path(u).name\n",
    "                if not dst.exists():\n",
    "                    print(f\"⬇️ DEMO {cyc} → {dst.name}\")\n",
    "                    _download(u, dst)\n",
    "                got = dst; break\n",
    "            except Exception as e:\n",
    "                print(\"  ⚠️\", e)\n",
    "        if got is None: continue\n",
    "        df = _read_xpt(got)\n",
    "        if {\"SEQN\", \"RIAGENDR\"}.issubset(df.columns):\n",
    "            parts.append(df[[\"SEQN\", \"RIAGENDR\"]])\n",
    "    if not parts:\n",
    "        raise RuntimeError(\"Could not build minimal DEMO (RIAGENDR).\")\n",
    "    demo = pd.concat(parts, ignore_index=True).drop_duplicates(\"SEQN\")\n",
    "    out_demo = store / \"demo_riagendr_min.parquet\"\n",
    "    demo.to_parquet(out_demo, index=False)\n",
    "    return demo\n",
    "\n",
    "def _download_and_stack_alq(cfg) -> pd.DataFrame:\n",
    "    ALC_STORE = Path(cfg.interim_dir) / \"alcohol\"\n",
    "    ALC_STORE.mkdir(parents=True, exist_ok=True)\n",
    "    ALQ_URLS = {\n",
    "        \"1999-2000\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/1999/DataFiles/ALQ.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/1999-2000/ALQ.XPT\",\n",
    "        ],\n",
    "        \"2001-2002\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2001/DataFiles/ALQ_B.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2001-2002/ALQ_B.XPT\",\n",
    "        ],\n",
    "        \"2003-2004\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2003/DataFiles/ALQ_C.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2003-2004/ALQ_C.XPT\",\n",
    "        ],\n",
    "        \"2005-2006\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2005/DataFiles/ALQ_D.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2005-2006/ALQ_D.XPT\",\n",
    "        ],\n",
    "        \"2007-2008\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2007/DataFiles/ALQ_E.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2007-2008/ALQ_E.XPT\",\n",
    "        ],\n",
    "        \"2009-2010\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2009/DataFiles/ALQ_F.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2009-2010/ALQ_F.XPT\",\n",
    "        ],\n",
    "        \"2011-2012\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2011/DataFiles/ALQ_G.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2011-2012/ALQ_G.XPT\",\n",
    "        ],\n",
    "        \"2013-2014\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2013/DataFiles/ALQ_H.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2013-2014/ALQ_H.XPT\",\n",
    "        ],\n",
    "        \"2015-2016\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2015/DataFiles/ALQ_I.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2015-2016/ALQ_I.XPT\",\n",
    "        ],\n",
    "        \"2017-2018\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2017/DataFiles/ALQ_J.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2017-2018/ALQ_J.XPT\",\n",
    "        ],\n",
    "        \"2017-March 2020 (pre-pandemic)\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2017/DataFiles/P_ALQ.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2017-2020/P_ALQ.XPT\",\n",
    "        ],\n",
    "        \"August 2021–August 2023\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2021/DataFiles/ALQ_L.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2021-2023/ALQ_L.XPT\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2021/DataFiles/ALQ_Q.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2021-2023/ALQ_Q.XPT\",\n",
    "        ],\n",
    "    }\n",
    "    parts = []\n",
    "    for cycle, urls in ALQ_URLS.items():\n",
    "        got = None\n",
    "        for u in urls:\n",
    "            try:\n",
    "                dst = ALC_STORE / Path(u).name\n",
    "                if not dst.exists():\n",
    "                    print(f\"⬇️ ALQ {cycle} → {dst.name}\")\n",
    "                    _download(u, dst)\n",
    "                got = dst; break\n",
    "            except Exception as e:\n",
    "                print(\"  ⚠️\", e)\n",
    "        if got is None: continue\n",
    "        df = _read_xpt(got)\n",
    "        df[\"CYCLE\"] = cycle\n",
    "        keep = [c for c in [\"SEQN\", \"CYCLE\", \"ALQ110\", \"ALQ151\", \"ALQ120Q\", \"ALQ120U\", \"ALQ130\"] if c in df.columns]\n",
    "        parts.append(df[keep])\n",
    "    if not parts:\n",
    "        raise RuntimeError(\"No ALQ data available (download failed).\")\n",
    "    alq = pd.concat(parts, ignore_index=True)\n",
    "    stacked = Path(cfg.interim_dir) / \"alq_9923.parquet\"\n",
    "    stacked.parent.mkdir(parents=True, exist_ok=True)\n",
    "    alq.to_parquet(stacked, index=False)\n",
    "    return alq\n",
    "\n",
    "def _drinks_per_day_from_alq(alq: pd.DataFrame) -> pd.Series:\n",
    "    d = upper_df(alq)\n",
    "    count = _clean_num(d.get(\"ALQ120Q\", pd.Series(np.nan, index=d.index)))\n",
    "    unit  = d.get(\"ALQ120U\", pd.Series(np.nan, index=d.index))\n",
    "    per_year = pd.Series(np.nan, index=d.index, dtype=\"float\")\n",
    "    per_year = per_year.where(~(unit == 1), 365.0)\n",
    "    per_year = per_year.where(~(unit == 2), 52.142)\n",
    "    per_year = per_year.where(~(unit == 3), 12.0)\n",
    "    per_year = per_year.where(~(unit == 4), 1.0)\n",
    "    occasions_per_year = count * per_year\n",
    "    drinks_per_occasion = _clean_num(d.get(\"ALQ130\", pd.Series(np.nan, index=d.index)))\n",
    "    dpd = (occasions_per_year * drinks_per_occasion) / 365.0\n",
    "    return dpd.where(occasions_per_year.notna() & drinks_per_occasion.notna())\n",
    "\n",
    "def _categorize_alcohol(dpd: pd.Series, sex: pd.Series, lifetime_lt12: pd.Series | None) -> pd.Series:\n",
    "    dpd  = pd.to_numeric(dpd, errors=\"coerce\").reset_index(drop=True)\n",
    "    sexM = pd.to_numeric(sex, errors=\"coerce\").map({1: \"M\", 2: \"F\"}).astype(\"string\").reset_index(drop=True)\n",
    "    if lifetime_lt12 is None:\n",
    "        life = pd.Series(pd.NA, index=dpd.index)\n",
    "    else:\n",
    "        life = pd.to_numeric(lifetime_lt12, errors=\"coerce\").reset_index(drop=True)\n",
    "    none_mask     = dpd.isna() | (dpd < 0.03) | (life == 1)\n",
    "    heavy_mask    = ((sexM == \"F\") & (dpd >= 1.0)) | ((sexM == \"M\") & (dpd >= 2.0))\n",
    "    moderate_mask = (~none_mask) & (~heavy_mask)\n",
    "    cat = pd.Series(\"NONE\", index=dpd.index, dtype=\"string\")\n",
    "    cat.loc[moderate_mask] = \"MODERATE\"\n",
    "    cat.loc[heavy_mask]    = \"HEAVY\"\n",
    "    return pd.Categorical(cat, categories=[\"NONE\", \"MODERATE\", \"HEAVY\"], ordered=True)\n",
    "\n",
    "def build_alc(cfg: \"Config\" = CONFIG, allow_fetch: bool = True) -> pd.DataFrame:\n",
    "    ensure_dir(cfg.out_dir)\n",
    "    out_path = Path(cfg.out_dir) / cfg.cov_alc\n",
    "    if out_path.exists():\n",
    "        return pd.read_parquet(out_path)\n",
    "\n",
    "    alq_path = pick_first_existing(\n",
    "        Path(cfg.interim_dir) / \"alq_9923.parquet\",\n",
    "        Path(cfg.interim_dir) / \"alq_9918.parquet\",\n",
    "    )\n",
    "    if alq_path:\n",
    "        alq = _read_any(alq_path)\n",
    "    else:\n",
    "        if not allow_fetch:\n",
    "            raise FileNotFoundError(\"ALQ stack not found (alq_9923 / alq_9918). Set allow_fetch=True to download from CDC.\")\n",
    "        alq = _download_and_stack_alq(cfg)\n",
    "\n",
    "    alq = upper_df(alq)\n",
    "    dpd = _drinks_per_day_from_alq(alq)\n",
    "\n",
    "    life = None\n",
    "    if \"ALQ110\" in alq.columns:\n",
    "        life = (pd.to_numeric(alq[\"ALQ110\"], errors=\"coerce\") == 2).astype(\"Int8\")\n",
    "    elif \"ALQ151\" in alq.columns:\n",
    "        life = (pd.to_numeric(alq[\"ALQ151\"], errors=\"coerce\") == 2).astype(\"Int8\")\n",
    "\n",
    "    demo = _ensure_demo_sex(cfg)\n",
    "    demo = upper_df(demo).drop_duplicates(\"SEQN\")\n",
    "    sex = demo.set_index(\"SEQN\").reindex(alq[\"SEQN\"])[\"RIAGENDR\"]\n",
    "\n",
    "    alc_cat = _categorize_alcohol(dpd, sex=sex, lifetime_lt12=life)\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"SEQN\": alq[\"SEQN\"].reset_index(drop=True),\n",
    "        \"DRINKS_PER_DAY\": pd.to_numeric(dpd, errors=\"coerce\").reset_index(drop=True),\n",
    "        \"ALCOHOL_CAT\": alc_cat,\n",
    "    })\n",
    "    out.to_parquet(out_path, index=False)\n",
    "    log(f\"✓ ALC → {out_path}\")\n",
    "    return out\n",
    "# ---------- end alcohol builder ----------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75222def-0c1a-430c-bd41-e6b46f5a8212",
   "metadata": {},
   "source": [
    "## Physical Activity 99-23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ffe138e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derive_ltpa_from_paq(paq: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Output:\n",
    "      LTPA (MET-h/week), METSCORE (MET-min/week), IMP (0/1)\n",
    "\n",
    "    Handles:\n",
    "      • 2007–2018 & P files: PAQ650/665 (yes/no), PAQ655/670 (days), PAD660/675 (minutes)\n",
    "      • 2021–2023 (PAQ_L): PAD790Q/U + PAD800 (moderate), PAD810Q/U + PAD820 (vigorous)\n",
    "    \"\"\"\n",
    "    paq = paq.copy()\n",
    "    paq.columns = [c.upper() for c in paq.columns]\n",
    "    out = pd.DataFrame({\"SEQN\": paq[\"SEQN\"]})\n",
    "\n",
    "    def num(col):\n",
    "        return pd.to_numeric(paq.get(col, np.nan), errors=\"coerce\")\n",
    "    def clean_days(s):\n",
    "        return s.mask(s.isin([77, 99]))\n",
    "    def clean_min(s):\n",
    "        return s.mask(s.isin([7777, 9999]))\n",
    "\n",
    "    cols = set(paq.columns)\n",
    "\n",
    "    # ===== New schema (PAQ_L 2021–2023) =====\n",
    "    if {\"PAD790Q\",\"PAD790U\",\"PAD800\",\"PAD810Q\",\"PAD810U\",\"PAD820\"}.issubset(cols):\n",
    "\n",
    "        def per_week(q_col, u_col):\n",
    "            q = num(q_col).mask(num(q_col).isin([7777, 9999]))\n",
    "            u = paq[u_col].astype(str).str.strip().str.upper()\n",
    "            # D=day, W=week, M=month, Y=year → times per week\n",
    "            mult = np.where(u==\"D\", 7.0,\n",
    "                    np.where(u==\"W\", 1.0,\n",
    "                    np.where(u==\"M\", 52.0/12.0,\n",
    "                    np.where(u==\"Y\", 52.0, np.nan))))\n",
    "            return q * pd.to_numeric(mult, errors=\"coerce\")\n",
    "\n",
    "        mod_week = per_week(\"PAD790Q\",\"PAD790U\")\n",
    "        vig_week = per_week(\"PAD810Q\",\"PAD810U\")\n",
    "        mod_min_occ = clean_min(num(\"PAD800\"))\n",
    "        vig_min_occ = clean_min(num(\"PAD820\"))\n",
    "\n",
    "        mod_met_min = 4.0 * mod_week.fillna(0) * mod_min_occ.fillna(0)\n",
    "        vig_met_min = 8.0 * vig_week.fillna(0) * vig_min_occ.fillna(0)\n",
    "        total_met_min = mod_met_min + vig_met_min\n",
    "\n",
    "        has_mod = mod_week.notna() & mod_min_occ.notna()\n",
    "        has_vig = vig_week.notna() & vig_min_occ.notna()\n",
    "        has_any = has_mod | has_vig\n",
    "\n",
    "        zero_mod = (num(\"PAD790Q\") == 0)\n",
    "        zero_vig = (num(\"PAD810Q\") == 0)\n",
    "        true_zero = (zero_mod & zero_vig)\n",
    "\n",
    "        total_met_min = total_met_min.where(has_any, np.nan).where(~true_zero, 0)\n",
    "\n",
    "        imp = pd.Series(0, index=paq.index, dtype=\"int8\")\n",
    "        imp[(~has_any) & (~true_zero)] = 1\n",
    "\n",
    "    # ===== Old schema (2007–2018 & P files) =====\n",
    "    else:\n",
    "        any_vig = num(\"PAQ650\")  # 1 yes / 2 no\n",
    "        any_mod = num(\"PAQ665\")  # 1 yes / 2 no\n",
    "\n",
    "        vig_days = clean_days(num(\"PAQ655\"))\n",
    "        vig_min  = clean_min(num(\"PAD660\"))\n",
    "        mod_days = clean_days(num(\"PAQ670\"))\n",
    "        mod_min  = clean_min(num(\"PAD675\"))\n",
    "\n",
    "        vig_met_min = 8.0 * vig_days.fillna(0) * vig_min.fillna(0)\n",
    "        mod_met_min = 4.0 * mod_days.fillna(0) * mod_min.fillna(0)\n",
    "        total_met_min = vig_met_min + mod_met_min\n",
    "\n",
    "        has_any = ((vig_days.notna() & vig_min.notna()) | (mod_days.notna() & mod_min.notna()))\n",
    "        both_no = (any_vig == 2) & (any_mod == 2)\n",
    "\n",
    "        total_met_min = total_met_min.where(has_any, np.nan).where(~both_no, 0)\n",
    "\n",
    "        imp = pd.Series(0, index=paq.index, dtype=\"int8\")\n",
    "        imp[(~has_any) & (~both_no)] = 1\n",
    "\n",
    "    out[\"LTPA\"] = (total_met_min / 60.0).clip(lower=0)       # MET-h/week\n",
    "    out[\"METSCORE\"] = total_met_min.clip(lower=0).round(0)   # MET-min/week\n",
    "    out[\"IMP\"] = imp\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93b305d2-c977-4a8a-b42c-76bfeaa150fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def ensure_totalpa_9923(cfg: Config, overwrite: bool = False) -> Path:\n",
    "    \"\"\"\n",
    "    Ensure {cfg.interim_dir}/totalpa_9923_imputed.(parquet|csv) exists.\n",
    "    Uses your 9918→9923 appender if needed; writes both CSV and Parquet.\n",
    "    Returns the path to the Parquet file.\n",
    "    \"\"\"\n",
    "    idir = Path(cfg.interim_dir)\n",
    "    pq  = idir / \"totalpa_9923_imputed.parquet\"\n",
    "    csv = idir / \"totalpa_9923_imputed.csv\"\n",
    "    idir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if pq.exists() and not overwrite:\n",
    "        return pq\n",
    "    if csv.exists() and not overwrite:\n",
    "        # also create parquet for faster loads\n",
    "        pd.read_csv(csv).to_parquet(pq, index=False)\n",
    "        return pq\n",
    "\n",
    "    # ---- build using your make_9923_from_9918() ----\n",
    "    old_file = idir / \"totalpa_9918_imputed.csv\"\n",
    "    if not old_file.exists():\n",
    "        raise FileNotFoundError(f\"Missing {old_file}. Provide your 1999–2018 PA stack first.\")\n",
    "\n",
    "    # reuse your exact function names/logic\n",
    "    new_df = make_9923_from_9918(str(old_file), str(csv))\n",
    "    # normalize SEQN dtype & write parquet\n",
    "    if \"SEQN\" in new_df.columns:\n",
    "        new_df[\"SEQN\"] = pd.to_numeric(new_df[\"SEQN\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    new_df.to_parquet(pq, index=False)\n",
    "    print(f\"✓ Wrote {pq}\")\n",
    "    return pq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "324dbcc1-58a8-45ff-8a6e-e5519946599b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pa(cfg: Config = CONFIG) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build PA covariates from totalpa_9923_imputed (or 9918 + 19–23 append).\n",
    "    Outputs only SEQN, LTPA, METSCORE, IMP.\n",
    "    Prefers CSV because some Parquet dumps are stubs without LTPA/METSCORE.\n",
    "    \"\"\"\n",
    "    ensure_dir(cfg.out_dir)\n",
    "\n",
    "    # --- prefer CSV first; then Parquet; then legacy 9918 if provided\n",
    "    src = pick_first_existing(\n",
    "        Path(cfg.interim_dir) / \"totalpa_9923_imputed.csv\",\n",
    "        Path(cfg.interim_dir) / \"totalpa_9923_imputed.parquet\",\n",
    "        getattr(cfg, \"pa_9918_imputed\", None),\n",
    "    )\n",
    "    if src is None:\n",
    "        raise FileNotFoundError(\n",
    "            f\"PA source not found under {cfg.interim_dir} \"\n",
    "            \"(need totalpa_9923_imputed.csv or .parquet; or set CONFIG.pa_9918_imputed).\"\n",
    "        )\n",
    "\n",
    "    pa = read_any(src)\n",
    "    pa = upper_df(pa)\n",
    "    if \"SEQN\" not in pa.columns:\n",
    "        raise KeyError(f\"PA file lacks SEQN: {src}\")\n",
    "\n",
    "    # --- flexible column picking (handles different headers)\n",
    "    def pick_one(cands):\n",
    "        for c in pa.columns:\n",
    "            if c.upper() in cands:\n",
    "                return c\n",
    "        return None\n",
    "\n",
    "    c_ltpa = pick_one({\"LTPA\", \"LTPA_MET_HR_WK\", \"LTPA_MET_HOURS_WEEK\"})\n",
    "    c_mets = pick_one({\"METSCORE\", \"MET_MIN_WEEK\", \"MET_MIN_WK\", \"METMINWEEK\"})\n",
    "    c_imp  = pick_one({\"IMP\", \"IMPUTED\", \"PA_IMPUTED_FLAG\", \"LTPA_IMPUTED_FLAG\", \"IMPUTED_FLAG\"})\n",
    "\n",
    "    # If neither metric exists, fail loudly\n",
    "    if c_ltpa is None and c_mets is None:\n",
    "        raise ValueError(\n",
    "            f\"PA file {src} has no LTPA/METSCORE columns. \"\n",
    "            f\"Columns present include: {list(pa.columns)[:20]} ...\"\n",
    "        )\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"SEQN\": pd.to_numeric(pa[\"SEQN\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    })\n",
    "\n",
    "    if c_ltpa:\n",
    "        out[\"LTPA\"] = pd.to_numeric(pa[c_ltpa], errors=\"coerce\")\n",
    "    else:\n",
    "        out[\"LTPA\"] = np.nan\n",
    "\n",
    "    if c_mets:\n",
    "        out[\"METSCORE\"] = pd.to_numeric(pa[c_mets], errors=\"coerce\")\n",
    "    else:\n",
    "        # derive when only LTPA present (MET-hours → MET-min)\n",
    "        out[\"METSCORE\"] = out[\"LTPA\"] * 60.0\n",
    "\n",
    "    if c_imp:\n",
    "        out[\"IMP\"] = pd.to_numeric(pa[c_imp], errors=\"coerce\").fillna(0).astype(\"Int8\")\n",
    "    else:\n",
    "        out[\"IMP\"] = 0\n",
    "\n",
    "    # one row per SEQN\n",
    "    out = (out.groupby(\"SEQN\", as_index=False)\n",
    "              .agg({\"LTPA\": \"max\", \"METSCORE\": \"max\", \"IMP\": \"max\"}))\n",
    "\n",
    "    # sanity: catch the “all NA” case (usually wrong source file)\n",
    "    if out[[\"LTPA\", \"METSCORE\"]].isna().all().all():\n",
    "        raise RuntimeError(\n",
    "            f\"All PA values are NA from source {src}. \"\n",
    "            \"Likely a stub Parquet — use the CSV or rebuild the 99–23 file.\"\n",
    "        )\n",
    "\n",
    "    outp = Path(cfg.out_dir) / cfg.cov_pa\n",
    "    out.to_parquet(outp, index=False)\n",
    "    log(f\"✓ PA  → {outp} (source: {Path(src).name})\")\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3891e24b-64b1-4822-a232-f31d91bb13ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ PA  → /Users/dengshuyue/Desktop/SDOH/analysis/output/cov_pa_1999_2023.parquet (source: totalpa_9923_imputed.csv)\n",
      "✓ CORE → /Users/dengshuyue/Desktop/SDOH/analysis/output/cov_core_1999_2023.parquet\n",
      "LTPA        43.46\n",
      "METSCORE    43.46\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# rebuild PA then core\n",
    "build_pa(CONFIG)\n",
    "build_core(CONFIG)\n",
    "\n",
    "core = pd.read_parquet(CONFIG.out_dir / CONFIG.cov_core)\n",
    "print((core[[\"LTPA\",\"METSCORE\"]].isna().mean()*100).round(2))  # % NA should not be ~100%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abc6a9b-600e-4326-9e76-22b934fed682",
   "metadata": {},
   "source": [
    "#### preview PA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f08e742f-685e-43d6-817a-bf6712a1f69d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEQN</th>\n",
       "      <th>LTPA</th>\n",
       "      <th>METSCORE</th>\n",
       "      <th>IMP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>41.066667</td>\n",
       "      <td>1920.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>3.033333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8160.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>300.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14</td>\n",
       "      <td>30.800000</td>\n",
       "      <td>10320.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15</td>\n",
       "      <td>38.577778</td>\n",
       "      <td>1680.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>16</td>\n",
       "      <td>48.533333</td>\n",
       "      <td>5040.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SEQN       LTPA  METSCORE  IMP\n",
       "0     2   0.000000      60.0    1\n",
       "1     5  41.066667    1920.0    1\n",
       "2     7   3.033333       0.0    1\n",
       "3    10   0.000000    8160.0    1\n",
       "4    12   5.600000       0.0    1\n",
       "5    13   0.000000     300.0    1\n",
       "6    14  30.800000   10320.0    1\n",
       "7    15  38.577778    1680.0    1\n",
       "8    16  48.533333    5040.0    1\n",
       "9    20   0.466667       0.0    1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEQN        0.000000\n",
      "LTPA        0.001358\n",
      "METSCORE    0.001358\n",
      "IMP         0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# quick peek\n",
    "pa = pd.read_parquet(Path(CONFIG.out_dir) / CONFIG.cov_pa)\n",
    "display(pa.head(10))\n",
    "print(pa.isna().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "63e2a0ea-2fd8-4d77-8c58-e456461fa8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   SEQN\n",
      "0     2\n",
      "1     5\n",
      "2     7\n",
      "3    10\n",
      "4    12\n",
      "         SEQN\n",
      "72922  142305\n",
      "72923  142307\n",
      "72924  142308\n",
      "72925  142309\n",
      "72926  142310\n"
     ]
    }
   ],
   "source": [
    "# check SEQN range \n",
    "print(pa.sort_values(\"SEQN\")[[\"SEQN\"]].head(5))\n",
    "print(pa.sort_values(\"SEQN\")[[\"SEQN\"]].tail(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3e1dbf-89a1-4ead-93fc-e7543cb80adc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd561718-74c8-4ec5-8c75-0c68658ba3a3",
   "metadata": {},
   "source": [
    "## BMX "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e8894de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_bmx(cfg: Config = CONFIG) -> pd.DataFrame:\n",
    "    \"\"\"Load anthropometrics (BMX) and compute BMI when missing.\"\"\"\n",
    "    ensure_dir(cfg.out_dir)\n",
    "\n",
    "    def _read_any(p: Path) -> pd.DataFrame:\n",
    "        return pd.read_parquet(p) if str(p).lower().endswith(\".parquet\") else pd.read_csv(p, low_memory=False)\n",
    "\n",
    "    if cfg.bmx_9923 and Path(cfg.bmx_9923).exists():\n",
    "        src = Path(cfg.bmx_9923)\n",
    "    else:\n",
    "        src = pick_first_existing(\n",
    "            cfg.interim_dir / \"bmx_9923.parquet\",\n",
    "            cfg.interim_dir / \"bmx_9923.csv\",\n",
    "            cfg.interim_dir / \"bmx_9918.parquet\",\n",
    "            cfg.interim_dir / \"bmx_9918.csv\",\n",
    "        )\n",
    "    if src is None:\n",
    "        raise FileNotFoundError(\"BMX not found. Put bmx_9923/bmx_9918 under interim, or set CONFIG.bmx_9923.\")\n",
    "\n",
    "    bmx = upper_df(_read_any(src))\n",
    "    for need in [\"SEQN\", \"BMXWT\", \"BMXHT\"]:\n",
    "        if need not in bmx.columns:\n",
    "            raise ValueError(f\"BMX table missing required column: {need}\")\n",
    "\n",
    "    def nan_series(df): \n",
    "        return pd.Series(np.nan, index=df.index, dtype=\"float\")\n",
    "\n",
    "    bmi_src = pd.to_numeric(bmx[\"BMXBMI\"], errors=\"coerce\") if \"BMXBMI\" in bmx.columns else nan_series(bmx)\n",
    "    wt = pd.to_numeric(bmx[\"BMXWT\"], errors=\"coerce\")\n",
    "    ht_cm = pd.to_numeric(bmx[\"BMXHT\"], errors=\"coerce\")\n",
    "\n",
    "    bmi = bmi_src.copy()\n",
    "    missing = bmi.isna()\n",
    "    if missing.any():\n",
    "        bmi.loc[missing] = wt.loc[missing] / (ht_cm.loc[missing] / 100.0) ** 2\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"SEQN\": bmx[\"SEQN\"],\n",
    "        \"BMXWT\": wt,\n",
    "        \"BMXHT\": ht_cm,\n",
    "        \"BMI\": pd.to_numeric(bmi, errors=\"coerce\"),\n",
    "    })\n",
    "\n",
    "    outp = cfg.out_dir / cfg.cov_bmx\n",
    "    out.to_parquet(outp, index=False)\n",
    "    log(f\"✓ BMX → {outp} (source: {src.name})\")\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7c794eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class ClinicalThresholds:\n",
    "    htn_sbp: float = 140.0\n",
    "    htn_dbp: float = 90.0\n",
    "    a1c_diabetes: float = 6.5\n",
    "    fpg_diabetes: float = 126.0  # mg/dL\n",
    "\n",
    "THR = ClinicalThresholds()\n",
    "\n",
    "def build_clinical(cfg: Config = CONFIG, thr: ClinicalThresholds = THR) -> pd.DataFrame:\n",
    "    ensure_dir(cfg.out_dir)\n",
    "\n",
    "    clin_9923 = pick_first_existing(cfg.interim_dir / \"clinical_9923.parquet\", cfg.interim_dir / \"clinical_9923.csv\")\n",
    "    if clin_9923:\n",
    "        clin = read_any(clin_9923)\n",
    "    else:\n",
    "        p9918 = pick_first_existing(\n",
    "            cfg.clinical_9918,\n",
    "            cfg.interim_dir / \"clinical_9918.parquet\",\n",
    "            cfg.interim_dir / \"clinical_9918.csv\",\n",
    "            cfg.interim_dir / \"nhanes_primary_anal_full_singleimputation_v2.parquet\",\n",
    "            cfg.interim_dir / \"nhanes_primary_anal_full_singleimputation_v2.csv\",\n",
    "        )\n",
    "        p1923 = pick_first_existing(cfg.clinical_1923, cfg.interim_dir / \"clinical_1923.parquet\", cfg.interim_dir / \"clinical_1923.csv\")\n",
    "        if p9918 is None:\n",
    "            raise FileNotFoundError(\"Provide clinical_9923 or clinical_9918 under interim.\")\n",
    "        clin = read_any(p9918)\n",
    "        if p1923:\n",
    "            clin = pd.concat([clin, read_any(p1923)], ignore_index=True)\n",
    "\n",
    "    clin = upper_df(clin)\n",
    "\n",
    "    # Derive BMI_CLAS if missing\n",
    "    if \"BMI_CLAS\" not in clin.columns:\n",
    "        bmi_src = None\n",
    "        bmx_path = cfg.out_dir / cfg.cov_bmx\n",
    "        if bmx_path.exists():\n",
    "            bmx = upper_df(pd.read_parquet(bmx_path))\n",
    "            if {\"SEQN\", \"BMI\"}.issubset(bmx.columns) and \"SEQN\" in clin.columns:\n",
    "                bmi_src = clin[\"SEQN\"].map(bmx.set_index(\"SEQN\")[\"BMI\"]).astype(float)\n",
    "        if bmi_src is None:\n",
    "            bmi_src = pd.to_numeric(clin.get(\"BMI\", np.nan), errors=\"coerce\")\n",
    "\n",
    "        def bmi_class(x):\n",
    "            if pd.isna(x): return pd.NA\n",
    "            if x < 18.5:  return \"UNDER\"\n",
    "            if x < 25:    return \"NORMAL\"\n",
    "            if x < 30:    return \"OVER\"\n",
    "            return \"OBESE\"\n",
    "\n",
    "        clin[\"BMI_CLAS\"] = pd.Series([bmi_class(v) for v in bmi_src], dtype=\"string\")\n",
    "\n",
    "    # Derive HTN if missing\n",
    "    if \"HTN\" not in clin.columns:\n",
    "        sbp = pd.to_numeric(clin.get(\"SBP\", np.nan), errors=\"coerce\")\n",
    "        dbp = pd.to_numeric(clin.get(\"DBP\", np.nan), errors=\"coerce\")\n",
    "        diag_col = next((c for c in clin.columns if ((\"HTN\" in c or \"HYPERT\" in c) and \"MED\" not in c and c != \"HTN\")), None)\n",
    "        med_col  = next((c for c in clin.columns if (\"MED\" in c and (\"BP\" in c or \"HYPER\" in c))), None)\n",
    "        htn = pd.Series(0, index=clin.index, dtype=\"Int8\")\n",
    "        if diag_col:\n",
    "            diag = pd.to_numeric(clin[diag_col], errors=\"coerce\")\n",
    "            htn = ((diag == 1) | (diag > 0)).astype(\"Int8\")\n",
    "        if med_col:\n",
    "            med = pd.to_numeric(clin[med_col], errors=\"coerce\")\n",
    "            htn = ((htn == 1) | (med == 1) | (med > 0)).astype(\"Int8\")\n",
    "        htn = ((htn == 1) | (sbp >= thr.htn_sbp) | (dbp >= thr.htn_dbp)).astype(\"Int8\")\n",
    "        clin[\"HTN\"] = htn\n",
    "\n",
    "    # Derive HIGH_CHOL if missing\n",
    "    if \"HIGH_CHOL\" not in clin.columns:\n",
    "        tch = pd.to_numeric(clin.get(\"TCHOL\", np.nan), errors=\"coerce\")\n",
    "        ldl = pd.to_numeric(clin.get(\"LDL\", np.nan), errors=\"coerce\")\n",
    "        med_col = next((c for c in clin.columns if (\"CHOL\" in c and \"MED\" in c)), None)\n",
    "        high = ((tch >= 240) | (ldl >= 160)).astype(\"Int8\")\n",
    "        if med_col:\n",
    "            med = pd.to_numeric(clin[med_col], errors=\"coerce\")\n",
    "            high = ((high == 1) | (med == 1) | (med > 0)).astype(\"Int8\")\n",
    "        clin[\"HIGH_CHOL\"] = high\n",
    "\n",
    "    keep = [\"SEQN\", \"BMI_CLAS\", \"DIABETES\", \"HTN\", \"HIGH_CHOL\", \"CVD\", \"CANCER\", \"SBP\", \"DBP\", \"TCHOL\", \"HDL\", \"LDL\", \"TG\"]\n",
    "    for k in keep:\n",
    "        if k not in clin.columns:\n",
    "            clin[k] = pd.Series(np.nan, index=clin.index)\n",
    "\n",
    "    out = clin[keep].copy()\n",
    "    for b in [\"DIABETES\", \"HTN\", \"HIGH_CHOL\", \"CVD\", \"CANCER\"]:\n",
    "        out[b] = pd.to_numeric(out[b], errors=\"coerce\").astype(\"Int8\")\n",
    "\n",
    "    outp = cfg.out_dir / cfg.cov_clinical\n",
    "    out.to_parquet(outp, index=False)\n",
    "    log(f\"✓ CLN → {outp}\")\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b9df11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SURVEY_KEEP = [\"SEQN\", \"SDDSRVYR\", \"SDMVPSU\", \"SDMVSTRA\", \"WTMEC2YR\"]\n",
    "\n",
    "def build_household(cfg: Config = CONFIG) -> pd.DataFrame:\n",
    "    ensure_dir(cfg.out_dir)\n",
    "    demo = upper_df(pd.read_parquet(cfg.demo_9923))\n",
    "    if \"DMDHHSIZ\" not in demo.columns:\n",
    "        raise ValueError(\"DMDHHSIZ not found in DEMO stack.\")\n",
    "    out = demo[[\"SEQN\", \"DMDHHSIZ\"]].drop_duplicates(\"SEQN\")\n",
    "    outp = cfg.out_dir / cfg.cov_household\n",
    "    out.to_parquet(outp, index=False)\n",
    "    log(f\"✓ HH  → {outp}\")\n",
    "    return out\n",
    "\n",
    "def get_survey_core(cfg: Config = CONFIG) -> pd.DataFrame:\n",
    "    demo_p = (cfg.demo_9923 if cfg.demo_9923 and Path(cfg.demo_9923).exists()\n",
    "              else cfg.demo_9918 if cfg.demo_9918 and Path(cfg.demo_9918).exists()\n",
    "              else pick_first_existing(cfg.interim_dir / \"demo_9923.parquet\",\n",
    "                                       cfg.interim_dir / \"demo_9918.parquet\"))\n",
    "    if demo_p is None:\n",
    "        raise FileNotFoundError(\"Could not find DEMO table.\")\n",
    "    demo = upper_df(pd.read_parquet(demo_p))\n",
    "    miss = [c for c in SURVEY_KEEP if c not in demo.columns]\n",
    "    if miss:\n",
    "        raise ValueError(f\"Missing survey fields in DEMO: {miss}\")\n",
    "    return demo[SURVEY_KEEP].drop_duplicates(\"SEQN\").copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492133b5-b1be-4410-83bd-9a85de1272e9",
   "metadata": {},
   "source": [
    "## merge all to one core file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5ec8220e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_core(cfg: Config = CONFIG) -> pd.DataFrame:\n",
    "    ensure_dir(cfg.out_dir)\n",
    "\n",
    "    survey = upper_df(get_survey_core(cfg))\n",
    "    smk    = upper_df(pd.read_parquet(cfg.out_dir / cfg.cov_smk))\n",
    "    alc    = upper_df(pd.read_parquet(cfg.out_dir / cfg.cov_alc))\n",
    "    pa     = upper_df(pd.read_parquet(cfg.out_dir / cfg.cov_pa))\n",
    "    bmx    = upper_df(pd.read_parquet(cfg.out_dir / cfg.cov_bmx))\n",
    "    clin   = upper_df(pd.read_parquet(cfg.out_dir / cfg.cov_clinical))\n",
    "    hh     = upper_df(pd.read_parquet(cfg.out_dir / cfg.cov_household))\n",
    "\n",
    "    # ensure SEQN types align\n",
    "    for df in (survey, smk, alc, pa, bmx, clin, hh):\n",
    "        df[\"SEQN\"] = pd.to_numeric(df[\"SEQN\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    # select only non-overlapping columns from each piece\n",
    "    smk  = smk[[\"SEQN\", \"SMK_STATUS\", \"CIGS_PER_DAY\", \"PACK_YEARS\", \"FORMER_SMOKER\"]]\n",
    "    alc  = alc[[\"SEQN\", \"DRINKS_PER_DAY\", \"ALCOHOL_CAT\"]]\n",
    "    pa   = pa[[\"SEQN\", \"LTPA\", \"METSCORE\", \"IMP\"]]\n",
    "    bmx  = bmx[[\"SEQN\", \"BMXWT\", \"BMXHT\", \"BMI\"]]\n",
    "    clin = clin[[\"SEQN\", \"BMI_CLAS\", \"DIABETES\", \"HTN\", \"HIGH_CHOL\", \"CVD\", \"CANCER\",\n",
    "                 \"SBP\", \"DBP\", \"TCHOL\", \"HDL\", \"LDL\", \"TG\"]]\n",
    "    hh   = hh[[\"SEQN\", \"DMDHHSIZ\"]]\n",
    "\n",
    "    # merge with parentheses (no backslashes)\n",
    "    core = (\n",
    "        survey\n",
    "        .merge(smk,  on=\"SEQN\", how=\"left\")\n",
    "        .merge(alc,  on=\"SEQN\", how=\"left\")\n",
    "        .merge(pa,   on=\"SEQN\", how=\"left\")\n",
    "        .merge(bmx,  on=\"SEQN\", how=\"left\")\n",
    "        .merge(clin, on=\"SEQN\", how=\"left\")\n",
    "        .merge(hh,   on=\"SEQN\", how=\"left\")\n",
    "    )\n",
    "\n",
    "    # safety check for accidental duplicates\n",
    "    if core.columns.duplicated().any():\n",
    "        dups = core.columns[core.columns.duplicated()].tolist()\n",
    "        raise RuntimeError(f\"Duplicate columns after merge: {dups}\")\n",
    "\n",
    "    outp = cfg.out_dir / cfg.cov_core\n",
    "    core.to_parquet(outp, index=False)\n",
    "    log(f\"✓ CORE → {outp}\")\n",
    "    return core\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6f20a399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ SMK → /Users/dengshuyue/Desktop/SDOH/analysis/output/cov_smk_1999_2023.parquet (using smk_9918.csv); missing: {'SMK_STATUS': 0.001, 'CIGS_PER_DAY': 0.794, 'PACK_YEARS': 0.78}\n",
      "✓ PA  → /Users/dengshuyue/Desktop/SDOH/analysis/output/cov_pa_1999_2023.parquet (source: totalpa_9923_imputed.csv)\n",
      "✓ BMX → /Users/dengshuyue/Desktop/SDOH/analysis/output/cov_bmx_1999_2023.parquet (source: bmx_9918.csv)\n",
      "✓ CLN → /Users/dengshuyue/Desktop/SDOH/analysis/output/cov_clinical_1999_2023.parquet\n",
      "✓ HH  → /Users/dengshuyue/Desktop/SDOH/analysis/output/cov_household_1999_2023.parquet\n",
      "✓ CORE → /Users/dengshuyue/Desktop/SDOH/analysis/output/cov_core_1999_2023.parquet\n",
      "Done. Files in: /Users/dengshuyue/Desktop/SDOH/analysis/output\n",
      "n_rows             128809.00\n",
      "n_unique_seqn      128809.00\n",
      "LTPA %NA               43.46\n",
      "METSCORE %NA           43.46\n",
      "ALCOHOL_CAT %NA        46.54\n",
      "SMK_STATUS %NA         57.29\n",
      "has_weights             1.00\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from typing import Dict\n",
    "\n",
    "def run_all(cfg: Config = CONFIG) -> Dict[str, pd.DataFrame]:\n",
    "    ensure_dir(cfg.out_dir)\n",
    "    out: Dict[str, pd.DataFrame] = {}\n",
    "    out[\"smk\"]      = build_smk(cfg)\n",
    "    out[\"alc\"]      = build_alc(cfg)\n",
    "    out[\"pa\"]       = build_pa(cfg)\n",
    "    out[\"bmx\"]      = build_bmx(cfg)\n",
    "    out[\"clinical\"] = build_clinical(cfg)\n",
    "    out[\"household\"]= build_household(cfg)\n",
    "    out[\"core\"]     = build_core(cfg)\n",
    "    return out\n",
    "\n",
    "def quick_checks(cfg: Config = CONFIG) -> pd.Series:\n",
    "    core = pd.read_parquet(cfg.out_dir / cfg.cov_core)\n",
    "    checks = {\n",
    "        \"n_rows\": int(len(core)),\n",
    "        \"n_unique_seqn\": int(core[\"SEQN\"].nunique()),\n",
    "        \"LTPA %NA\": float(core[\"LTPA\"].isna().mean()*100),\n",
    "        \"METSCORE %NA\": float(core[\"METSCORE\"].isna().mean()*100),\n",
    "        \"ALCOHOL_CAT %NA\": float(core[\"ALCOHOL_CAT\"].isna().mean()*100),\n",
    "        \"SMK_STATUS %NA\": float(core[\"SMK_STATUS\"].isna().mean()*100),\n",
    "        \"has_weights\": int(\"WTMEC2YR\" in core.columns),\n",
    "    }\n",
    "    return pd.Series(checks).round(2)\n",
    "\n",
    "# --- set DEMO path and RUN ---\n",
    "CONFIG.demo_9923 = Path(\"/Users/dengshuyue/Desktop/SDOH/analysis/data/cov/demo9923.parquet\")\n",
    "assert CONFIG.demo_9923.exists(), \"demo9923.parquet not found\"\n",
    "\n",
    "# Optional: clean old core/pa artifacts if they exist\n",
    "for fn in (CONFIG.cov_core, CONFIG.cov_pa):\n",
    "    f = Path(CONFIG.out_dir) / fn\n",
    "    if f.exists():\n",
    "        f.unlink()\n",
    "\n",
    "out = run_all(CONFIG)\n",
    "print(\"Done. Files in:\", CONFIG.out_dir)\n",
    "print(quick_checks(CONFIG))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728e576c-f566-408f-979c-0d6d1e05fbca",
   "metadata": {},
   "source": [
    "## check merged table (cov_core_1999_2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "83df4ce7-c9b3-4d3c-99cb-b949c7861816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyarrow._parquet.FileMetaData object at 0x1258e5c10>\n",
      "  created_by: parquet-cpp-arrow version 21.0.0\n",
      "  num_columns: 30\n",
      "  num_rows: 128809\n",
      "  num_row_groups: 1\n",
      "  format_version: 2.6\n",
      "  serialized_size: 14790\n",
      "<pyarrow._parquet.ParquetSchema object at 0x11433c9c0>\n",
      "required group field_id=-1 schema {\n",
      "  optional int64 field_id=-1 SEQN;\n",
      "  optional double field_id=-1 SDDSRVYR;\n",
      "  optional double field_id=-1 SDMVPSU;\n",
      "  optional double field_id=-1 SDMVSTRA;\n",
      "  optional double field_id=-1 WTMEC2YR;\n",
      "  optional binary field_id=-1 SMK_STATUS (String);\n",
      "  optional double field_id=-1 CIGS_PER_DAY;\n",
      "  optional double field_id=-1 PACK_YEARS;\n",
      "  optional double field_id=-1 FORMER_SMOKER;\n",
      "  optional double field_id=-1 DRINKS_PER_DAY;\n",
      "  optional binary field_id=-1 ALCOHOL_CAT (String);\n",
      "  optional double field_id=-1 LTPA;\n",
      "  optional double field_id=-1 METSCORE;\n",
      "  optional int32 field_id=-1 IMP (Int(bitWidth=8, isSigned=true));\n",
      "  optional double field_id=-1 BMXWT;\n",
      "  optional double field_id=-1 BMXHT;\n",
      "  optional double field_id=-1 BMI;\n",
      "  optional binary field_id=-1 BMI_CLAS (String);\n",
      "  optional int32 field_id=-1 DIABETES (Int(bitWidth=8, isSigned=true));\n",
      "  optional int32 field_id=-1 HTN (Int(bitWidth=8, isSigned=true));\n",
      "  optional int32 field_id=-1 HIGH_CHOL (Int(bitWidth=8, isSigned=true));\n",
      "  optional int32 field_id=-1 CVD (Int(bitWidth=8, isSigned=true));\n",
      "  optional int32 field_id=-1 CANCER (Int(bitWidth=8, isSigned=true));\n",
      "  optional double field_id=-1 SBP;\n",
      "  optional double field_id=-1 DBP;\n",
      "  optional double field_id=-1 TCHOL;\n",
      "  optional double field_id=-1 HDL;\n",
      "  optional double field_id=-1 LDL;\n",
      "  optional double field_id=-1 TG;\n",
      "  optional double field_id=-1 DMDHHSIZ;\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEQN</th>\n",
       "      <th>SDDSRVYR</th>\n",
       "      <th>WTMEC2YR</th>\n",
       "      <th>SMK_STATUS</th>\n",
       "      <th>ALCOHOL_CAT</th>\n",
       "      <th>LTPA</th>\n",
       "      <th>METSCORE</th>\n",
       "      <th>BMXWT</th>\n",
       "      <th>BMXHT</th>\n",
       "      <th>BMI</th>\n",
       "      <th>BMI_CLAS</th>\n",
       "      <th>DIABETES</th>\n",
       "      <th>HTN</th>\n",
       "      <th>HIGH_CHOL</th>\n",
       "      <th>SBP</th>\n",
       "      <th>DBP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10982.898896</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.5</td>\n",
       "      <td>91.6</td>\n",
       "      <td>14.897695</td>\n",
       "      <td>UNDER</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>91.333333</td>\n",
       "      <td>56.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28325.384898</td>\n",
       "      <td>NEVER</td>\n",
       "      <td>MODERATE</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>60.0</td>\n",
       "      <td>75.4</td>\n",
       "      <td>174.0</td>\n",
       "      <td>24.904215</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100.666667</td>\n",
       "      <td>56.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46192.256945</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.9</td>\n",
       "      <td>136.6</td>\n",
       "      <td>17.631713</td>\n",
       "      <td>UNDER</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>108.666667</td>\n",
       "      <td>62.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10251.260020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>95.333333</td>\n",
       "      <td>61.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>99445.065735</td>\n",
       "      <td>FORMER</td>\n",
       "      <td>HEAVY</td>\n",
       "      <td>41.066667</td>\n",
       "      <td>1920.0</td>\n",
       "      <td>92.5</td>\n",
       "      <td>178.3</td>\n",
       "      <td>29.096386</td>\n",
       "      <td>OVER</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>82.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39656.600444</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>59.2</td>\n",
       "      <td>162.0</td>\n",
       "      <td>22.557537</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>114.666667</td>\n",
       "      <td>68.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25525.423409</td>\n",
       "      <td>FORMER</td>\n",
       "      <td>NONE</td>\n",
       "      <td>3.033333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>162.9</td>\n",
       "      <td>29.393577</td>\n",
       "      <td>OVER</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>125.333333</td>\n",
       "      <td>80.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>31510.587866</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.7</td>\n",
       "      <td>162.0</td>\n",
       "      <td>15.508307</td>\n",
       "      <td>UNDER</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100.666667</td>\n",
       "      <td>49.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7575.870247</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>45.5</td>\n",
       "      <td>156.9</td>\n",
       "      <td>18.482704</td>\n",
       "      <td>UNDER</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>109.333333</td>\n",
       "      <td>53.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22445.808572</td>\n",
       "      <td>CURRENT</td>\n",
       "      <td>MODERATE</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8160.0</td>\n",
       "      <td>111.8</td>\n",
       "      <td>190.1</td>\n",
       "      <td>30.936955</td>\n",
       "      <td>OBESE</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>145.333333</td>\n",
       "      <td>96.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>38983.169313</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65.0</td>\n",
       "      <td>171.9</td>\n",
       "      <td>21.996906</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>51.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>95494.214052</td>\n",
       "      <td>NEVER</td>\n",
       "      <td>MODERATE</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.2</td>\n",
       "      <td>180.0</td>\n",
       "      <td>30.617284</td>\n",
       "      <td>OBESE</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>176.666667</td>\n",
       "      <td>102.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1843.950828</td>\n",
       "      <td>FORMER</td>\n",
       "      <td>HEAVY</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>300.0</td>\n",
       "      <td>63.6</td>\n",
       "      <td>157.7</td>\n",
       "      <td>25.573710</td>\n",
       "      <td>OVER</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>133.333333</td>\n",
       "      <td>70.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19486.733790</td>\n",
       "      <td>CURRENT</td>\n",
       "      <td>MODERATE</td>\n",
       "      <td>30.800000</td>\n",
       "      <td>10320.0</td>\n",
       "      <td>75.5</td>\n",
       "      <td>166.2</td>\n",
       "      <td>27.332850</td>\n",
       "      <td>OVER</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>59.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>114519.177908</td>\n",
       "      <td>CURRENT</td>\n",
       "      <td>HEAVY</td>\n",
       "      <td>38.577778</td>\n",
       "      <td>1680.0</td>\n",
       "      <td>81.6</td>\n",
       "      <td>174.9</td>\n",
       "      <td>26.675375</td>\n",
       "      <td>OVER</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>68.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12565.995924</td>\n",
       "      <td>NEVER</td>\n",
       "      <td>NONE</td>\n",
       "      <td>48.533333</td>\n",
       "      <td>5040.0</td>\n",
       "      <td>41.5</td>\n",
       "      <td>144.2</td>\n",
       "      <td>19.958026</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>147.333333</td>\n",
       "      <td>60.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10390.427218</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.4</td>\n",
       "      <td>88.6</td>\n",
       "      <td>14.522367</td>\n",
       "      <td>UNDER</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>56.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>1.0</td>\n",
       "      <td>34423.943439</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>68.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2116.330044</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>82.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16736.882281</td>\n",
       "      <td>FORMER</td>\n",
       "      <td>HEAVY</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>59.8</td>\n",
       "      <td>158.9</td>\n",
       "      <td>23.683909</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>102.666667</td>\n",
       "      <td>58.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    SEQN  SDDSRVYR       WTMEC2YR SMK_STATUS ALCOHOL_CAT       LTPA  METSCORE  BMXWT  BMXHT        BMI BMI_CLAS  DIABETES  HTN  HIGH_CHOL         SBP         DBP\n",
       "0      1       1.0   10982.898896        NaN         NaN        NaN       NaN   12.5   91.6  14.897695    UNDER         0    0          0   91.333333   56.000000\n",
       "1      2       1.0   28325.384898      NEVER    MODERATE   0.000000      60.0   75.4  174.0  24.904215   NORMAL         0    0          0  100.666667   56.666667\n",
       "2      3       1.0   46192.256945        NaN         NaN        NaN       NaN   32.9  136.6  17.631713    UNDER         0    0          0  108.666667   62.000000\n",
       "3      4       1.0   10251.260020        NaN         NaN        NaN       NaN   13.3    NaN        NaN     <NA>         0    0          1   95.333333   61.333333\n",
       "4      5       1.0   99445.065735     FORMER       HEAVY  41.066667    1920.0   92.5  178.3  29.096386     OVER         0    1          1  122.000000   82.666667\n",
       "5      6       1.0   39656.600444        NaN         NaN        NaN       NaN   59.2  162.0  22.557537   NORMAL         0    0          0  114.666667   68.000000\n",
       "6      7       1.0   25525.423409     FORMER        NONE   3.033333       0.0   78.0  162.9  29.393577     OVER         0    0          1  125.333333   80.000000\n",
       "7      8       1.0   31510.587866        NaN         NaN        NaN       NaN   40.7  162.0  15.508307    UNDER         0    0          0  100.666667   49.333333\n",
       "8      9       1.0    7575.870247        NaN         NaN        NaN       NaN   45.5  156.9  18.482704    UNDER         0    0          0  109.333333   53.333333\n",
       "9     10       1.0   22445.808572    CURRENT    MODERATE   0.000000    8160.0  111.8  190.1  30.936955    OBESE         0    1          0  145.333333   96.000000\n",
       "10    11       1.0   38983.169313        NaN         NaN        NaN       NaN   65.0  171.9  21.996906   NORMAL         0    0          0  108.000000   51.333333\n",
       "11    12       1.0   95494.214052      NEVER    MODERATE   5.600000       0.0   99.2  180.0  30.617284    OBESE         0    1          0  176.666667  102.000000\n",
       "12    13       1.0    1843.950828     FORMER       HEAVY   0.000000     300.0   63.6  157.7  25.573710     OVER         1    1          1  133.333333   70.000000\n",
       "13    14       1.0   19486.733790    CURRENT    MODERATE  30.800000   10320.0   75.5  166.2  27.332850     OVER         0    0          0  138.000000   59.333333\n",
       "14    15       1.0  114519.177908    CURRENT       HEAVY  38.577778    1680.0   81.6  174.9  26.675375     OVER         0    0          0  108.000000   68.666667\n",
       "15    16       1.0   12565.995924      NEVER        NONE  48.533333    5040.0   41.5  144.2  19.958026   NORMAL         0    1          0  147.333333   60.666667\n",
       "16    17       1.0   10390.427218        NaN         NaN        NaN       NaN   11.4   88.6  14.522367    UNDER         0    0          0  100.000000   56.666667\n",
       "17    18       1.0   34423.943439        NaN         NaN        NaN       NaN   11.1    NaN        NaN     <NA>         0    0          0  102.000000   68.666667\n",
       "18    19       1.0    2116.330044        NaN         NaN        NaN       NaN   11.5    NaN        NaN     <NA>         1    0          0   94.000000   82.000000\n",
       "19    20       1.0   16736.882281     FORMER       HEAVY   0.466667       0.0   59.8  158.9  23.683909   NORMAL         0    0          0  102.666667   58.666667"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "OUT = Path(\"/Users/dengshuyue/Desktop/SDOH/analysis/output\")\n",
    "p = OUT / \"cov_core_1999_2023.parquet\"\n",
    "\n",
    "# schema/meta without loading full table\n",
    "pf = pq.ParquetFile(p)\n",
    "print(pf.metadata)          # row count, row groups, created by, etc.\n",
    "print(pf.schema)            # column names + types\n",
    "\n",
    "# show a few useful columns\n",
    "cols = [\"SEQN\",\"SDDSRVYR\",\"WTMEC2YR\",\"SMK_STATUS\",\"ALCOHOL_CAT\",\"LTPA\",\"METSCORE\",\n",
    "        \"BMXWT\",\"BMXHT\",\"BMI\",\"BMI_CLAS\",\"DIABETES\",\"HTN\",\"HIGH_CHOL\",\"SBP\",\"DBP\"]\n",
    "cols = [c for c in cols if c in pf.schema.names]  # keep only those present\n",
    "df = pd.read_parquet(p, columns=cols)\n",
    "display(df.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c04dd7a-0295-4035-94d0-7201faad53d3",
   "metadata": {},
   "source": [
    "#### check what cycle are included "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a67767f2-cede-4dfb-ac2e-0edd4dd4db46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cycles present: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 66]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "OUT = Path(\"/Users/dengshuyue/Desktop/SDOH/analysis/output\")\n",
    "p = OUT / \"cov_core_1999_2023.parquet\"\n",
    "\n",
    "cycles = (\n",
    "    pd.read_parquet(p, columns=[\"SDDSRVYR\"])   # loads only this column\n",
    "      .dropna()\n",
    "      .astype({\"SDDSRVYR\":\"Int64\"})            # tidy types\n",
    "      [\"SDDSRVYR\"]\n",
    "      .sort_values()\n",
    "      .unique()\n",
    ")\n",
    "print(\"Cycles present:\", cycles.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbe150c-050d-4a9d-abf6-054e5eafc287",
   "metadata": {},
   "source": [
    "#### Preview rows for a specific cycle (fast, column-pruned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "187d2249-8b6e-4c59-87b6-d2f4c29ec82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cycle 66: rows=15560\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEQN</th>\n",
       "      <th>SDDSRVYR</th>\n",
       "      <th>WTMEC2YR</th>\n",
       "      <th>SMK_STATUS</th>\n",
       "      <th>ALCOHOL_CAT</th>\n",
       "      <th>LTPA</th>\n",
       "      <th>METSCORE</th>\n",
       "      <th>BMXWT</th>\n",
       "      <th>BMXHT</th>\n",
       "      <th>BMI</th>\n",
       "      <th>BMI_CLAS</th>\n",
       "      <th>DIABETES</th>\n",
       "      <th>HTN</th>\n",
       "      <th>HIGH_CHOL</th>\n",
       "      <th>SBP</th>\n",
       "      <th>DBP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>109263</td>\n",
       "      <td>66.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>109264</td>\n",
       "      <td>66.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>109265</td>\n",
       "      <td>66.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>109266</td>\n",
       "      <td>66.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NONE</td>\n",
       "      <td>48.0</td>\n",
       "      <td>2880.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>109267</td>\n",
       "      <td>66.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72.0</td>\n",
       "      <td>4320.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>109268</td>\n",
       "      <td>66.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>109269</td>\n",
       "      <td>66.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>109270</td>\n",
       "      <td>66.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>109271</td>\n",
       "      <td>66.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NONE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>109272</td>\n",
       "      <td>66.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>109273</td>\n",
       "      <td>66.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NONE</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1920.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>109274</td>\n",
       "      <td>66.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NONE</td>\n",
       "      <td>12.0</td>\n",
       "      <td>720.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>109275</td>\n",
       "      <td>66.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>109276</td>\n",
       "      <td>66.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>109277</td>\n",
       "      <td>66.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>109278</td>\n",
       "      <td>66.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>109279</td>\n",
       "      <td>66.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>109280</td>\n",
       "      <td>66.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>109281</td>\n",
       "      <td>66.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>109282</td>\n",
       "      <td>66.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NONE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      SEQN  SDDSRVYR  WTMEC2YR SMK_STATUS ALCOHOL_CAT  LTPA  METSCORE  BMXWT  BMXHT  BMI BMI_CLAS  DIABETES   HTN  HIGH_CHOL  SBP  DBP\n",
       "0   109263      66.0       NaN        NaN         NaN   NaN       NaN    NaN    NaN  NaN     <NA>      <NA>  <NA>       <NA>  NaN  NaN\n",
       "1   109264      66.0       NaN        NaN         NaN   NaN       NaN    NaN    NaN  NaN     <NA>      <NA>  <NA>       <NA>  NaN  NaN\n",
       "2   109265      66.0       NaN        NaN         NaN   NaN       NaN    NaN    NaN  NaN     <NA>      <NA>  <NA>       <NA>  NaN  NaN\n",
       "3   109266      66.0       NaN        NaN        NONE  48.0    2880.0    NaN    NaN  NaN     <NA>      <NA>  <NA>       <NA>  NaN  NaN\n",
       "4   109267      66.0       NaN        NaN         NaN  72.0    4320.0    NaN    NaN  NaN     <NA>      <NA>  <NA>       <NA>  NaN  NaN\n",
       "5   109268      66.0       NaN        NaN         NaN   0.0       0.0    NaN    NaN  NaN     <NA>      <NA>  <NA>       <NA>  NaN  NaN\n",
       "6   109269      66.0       NaN        NaN         NaN   NaN       NaN    NaN    NaN  NaN     <NA>      <NA>  <NA>       <NA>  NaN  NaN\n",
       "7   109270      66.0       NaN        NaN         NaN   NaN       NaN    NaN    NaN  NaN     <NA>      <NA>  <NA>       <NA>  NaN  NaN\n",
       "8   109271      66.0       NaN        NaN        NONE   0.0       0.0    NaN    NaN  NaN     <NA>      <NA>  <NA>       <NA>  NaN  NaN\n",
       "9   109272      66.0       NaN        NaN         NaN   NaN       NaN    NaN    NaN  NaN     <NA>      <NA>  <NA>       <NA>  NaN  NaN\n",
       "10  109273      66.0       NaN        NaN        NONE  32.0    1920.0    NaN    NaN  NaN     <NA>      <NA>  <NA>       <NA>  NaN  NaN\n",
       "11  109274      66.0       NaN        NaN        NONE  12.0     720.0    NaN    NaN  NaN     <NA>      <NA>  <NA>       <NA>  NaN  NaN\n",
       "12  109275      66.0       NaN        NaN         NaN   NaN       NaN    NaN    NaN  NaN     <NA>      <NA>  <NA>       <NA>  NaN  NaN\n",
       "13  109276      66.0       NaN        NaN         NaN   NaN       NaN    NaN    NaN  NaN     <NA>      <NA>  <NA>       <NA>  NaN  NaN\n",
       "14  109277      66.0       NaN        NaN         NaN   NaN       NaN    NaN    NaN  NaN     <NA>      <NA>  <NA>       <NA>  NaN  NaN\n",
       "15  109278      66.0       NaN        NaN         NaN   NaN       NaN    NaN    NaN  NaN     <NA>      <NA>  <NA>       <NA>  NaN  NaN\n",
       "16  109279      66.0       NaN        NaN         NaN   NaN       NaN    NaN    NaN  NaN     <NA>      <NA>  <NA>       <NA>  NaN  NaN\n",
       "17  109280      66.0       NaN        NaN         NaN   NaN       NaN    NaN    NaN  NaN     <NA>      <NA>  <NA>       <NA>  NaN  NaN\n",
       "18  109281      66.0       NaN        NaN         NaN   NaN       NaN    NaN    NaN  NaN     <NA>      <NA>  <NA>       <NA>  NaN  NaN\n",
       "19  109282      66.0       NaN        NaN        NONE   0.0       0.0    NaN    NaN  NaN     <NA>      <NA>  <NA>       <NA>  NaN  NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyarrow.dataset as ds\n",
    "import pyarrow.compute as pc\n",
    "import pandas as pd\n",
    "\n",
    "cols = [\"SEQN\",\"SDDSRVYR\",\"WTMEC2YR\",\"SMK_STATUS\",\"ALCOHOL_CAT\",\"LTPA\",\"METSCORE\",\n",
    "        \"BMXWT\",\"BMXHT\",\"BMI\",\"BMI_CLAS\",\"DIABETES\",\"HTN\",\"HIGH_CHOL\",\"SBP\",\"DBP\"]\n",
    "\n",
    "target_cycle = 66  # ← change this to whatever cycle you want\n",
    "\n",
    "dataset = ds.dataset(str(p))  # single-file dataset is fine\n",
    "table = dataset.to_table(\n",
    "    filter = pc.field(\"SDDSRVYR\") == target_cycle,\n",
    "    columns = [c for c in cols if c in dataset.schema.names],\n",
    ")\n",
    "df_cycle = table.to_pandas()\n",
    "\n",
    "print(f\"Cycle {target_cycle}: rows={len(df_cycle)}\")\n",
    "display(df_cycle.head(20))\n",
    "\n",
    "## currently extend PA to 2023 \n",
    "## also need to extend all other cov to 2023 later by the same method \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2469fac9-c9c9-4509-9544-1d4d71e0f24c",
   "metadata": {},
   "source": [
    "#### Notes \n",
    "- currently extend PA to 2023 \n",
    "- also need to extend all other cov to 2023 later by the same method \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862e5d47-1e6e-42df-9e2d-02ebb0e563b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7434f9c-61aa-4683-9ae6-b60470438482",
   "metadata": {},
   "source": [
    "#### check PA by cycle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6173484a-76ab-49e8-918d-d972133c1d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All cycles in CORE:\n",
      " SDDSRVYR\n",
      "1.0      9965\n",
      "2.0     11039\n",
      "3.0     10122\n",
      "4.0     10348\n",
      "5.0     10149\n",
      "6.0     10537\n",
      "7.0      9756\n",
      "8.0     10175\n",
      "9.0      9971\n",
      "10.0     9254\n",
      "12.0    11933\n",
      "66.0    15560\n",
      "Name: rows, dtype: int64\n",
      "\n",
      "Cycles with PA (non-missing LTPA):\n",
      " SDDSRVYR\n",
      "1.0     4880\n",
      "2.0     5411\n",
      "3.0     5041\n",
      "4.0     4979\n",
      "5.0     5935\n",
      "6.0     6218\n",
      "7.0     5560\n",
      "8.0     5769\n",
      "9.0     5719\n",
      "10.0    5569\n",
      "12.0    8070\n",
      "66.0    9677\n",
      "Name: rows_with_pa, dtype: int64\n",
      "\n",
      "PA coverage by cycle (% of rows with LTPA):\n",
      " SDDSRVYR\n",
      "1.0     49.0\n",
      "2.0     49.0\n",
      "3.0     49.8\n",
      "4.0     48.1\n",
      "5.0     58.5\n",
      "6.0     59.0\n",
      "7.0     57.0\n",
      "8.0     56.7\n",
      "9.0     57.4\n",
      "10.0    60.2\n",
      "12.0    67.6\n",
      "66.0    62.2\n",
      "Name: pa_coverage_pct, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# check PA by cycle \n",
    "# 66 is 17-20\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "core = pd.read_parquet(Path(CONFIG.out_dir) / CONFIG.cov_core)\n",
    "\n",
    "# All cycles present\n",
    "by_cycle = core.groupby(\"SDDSRVYR\").size().rename(\"rows\").sort_index()\n",
    "print(\"All cycles in CORE:\\n\", by_cycle)\n",
    "\n",
    "# Cycles with PA available (LTPA non-missing)\n",
    "pa_cycles = (core[core[\"LTPA\"].notna()]\n",
    "             .groupby(\"SDDSRVYR\").size().rename(\"rows_with_pa\").sort_index())\n",
    "print(\"\\nCycles with PA (non-missing LTPA):\\n\", pa_cycles)\n",
    "\n",
    "# Coverage (% rows with PA per cycle)\n",
    "cov = (pa_cycles / by_cycle * 100).round(1).rename(\"pa_coverage_pct\")\n",
    "print(\"\\nPA coverage by cycle (% of rows with LTPA):\\n\", cov.fillna(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8891ad30-fbcd-4a56-97d5-24d49fddb33e",
   "metadata": {},
   "source": [
    "#### check PA missing by age "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861e715c-7e60-4687-b076-e792ccc656ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30176f7c-a8d6-4622-84b9-c6e9b962f393",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "26c35369-18e0-42ab-a71b-2ad8d81feda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All ages:\n",
      "            rows  rows_with_pa  pa_cov_pct_all\n",
      "SDDSRVYR                                     \n",
      "1.0        9965          4880            49.0\n",
      "2.0       11039          5411            49.0\n",
      "3.0       10122          5041            49.8\n",
      "4.0       10348          4979            48.1\n",
      "5.0       10149          5935            58.5\n",
      "6.0       10537          6218            59.0\n",
      "7.0        9756          5560            57.0\n",
      "8.0       10175          5769            56.7\n",
      "9.0        9971          5719            57.4\n",
      "10.0       9254          5569            60.2\n",
      "12.0      11933          8070            67.6\n",
      "66.0      15560          9677            62.2\n",
      "\n",
      "Age ≥12:\n",
      "            rows  rows_with_pa  pa_cov_pct_age12+\n",
      "SDDSRVYR                                        \n",
      "1.0        7295          4880               66.9\n",
      "2.0        7898          5411               68.5\n",
      "3.0        7344          5041               68.6\n",
      "4.0        7267          4979               68.5\n",
      "5.0        7173          5935               82.7\n",
      "6.0        7557          6218               82.3\n",
      "7.0        6833          5560               81.4\n",
      "8.0        7201          5769               80.1\n",
      "9.0        7035          5719               81.3\n",
      "10.0       6763          5569               82.3\n",
      "12.0       9388          8070               86.0\n",
      "66.0      11233          9677               86.1\n",
      "\n",
      "Age ≥18:\n",
      "           rows  rows_with_pa  pa_cov_pct_age18+\n",
      "SDDSRVYR                                       \n",
      "1.0       5448          4880               89.6\n",
      "2.0       5993          5411               90.3\n",
      "3.0       5620          5041               89.7\n",
      "4.0       5563          4979               89.5\n",
      "5.0       6228          5935               95.3\n",
      "6.0       6527          6218               95.3\n",
      "7.0       5864          5560               94.8\n",
      "8.0       6113          5769               94.4\n",
      "9.0       5992          5719               95.4\n",
      "10.0      5856          5569               95.1\n",
      "12.0      8153          8070               99.0\n",
      "66.0      9693          9677               99.8\n"
     ]
    }
   ],
   "source": [
    "# check PA missing by age \n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "core = pd.read_parquet(Path(CONFIG.out_dir) / CONFIG.cov_core)\n",
    "demo = pd.read_parquet(CONFIG.demo_9923)[[\"SEQN\",\"RIDAGEYR\",\"SDDSRVYR\"]]\n",
    "\n",
    "# attach age\n",
    "core = core.merge(demo, on=[\"SEQN\",\"SDDSRVYR\"], how=\"left\")\n",
    "\n",
    "def coverage(df, label):\n",
    "    by_cycle = df.groupby(\"SDDSRVYR\").size().rename(\"rows\")\n",
    "    with_pa  = df[df[\"LTPA\"].notna()].groupby(\"SDDSRVYR\").size().rename(\"rows_with_pa\")\n",
    "    cov = (with_pa / by_cycle * 100).round(1).rename(f\"pa_cov_pct_{label}\")\n",
    "    return pd.concat([by_cycle, with_pa, cov], axis=1).fillna(0).sort_index()\n",
    "\n",
    "print(\"All ages:\\n\", coverage(core, \"all\"))\n",
    "\n",
    "# Typical PAQ eligibility (NHANES self-report) is ≥12; many analyses use ≥18\n",
    "print(\"\\nAge ≥12:\\n\", coverage(core[core[\"RIDAGEYR\"] >= 12], \"age12+\"))\n",
    "print(\"\\nAge ≥18:\\n\", coverage(core[core[\"RIDAGEYR\"] >= 18], \"age18+\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb5e06a-17e0-46c5-a7d8-f5bcf22961f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
