{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69e559e5",
   "metadata": {},
   "source": [
    "\n",
    "# 00 ‚Äî DEMO + Mortality + (optional) SDOH Core\n",
    "\n",
    "This notebook:\n",
    "1. Builds/loads the **DEMO 1999‚Äì2023** stack (local-first; optional CDC download).\n",
    "2. Loads **mortality-linked** file (1999‚Äì2018) from your `data/` folder and merges with DEMO.\n",
    "3. Optionally merges the result with your **covariate core** (`output/cov_core_1999_2023.parquet`) if present.\n",
    "4. Writes tidy outputs to `output/`.\n",
    "\n",
    "> **Tip:** Change `BASE` below if your project root is different.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a105ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: /Users/dengshuyue/Desktop/SDOH/analysis\n",
      "Data dir exists: True\n",
      "Output dir exists: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %% Setup\n",
    "from pathlib import Path\n",
    "import os, warnings, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Project root (EDIT if needed)\n",
    "BASE = Path(\"/Users/dengshuyue/Desktop/SDOH/analysis\")\n",
    "\n",
    "# Folders (created only when saving new files)\n",
    "DATA = BASE / \"data\"\n",
    "OUT  = BASE / \"output\"\n",
    "(DATA / \"nhanes_by_module\" / \"DEMO\").mkdir(parents=True, exist_ok=True)\n",
    "(DATA / \"cov\").mkdir(parents=True, exist_ok=True)\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Toggle: allow downloading DEMO XPTs from CDC if not found locally\n",
    "ALLOW_DOWNLOAD = True\n",
    "\n",
    "# Display prefs\n",
    "pd.options.display.max_rows = 60\n",
    "pd.options.display.max_columns = 120\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"ROOT:\", BASE)\n",
    "print(\"Data dir exists:\", DATA.exists())\n",
    "print(\"Output dir exists:\", OUT.exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a92077e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% Helpers\n",
    "import requests\n",
    "\n",
    "def nhanes_demo_urls():\n",
    "    return {\n",
    "        \"1999-2000\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/1999/DataFiles/DEMO.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/1999-2000/DEMO.XPT\",\n",
    "        ],\n",
    "        \"2001-2002\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2001/DataFiles/DEMO_B.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2001-2002/DEMO_B.XPT\",\n",
    "        ],\n",
    "        \"2003-2004\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2003/DataFiles/DEMO_C.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2003-2004/DEMO_C.XPT\",\n",
    "        ],\n",
    "        \"2005-2006\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2005/DataFiles/DEMO_D.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2005-2006/DEMO_D.XPT\",\n",
    "        ],\n",
    "        \"2007-2008\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2007/DataFiles/DEMO_E.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2007-2008/DEMO_E.XPT\",\n",
    "        ],\n",
    "        \"2009-2010\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2009/DataFiles/DEMO_F.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2009-2010/DEMO_F.XPT\",\n",
    "        ],\n",
    "        \"2011-2012\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2011/DataFiles/DEMO_G.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2011-2012/DEMO_G.XPT\",\n",
    "        ],\n",
    "        \"2013-2014\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2013/DataFiles/DEMO_H.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2013-2014/DEMO_H.XPT\",\n",
    "        ],\n",
    "        \"2015-2016\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2015/DataFiles/DEMO_I.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2015-2016/DEMO_I.XPT\",\n",
    "        ],\n",
    "        \"2017-2018\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2017/DataFiles/DEMO_J.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2017-2018/DEMO_J.XPT\",\n",
    "        ],\n",
    "        \"2017-March 2020 (pre-pandemic)\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2017/DataFiles/P_DEMO.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2017-2020/P_DEMO.XPT\",\n",
    "        ],\n",
    "        \"August 2021‚ÄìAugust 2023\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2021/DataFiles/DEMO_L.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2021-2023/DEMO_L.XPT\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2021/DataFiles/DEMO_Q.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2021-2023/DEMO_Q.XPT\",\n",
    "        ],\n",
    "    }\n",
    "\n",
    "def candidates_from_urls(urls):\n",
    "    out, seen = [], set()\n",
    "    for url in urls:\n",
    "        name = Path(url).name\n",
    "        for variant in (name, name.upper(), name.lower(), name.capitalize()):\n",
    "            if variant not in seen:\n",
    "                seen.add(variant); out.append(variant)\n",
    "    return out\n",
    "\n",
    "NHANES_URLS = nhanes_demo_urls()\n",
    "LOCAL_CANDIDATES = {c: candidates_from_urls(urls) for c, urls in NHANES_URLS.items()}\n",
    "\n",
    "def find_first_under(base: Path, patterns):\n",
    "    for pat in patterns:\n",
    "        hits = list(base.rglob(pat))\n",
    "        if hits:\n",
    "            return hits[0]\n",
    "    return None\n",
    "\n",
    "def download_to(path: Path, url: str, timeout=90):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    headers = {\"User-Agent\": \"nhanes-fetch/1.0\"}\n",
    "    last = None\n",
    "    for _ in range(2):\n",
    "        try:\n",
    "            with requests.get(url, headers=headers, timeout=timeout, stream=True) as r:\n",
    "                r.raise_for_status()\n",
    "                tmp = path.with_suffix(path.suffix + \".downloading\")\n",
    "                with open(tmp, \"wb\") as f:\n",
    "                    for chunk in r.iter_content(1<<15):\n",
    "                        if chunk: f.write(chunk)\n",
    "                tmp.rename(path)\n",
    "            return path\n",
    "        except Exception as e:\n",
    "            last = e\n",
    "    raise last\n",
    "\n",
    "def read_demo_file(p: Path) -> pd.DataFrame:\n",
    "    if p.suffix.lower() == \".xpt\":\n",
    "        try:\n",
    "            import pyreadstat\n",
    "            df, _ = pyreadstat.read_xport(p)\n",
    "        except Exception:\n",
    "            df = pd.read_sas(p, format=\"xport\")\n",
    "    elif p.suffix.lower() == \".sas7bdat\":\n",
    "        try:\n",
    "            import pyreadstat\n",
    "            df, _ = pyreadstat.read_sas7bdat(p)\n",
    "        except Exception:\n",
    "            df = pd.read_sas(p, format=\"sas7bdat\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported DEMO file type: {p.suffix}\")\n",
    "    df.columns = [c.upper() for c in df.columns]\n",
    "    return df\n",
    "\n",
    "def ensure_demo_file(cycle_label: str) -> Path:\n",
    "    local = find_first_under(DATA, LOCAL_CANDIDATES[cycle_label])\n",
    "    if local:\n",
    "        return local\n",
    "    if not ALLOW_DOWNLOAD:\n",
    "        raise FileNotFoundError(f\"DEMO file not found locally for {cycle_label} and downloads are disabled.\")\n",
    "    for url in NHANES_URLS[cycle_label]:\n",
    "        out = DATA / \"nhanes_by_module\" / \"DEMO\" / Path(url).name\n",
    "        try:\n",
    "            print(f\"‚¨áÔ∏è  Download {cycle_label}: {out.name}\")\n",
    "            return download_to(out, url)\n",
    "        except Exception as e:\n",
    "            print(\"   ‚ö†Ô∏è\", e)\n",
    "    raise FileNotFoundError(f\"No DEMO file available for {cycle_label}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bc52271",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% Recodes + normalization\n",
    "def recode_L_to_4(s):\n",
    "    s = pd.to_numeric(s, errors=\"coerce\")\n",
    "    out = pd.Series(pd.NA, index=s.index, dtype=\"Int64\")\n",
    "    out = out.where(~s.isin([1, 6]), 1)   # married/partner\n",
    "    out = out.where(~s.isin([3, 4]), 2)   # widowed/divorced\n",
    "    out = out.where(~(s == 2), 3)         # never married\n",
    "    out = out.where(~(s == 5), 4)         # separated\n",
    "    out = out.where(~s.isin([77, 99]), pd.NA)\n",
    "    return out\n",
    "\n",
    "def recode_L_to_3(s):\n",
    "    s = pd.to_numeric(s, errors=\"coerce\")\n",
    "    out = pd.Series(pd.NA, index=s.index, dtype=\"Int64\")\n",
    "    out = out.where(~s.isin([1, 6]), 1)\n",
    "    out = out.where(~s.isin([3, 4, 5]), 2)\n",
    "    out = out.where(~(s == 2), 3)\n",
    "    out = out.where(~s.isin([77, 99]), pd.NA)\n",
    "    return out\n",
    "\n",
    "def recode_Z_to_3(s):\n",
    "    s = pd.to_numeric(s, errors=\"coerce\")\n",
    "    out = pd.Series(pd.NA, index=s.index, dtype=\"Int64\")\n",
    "    out = out.where(~(s == 1), 1)\n",
    "    out = out.where(~(s == 2), 2)\n",
    "    out = out.where(~(s == 3), 3)\n",
    "    out = out.where(~s.isin([77, 99]), pd.NA)\n",
    "    return out\n",
    "\n",
    "def compute_marriage_cols(df_upper: pd.DataFrame):\n",
    "    hasL = \"DMDMARTL\" in df_upper.columns\n",
    "    hasZ = \"DMDMARTZ\" in df_upper.columns\n",
    "    M4 = pd.Series(pd.NA, index=df_upper.index, dtype=\"Int64\")\n",
    "    M3 = pd.Series(pd.NA, index=df_upper.index, dtype=\"Int64\")\n",
    "    if hasL:\n",
    "        M4 = recode_L_to_4(df_upper[\"DMDMARTL\"])\n",
    "        M3 = recode_L_to_3(df_upper[\"DMDMARTL\"])\n",
    "    elif hasZ:\n",
    "        M3 = recode_Z_to_3(df_upper[\"DMDMARTZ\"])\n",
    "    return M4, M3\n",
    "\n",
    "def normalize_survey_and_sex(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    d = df.copy()\n",
    "    d.columns = [c.upper() for c in d.columns]\n",
    "    for c in [\"RIAGENDR\",\"DMDSEX\",\"SEX\",\"GENDER\",\"RIAGENDER\"]:\n",
    "        if c in d.columns:\n",
    "            d[\"RIAGENDR\"] = pd.to_numeric(d[c], errors=\"coerce\")\n",
    "            break\n",
    "    for c in [\"WTMEC2YR\",\"WTMEC2YR_P\",\"WTMEC2YRA\"]:\n",
    "        if c in d.columns:\n",
    "            d[\"WTMEC2YR\"] = pd.to_numeric(d[c], errors=\"coerce\")\n",
    "            break\n",
    "    for c in [\"SDMVPSU\",\"SDMVSTRA\",\"SDDSRVYR\",\"SEQN\"]:\n",
    "        if c in d.columns:\n",
    "            d[c] = pd.to_numeric(d[c], errors=\"coerce\")\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9544cfab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved DEMO stack:\n",
      "  ‚Ä¢ /Users/dengshuyue/Desktop/SDOH/analysis/data/demo9923.pkl\n",
      "  ‚Ä¢ /Users/dengshuyue/Desktop/SDOH/analysis/data/cov/demo9923.parquet\n",
      "Rows: 128809 | Unique SEQN: 128809\n",
      "Columns: ['SEQN', 'RIDAGEYR', 'SDDSRVYR', 'SDMVPSU', 'SDMVSTRA', 'WTMEC2YR', 'CYCLE', 'MARRIAGE', 'MARRIAGE3', 'RIAGENDR', 'DMDHHSIZ']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %% Build DEMO 1999‚Äì2023\n",
    "DEMO_CYCLES = [\n",
    "    \"1999-2000\",\"2001-2002\",\"2003-2004\",\"2005-2006\",\"2007-2008\",\n",
    "    \"2009-2010\",\"2011-2012\",\"2013-2014\",\"2015-2016\",\"2017-2018\",\n",
    "    \"2017-March 2020 (pre-pandemic)\",\"August 2021‚ÄìAugust 2023\",\n",
    "]\n",
    "\n",
    "demo_parts, missing = [], []\n",
    "for cyc in DEMO_CYCLES:\n",
    "    try:\n",
    "        p = ensure_demo_file(cyc)\n",
    "    except FileNotFoundError as e:\n",
    "        print(\"‚ö†Ô∏è\", e); missing.append(cyc); continue\n",
    "    df = read_demo_file(p)\n",
    "    df[\"CYCLE\"] = cyc\n",
    "    M4, M3 = compute_marriage_cols(df)\n",
    "    df[\"MARRIAGE\"]  = M4\n",
    "    df[\"MARRIAGE3\"] = M3\n",
    "    df = normalize_survey_and_sex(df)\n",
    "    keep = [c for c in [\n",
    "        \"SEQN\",\"RIDAGEYR\",\"SDDSRVYR\",\"SDMVPSU\",\"SDMVSTRA\",\"WTMEC2YR\",\n",
    "        \"CYCLE\",\"MARRIAGE\",\"MARRIAGE3\",\"RIAGENDR\",\"DMDHHSIZ\"\n",
    "    ] if c in df.columns]\n",
    "    demo_parts.append(df[keep])\n",
    "\n",
    "if missing:\n",
    "    print(\"‚ö†Ô∏è Missing DEMO cycles (not found):\", missing)\n",
    "if not demo_parts:\n",
    "    raise RuntimeError(\"No DEMO pieces available.\")\n",
    "\n",
    "demo9923 = pd.concat(demo_parts, ignore_index=True).drop_duplicates(\"SEQN\", keep=\"first\")\n",
    "\n",
    "# Save for reuse\n",
    "pkl_out  = DATA / \"demo9923.pkl\"\n",
    "parq_out = DATA / \"cov\" / \"demo9923.parquet\"\n",
    "demo9923.to_pickle(pkl_out)\n",
    "demo9923.to_parquet(parq_out, index=False)\n",
    "\n",
    "print(\"‚úÖ Saved DEMO stack:\")\n",
    "print(\"  ‚Ä¢\", pkl_out)\n",
    "print(\"  ‚Ä¢\", parq_out)\n",
    "print(\"Rows:\", len(demo9923), \"| Unique SEQN:\", demo9923[\"SEQN\"].nunique())\n",
    "print(\"Columns:\", demo9923.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fbe6b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mortality file: /Users/dengshuyue/Desktop/SDOH/analysis/data/less_important/mortality9918.sas7bdat\n",
      "\n",
      "üìä NHANES cycles in mortality-linked (by SDDSRVYR code):\n",
      "SDDSRVYR\n",
      "1.0     4973\n",
      "2.0     5586\n",
      "3.0     5293\n",
      "4.0     5332\n",
      "5.0     5989\n",
      "6.0     6346\n",
      "7.0     5603\n",
      "8.0     5913\n",
      "9.0     5720\n",
      "10.0    5498\n",
      "Name: count, dtype: int64\n",
      "\n",
      "‚è±Ô∏è Survival summary:\n",
      "  N (unique SEQN): 56253\n",
      "  Events (%): 14.87\n",
      "  TIME_Y (min/median/max): 0.0 9.416666666666666 20.75\n",
      "\n",
      "‚úÖ wrote /Users/dengshuyue/Desktop/SDOH/analysis/output/mort_demo_merged.parquet\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %% Mortality link (1999‚Äì2018)\n",
    "def find_mortality_file(base: Path) -> Path | None:\n",
    "    patterns = [\n",
    "        \"*mortality*.sas7bdat\",\"*mort*.sas7bdat\",\n",
    "        \"*mortality*.xpt\",\"*mort*.xpt\",\n",
    "        \"*mortality*.csv\",\"*mort*.csv\",\n",
    "        \"NHANES_1999_2019_LMF_public.csv\"\n",
    "    ]\n",
    "    for pat in patterns:\n",
    "        hits = list(base.rglob(pat))\n",
    "        if hits:\n",
    "            return hits[0]\n",
    "    return None\n",
    "\n",
    "mort_p = find_mortality_file(DATA)\n",
    "if mort_p is None:\n",
    "    raise FileNotFoundError(f\"No mortality file found under {DATA}. Place e.g. 'mortality9918.sas7bdat' or the LMF csv.\")\n",
    "\n",
    "print(\"Using mortality file:\", mort_p)\n",
    "sfx = mort_p.suffix.lower()\n",
    "if sfx == \".xpt\":\n",
    "    try:\n",
    "        import pyreadstat\n",
    "        mort, _ = pyreadstat.read_xport(mort_p)\n",
    "    except Exception:\n",
    "        mort = pd.read_sas(mort_p, format=\"xport\")\n",
    "elif sfx == \".sas7bdat\":\n",
    "    try:\n",
    "        import pyreadstat\n",
    "        mort, _ = pyreadstat.read_sas7bdat(mort_p)\n",
    "    except Exception:\n",
    "        mort = pd.read_sas(mort_p, format=\"sas7bdat\")\n",
    "elif sfx == \".csv\":\n",
    "    mort = pd.read_csv(mort_p)\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported mortality file type: {sfx}\")\n",
    "\n",
    "mort.columns = [c.upper() for c in mort.columns]\n",
    "keep_cols = [c for c in [\n",
    "    \"SEQN\",\"ELIGSTAT\",\"MORTSTAT\",\"PERMTH_EXM\",\"PERMTH_INT\",\n",
    "    \"UCOD_LEADING\",\"DIABETES\",\"HYPERTEN\",\"DODQTR\",\"DODYEAR\"\n",
    "] if c in mort.columns]\n",
    "mort = mort[keep_cols].copy()\n",
    "\n",
    "if \"ELIGSTAT\" in mort.columns:\n",
    "    mort = mort[mort[\"ELIGSTAT\"] == 1].copy()\n",
    "\n",
    "TIME_COL = \"PERMTH_EXM\" if \"PERMTH_EXM\" in mort.columns else (\"PERMTH_INT\" if \"PERMTH_INT\" in mort.columns else None)\n",
    "if TIME_COL is None:\n",
    "    raise ValueError(\"Neither PERMTH_EXM nor PERMTH_INT found in mortality file.\")\n",
    "\n",
    "mort[\"TIME_Y\"] = pd.to_numeric(mort[TIME_COL], errors=\"coerce\") / 12.0\n",
    "mort[\"EVENT\"]  = (mort[\"MORTSTAT\"] == 1).astype(\"Int64\")\n",
    "mort = mort[(mort[\"TIME_Y\"].notna()) & (mort[\"TIME_Y\"] >= 0)].copy()\n",
    "\n",
    "# Merge with DEMO (age + survey year)\n",
    "d = demo9923.copy()\n",
    "d[\"SDDSRVYR\"] = pd.to_numeric(d[\"SDDSRVYR\"], errors=\"coerce\")\n",
    "cols_merge = [c for c in [\"SEQN\",\"RIDAGEYR\",\"SDDSRVYR\",\"RIAGENDR\"] if c in d.columns]\n",
    "mort_demo = mort.merge(d[cols_merge], on=\"SEQN\", how=\"left\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\nüìä NHANES cycles in mortality-linked (by SDDSRVYR code):\")\n",
    "print(mort_demo[\"SDDSRVYR\"].value_counts(dropna=False).sort_index())\n",
    "\n",
    "print(\"\\n‚è±Ô∏è Survival summary:\")\n",
    "print(\"  N (unique SEQN):\", mort_demo[\"SEQN\"].nunique())\n",
    "evt_rate = float(mort_demo[\"EVENT\"].mean(skipna=True)) if mort_demo[\"EVENT\"].notna().any() else np.nan\n",
    "print(\"  Events (%):\", None if np.isnan(evt_rate) else round(100*evt_rate, 2))\n",
    "print(\"  TIME_Y (min/median/max):\",\n",
    "      np.nanmin(mort_demo[\"TIME_Y\"]),\n",
    "      np.nanmedian(mort_demo[\"TIME_Y\"]),\n",
    "      np.nanmax(mort_demo[\"TIME_Y\"]))\n",
    "\n",
    "# Save\n",
    "mort_demo_out = OUT / \"mort_demo_merged.parquet\"\n",
    "mort_demo.to_parquet(mort_demo_out, index=False)\n",
    "print(\"\\n‚úÖ wrote\", mort_demo_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "293b731f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ wrote /Users/dengshuyue/Desktop/SDOH/analysis/output/nhanes_mort_sdoh_core.parquet   (shape: (56253, 41) )\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %% Optional: merge with covariate core (if present)\n",
    "core_path = OUT / \"cov_core_1999_2023.parquet\"\n",
    "if core_path.exists():\n",
    "    core = pd.read_parquet(core_path)\n",
    "    core.columns = [c.upper() for c in core.columns]\n",
    "    merged = mort_demo.merge(core, on=\"SEQN\", how=\"left\", suffixes=(\"\", \"_CORE\"))\n",
    "    outp = OUT / \"nhanes_mort_sdoh_core.parquet\"\n",
    "    merged.to_parquet(outp, index=False)\n",
    "    print(\"‚úÖ wrote\", outp, \"  (shape:\", merged.shape, \")\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  Core file not found at\", core_path, \"‚Äî skipping this optional merge.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95de29c-517f-4d63-b759-e6afbaa1d40b",
   "metadata": {},
   "source": [
    "<h2>Sanity Check </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89c5934d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEMO shape: (128809, 11)\n",
      "DEMO columns: ['SEQN', 'RIDAGEYR', 'SDDSRVYR', 'SDMVPSU', 'SDMVSTRA', 'WTMEC2YR', 'CYCLE', 'MARRIAGE', 'MARRIAGE3', 'RIAGENDR', 'DMDHHSIZ'] ...\n",
      "mort_demo_merged shape: (56253, 13)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %% Quick checks\n",
    "print(\"DEMO shape:\", demo9923.shape)\n",
    "print(\"DEMO columns:\", demo9923.columns.tolist()[:15], \"...\")\n",
    "try:\n",
    "    md = pd.read_parquet(OUT / \"mort_demo_merged.parquet\")\n",
    "    print(\"mort_demo_merged shape:\", md.shape)\n",
    "except Exception as e:\n",
    "    print(\"Couldn't read mort_demo_merged:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2faa5ea4-b02d-4894-be26-4403f664403a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEQN</th>\n",
       "      <th>ELIGSTAT</th>\n",
       "      <th>MORTSTAT</th>\n",
       "      <th>PERMTH_EXM</th>\n",
       "      <th>PERMTH_INT</th>\n",
       "      <th>UCOD_LEADING</th>\n",
       "      <th>DIABETES</th>\n",
       "      <th>HYPERTEN</th>\n",
       "      <th>TIME_Y</th>\n",
       "      <th>EVENT</th>\n",
       "      <th>RIDAGEYR</th>\n",
       "      <th>SDDSRVYR</th>\n",
       "      <th>RIAGENDR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.750000</td>\n",
       "      <td>1</td>\n",
       "      <td>77.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>244.0</td>\n",
       "      <td>244.0</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>246.0</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.416667</td>\n",
       "      <td>0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>237.0</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.666667</td>\n",
       "      <td>0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>231.0</td>\n",
       "      <td>231.0</td>\n",
       "      <td>001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.250000</td>\n",
       "      <td>1</td>\n",
       "      <td>43.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.666667</td>\n",
       "      <td>0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>1</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.333333</td>\n",
       "      <td>1</td>\n",
       "      <td>81.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>231.0</td>\n",
       "      <td>231.0</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.250000</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.166667</td>\n",
       "      <td>1</td>\n",
       "      <td>85.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SEQN  ELIGSTAT  MORTSTAT  PERMTH_EXM  PERMTH_INT UCOD_LEADING  DIABETES  \\\n",
       "0   2.0       1.0       1.0       177.0       177.0          006       0.0   \n",
       "1   5.0       1.0       0.0       244.0       244.0                    NaN   \n",
       "2   6.0       1.0       0.0       245.0       246.0                    NaN   \n",
       "3   7.0       1.0       0.0       236.0       237.0                    NaN   \n",
       "4  10.0       1.0       1.0       231.0       231.0          001       0.0   \n",
       "5  12.0       1.0       0.0       236.0       236.0                    NaN   \n",
       "6  13.0       1.0       1.0        16.0        16.0          001       0.0   \n",
       "7  14.0       1.0       1.0       136.0       137.0          003       0.0   \n",
       "8  15.0       1.0       0.0       231.0       231.0                    NaN   \n",
       "9  16.0       1.0       1.0        62.0        63.0          002       0.0   \n",
       "\n",
       "   HYPERTEN     TIME_Y  EVENT  RIDAGEYR  SDDSRVYR  RIAGENDR  \n",
       "0       0.0  14.750000      1      77.0       1.0       1.0  \n",
       "1       NaN  20.333333      0      49.0       1.0       1.0  \n",
       "2       NaN  20.416667      0      19.0       1.0       2.0  \n",
       "3       NaN  19.666667      0      59.0       1.0       2.0  \n",
       "4       0.0  19.250000      1      43.0       1.0       1.0  \n",
       "5       NaN  19.666667      0      37.0       1.0       1.0  \n",
       "6       0.0   1.333333      1      70.0       1.0       1.0  \n",
       "7       0.0  11.333333      1      81.0       1.0       1.0  \n",
       "8       NaN  19.250000      0      38.0       1.0       2.0  \n",
       "9       0.0   5.166667      1      85.0       1.0       2.0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "389f4052-083f-42a4-9261-750903eee717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mortality file loaded.\n",
      "mort shape: (56253, 10)\n",
      "Total rows: 56,253 | Unique SEQN: 56,253\n",
      "After ELIGSTAT==1 (or skip if missing): 56,253\n",
      "Time present: EXAM=56,253 | INT=56,253 | either=56,253 | both=56,253 | neither=0\n",
      "Rows with EXAM months (>=0): 56,253\n",
      "Rows with EXAM-or-INT months (>=0): 56,253\n",
      "Unique SEQN after merge to DEMO ‚Üí EXAM-only: 56,253 | EXAM-or-INT: 56,253\n"
     ]
    }
   ],
   "source": [
    "# ==== Mortality row checks ====\n",
    "import pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# --- 0) Load mort if not already loaded (searches under your project /data) ---\n",
    "if \"mort\" not in globals():\n",
    "    ROOT = Path(\"/Users/dengshuyue/Desktop/SDOH/analysis\")\n",
    "    DATA = ROOT / \"data\"\n",
    "    pats = [\"*mortality*.sas7bdat\",\"*mort*.sas7bdat\",\"*mortality*.xpt\",\"*mort*.xpt\",\"*mortality*.csv\",\"*mort*.csv\"]\n",
    "    mort_path = next((p for pat in pats for p in DATA.rglob(pat)), None)\n",
    "    if mort_path is None:\n",
    "        raise FileNotFoundError(\"No mortality file found under data/.\")\n",
    "    suf = mort_path.suffix.lower()\n",
    "    if suf == \".xpt\":\n",
    "        try:\n",
    "            import pyreadstat\n",
    "            mort, _ = pyreadstat.read_xport(mort_path)\n",
    "        except Exception:\n",
    "            mort = pd.read_sas(mort_path, format=\"xport\")\n",
    "    elif suf == \".sas7bdat\":\n",
    "        try:\n",
    "            import pyreadstat\n",
    "            mort, _ = pyreadstat.read_sas7bdat(mort_path)\n",
    "        except Exception:\n",
    "            mort = pd.read_sas(mort_path, format=\"sas7bdat\")\n",
    "    elif suf == \".csv\":\n",
    "        mort = pd.read_csv(mort_path)\n",
    "    mort.columns = mort.columns.str.upper()\n",
    "\n",
    "print(\"Mortality file loaded.\")\n",
    "print(\"mort shape:\", mort.shape)\n",
    "\n",
    "# --- 1) Basic counts ---\n",
    "tot_rows = len(mort)\n",
    "uniq_seqn = mort[\"SEQN\"].nunique() if \"SEQN\" in mort.columns else np.nan\n",
    "print(f\"Total rows: {tot_rows:,} | Unique SEQN: {uniq_seqn:,}\")\n",
    "\n",
    "# --- 2) Eligibility filter ---\n",
    "if \"ELIGSTAT\" in mort.columns:\n",
    "    m_elig = mort[mort[\"ELIGSTAT\"] == 1].copy()\n",
    "else:\n",
    "    m_elig = mort.copy()\n",
    "print(f\"After ELIGSTAT==1 (or skip if missing): {len(m_elig):,}\")\n",
    "\n",
    "# --- 3) Time availability (EXAM / INTERVIEW) ---\n",
    "for c in [\"PERMTH_EXM\",\"PERMTH_INT\"]:\n",
    "    if c in m_elig.columns:\n",
    "        m_elig[c] = pd.to_numeric(m_elig[c], errors=\"coerce\")\n",
    "\n",
    "has_exm = m_elig[\"PERMTH_EXM\"].notna().sum() if \"PERMTH_EXM\" in m_elig.columns else 0\n",
    "has_int = m_elig[\"PERMTH_INT\"].notna().sum() if \"PERMTH_INT\" in m_elig.columns else 0\n",
    "if {\"PERMTH_EXM\",\"PERMTH_INT\"}.issubset(m_elig.columns):\n",
    "    has_either = m_elig[[\"PERMTH_EXM\",\"PERMTH_INT\"]].notna().any(axis=1).sum()\n",
    "    has_both   = m_elig[[\"PERMTH_EXM\",\"PERMTH_INT\"]].notna().all(axis=1).sum()\n",
    "    has_neither= len(m_elig) - has_either\n",
    "else:\n",
    "    # Only one column present\n",
    "    only = \"PERMTH_EXM\" if \"PERMTH_EXM\" in m_elig.columns else (\"PERMTH_INT\" if \"PERMTH_INT\" in m_elig.columns else None)\n",
    "    has_either = m_elig[only].notna().sum() if only else 0\n",
    "    has_both = 0\n",
    "    has_neither = len(m_elig) - has_either\n",
    "\n",
    "print(f\"Time present: EXAM={has_exm:,} | INT={has_int:,} | either={has_either:,} | both={has_both:,} | neither={has_neither:,}\")\n",
    "\n",
    "# --- 4) Your current EXAM-only logic vs EXAM-or-INT fallback ---\n",
    "# EXAM-only (what caused 56,253 in your run)\n",
    "m_exm_only = m_elig[m_elig.get(\"PERMTH_EXM\").notna()] if \"PERMTH_EXM\" in m_elig.columns else m_elig.iloc[0:0]\n",
    "m_exm_only = m_exm_only[pd.to_numeric(m_exm_only[\"PERMTH_EXM\"], errors=\"coerce\").ge(0)]\n",
    "print(f\"Rows with EXAM months (>=0): {len(m_exm_only):,}\")\n",
    "\n",
    "# EXAM-or-INT (recommended)\n",
    "m_time = m_elig.copy()\n",
    "m_time[\"TIME_M\"] = np.where(\n",
    "    m_time.get(\"PERMTH_EXM\").notna() if \"PERMTH_EXM\" in m_time else False,\n",
    "    m_time.get(\"PERMTH_EXM\"),\n",
    "    m_time.get(\"PERMTH_INT\")\n",
    ")\n",
    "m_time[\"TIME_M\"] = pd.to_numeric(m_time[\"TIME_M\"], errors=\"coerce\")\n",
    "m_time = m_time[m_time[\"TIME_M\"].notna() & (m_time[\"TIME_M\"] >= 0)]\n",
    "print(f\"Rows with EXAM-or-INT months (>=0): {len(m_time):,}\")\n",
    "\n",
    "# --- 5) How many merge to DEMO (if available) ---\n",
    "if \"demo9923\" in globals():\n",
    "    d = demo9923.copy()\n",
    "    d.columns = d.columns.str.upper()\n",
    "    # EXAM-only merge\n",
    "    n_merge_exm = m_exm_only.merge(d[[\"SEQN\"]], on=\"SEQN\", how=\"inner\")[\"SEQN\"].nunique()\n",
    "    # EXAM-or-INT merge\n",
    "    n_merge_either = m_time.merge(d[[\"SEQN\"]], on=\"SEQN\", how=\"inner\")[\"SEQN\"].nunique()\n",
    "    print(f\"Unique SEQN after merge to DEMO ‚Üí EXAM-only: {n_merge_exm:,} | EXAM-or-INT: {n_merge_either:,}\")\n",
    "else:\n",
    "    print(\"demo9923 not in memory; skipping merge counts.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7841716e-3149-44ef-b153-bd50b78d2a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In mortality (by CYCLE):\n",
      "CYCLE\n",
      "1999-2000    4973\n",
      "2001-2002    5586\n",
      "2003-2004    5293\n",
      "2005-2006    5332\n",
      "2007-2008    5989\n",
      "2009-2010    6346\n",
      "2011-2012    5603\n",
      "2013-2014    5913\n",
      "2015-2016    5720\n",
      "2017-2018    5498\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Not in mortality (by CYCLE):\n",
      "CYCLE\n",
      "1999-2000                          4992\n",
      "2001-2002                          5453\n",
      "2003-2004                          4829\n",
      "2005-2006                          5016\n",
      "2007-2008                          4160\n",
      "2009-2010                          4191\n",
      "2011-2012                          4153\n",
      "2013-2014                          4262\n",
      "2015-2016                          4251\n",
      "2017-2018                          3756\n",
      "2017-March 2020 (pre-pandemic)    15560\n",
      "August 2021‚ÄìAugust 2023           11933\n",
      "Name: count, dtype: int64\n",
      "\n",
      "core_mort shape: (56253, 20)\n",
      "core_full shape: (128809, 19)\n"
     ]
    }
   ],
   "source": [
    "# --- Coverage by cycle ---\n",
    "mort_ids = set(mort[\"SEQN\"])\n",
    "demo_ids = set(demo9923[\"SEQN\"])\n",
    "\n",
    "demo_in_mort = demo9923[demo9923[\"SEQN\"].isin(mort_ids)]\n",
    "demo_only    = demo9923[~demo9923[\"SEQN\"].isin(mort_ids)]\n",
    "\n",
    "print(\"In mortality (by CYCLE):\")\n",
    "print(demo_in_mort[\"CYCLE\"].value_counts().sort_index(), \"\\n\")\n",
    "\n",
    "print(\"Not in mortality (by CYCLE):\")\n",
    "print(demo_only[\"CYCLE\"].value_counts().sort_index())\n",
    "\n",
    "# --- Build two cores ---\n",
    "# 1) Mortality-linked cohort only (inner)\n",
    "core_mort = demo9923.merge(mort, on=\"SEQN\", how=\"inner\")\n",
    "print(\"\\ncore_mort shape:\", core_mort.shape)\n",
    "\n",
    "# 2) Full DEMO with mortality columns when available (left)\n",
    "mort_slim = mort.copy()\n",
    "for c in [\"PERMTH_EXM\",\"PERMTH_INT\"]:\n",
    "    if c in mort_slim:\n",
    "        mort_slim[c] = pd.to_numeric(mort_slim[c], errors=\"coerce\")\n",
    "\n",
    "# EXAM-or-INT time in years\n",
    "if \"PERMTH_EXM\" in mort_slim.columns or \"PERMTH_INT\" in mort_slim.columns:\n",
    "    mort_slim[\"TIME_M\"] = mort_slim[\"PERMTH_EXM\"].where(mort_slim[\"PERMTH_EXM\"].notna(), mort_slim.get(\"PERMTH_INT\"))\n",
    "    mort_slim[\"TIME_Y\"] = mort_slim[\"TIME_M\"] / 12.0\n",
    "\n",
    "keep_cols = [\"SEQN\",\"MORTSTAT\",\"ELIGSTAT\",\"TIME_Y\",\"PERMTH_EXM\",\"PERMTH_INT\",\"UCOD_LEADING\",\"DIABETES\",\"HYPERTEN\",\"DODQTR\",\"DODYEAR\"]\n",
    "keep_cols = [c for c in keep_cols if c in mort_slim.columns]\n",
    "core_full = demo9923.merge(mort_slim[keep_cols], on=\"SEQN\", how=\"left\")\n",
    "print(\"core_full shape:\", core_full.shape)\n",
    "\n",
    "# Optional: save\n",
    "# core_mort.to_parquet(\"/Users/dengshuyue/Desktop/SDOH/analysis/output/core_mort.parquet\", index=False)\n",
    "# core_full.to_parquet(\"/Users/dengshuyue/Desktop/SDOH/analysis/output/core_full.parquet\", index=False)\n",
    "# print(\"\\nSaved:\\n - output/core_mort.parquet (mortality-linked only)\\n - output/core_full.parquet (all DEMO; mortality NA where missing)\")\n",
    "\n",
    "''''\n",
    "core_mort has 56,253 mortality-linked participants from 1999‚Äì2018; core_full has all 128,809 DEMO participants from 1999‚Äì2023. \n",
    "The gap is unlinked 1999‚Äì2018 cases plus no public mortality linkage for 2017‚ÄìMar 2020 and 2021‚Äì2023.\n",
    "''''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d9db90-5dcf-44a2-84d4-67c666882dc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
