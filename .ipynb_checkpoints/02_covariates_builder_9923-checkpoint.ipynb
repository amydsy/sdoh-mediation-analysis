{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2da36e1",
   "metadata": {},
   "source": [
    "\n",
    "# covariates_builder_9923.ipynb\n",
    "\n",
    "A readable notebook version of the NHANES **1999–2023 Covariates Builder**.\n",
    "\n",
    "**What this builds (to your `Config.out_dir`):**\n",
    "- `cov_smk_1999_2023.parquet`\n",
    "- `cov_alc_1999_2023.parquet`\n",
    "- `cov_pa_1999_2023.parquet`\n",
    "- `cov_bmx_1999_2023.parquet`\n",
    "- `cov_clinical_1999_2023.parquet`\n",
    "- `cov_household_1999_2023.parquet`\n",
    "- `cov_core_1999_2023.parquet`\n",
    "\n",
    "Tip: keep this notebook for reading, and call the functions from your thin `02_build_covariates.ipynb`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c388470",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc5daf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Display options (optional)\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6400001",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d61eb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Base project folder (adjust if needed)\n",
    "BASE = Path(\"/Users/dengshuyue/Desktop/SDOH/analysis\")\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Where inputs/outputs live\n",
    "    raw_dir: Path = BASE / \"data\"\n",
    "    interim_dir: Path = BASE / \"data\" / \"cov\"\n",
    "    out_dir: Path = BASE / \"output\"\n",
    "\n",
    "    # Preferred inputs\n",
    "    demo_9923: Path = BASE / \"data\" / \"cov\" / \"demo9923.parquet\"\n",
    "    demo_9918: Optional[Path] = None\n",
    "\n",
    "    bmx_9923: Optional[Path] = None\n",
    "\n",
    "    smk_9918: Optional[Path] = None\n",
    "    smk_1923: Optional[Path] = None\n",
    "\n",
    "    pa_9918_imputed: Optional[Path] = None\n",
    "    pa_1923: Optional[Path] = None\n",
    "\n",
    "    clinical_9918: Optional[Path] = None\n",
    "    clinical_1923: Optional[Path] = None\n",
    "\n",
    "    # Output file names\n",
    "    cov_smk: str = \"cov_smk_1999_2023.parquet\"\n",
    "    cov_alc: str = \"cov_alc_1999_2023.parquet\"\n",
    "    cov_pa: str = \"cov_pa_1999_2023.parquet\"\n",
    "    cov_bmx: str = \"cov_bmx_1999_2023.parquet\"\n",
    "    cov_clinical: str = \"cov_clinical_1999_2023.parquet\"\n",
    "    cov_household: str = \"cov_household_1999_2023.parquet\"\n",
    "    cov_core: str = \"cov_core_1999_2023.parquet\"\n",
    "\n",
    "CONFIG = Config()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6e990f",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5264c03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NHANES_MISS = {7, 9, 77, 99, 777, 999, 7777, 9999, 77777, 99999}\n",
    "\n",
    "def log(msg: str) -> None:\n",
    "    print(msg, flush=True)\n",
    "\n",
    "def ensure_dir(p: Path) -> None:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def upper_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    d = df.copy()\n",
    "    d.columns = [c.upper() for c in d.columns]\n",
    "    return d\n",
    "\n",
    "def nhanes_na(s: pd.Series) -> pd.Series:\n",
    "    return pd.to_numeric(s, errors=\"coerce\").mask(lambda x: x.isin(NHANES_MISS))\n",
    "\n",
    "def read_any(p: Path) -> pd.DataFrame:\n",
    "    return pd.read_parquet(p) if p.suffix == \".parquet\" else pd.read_csv(p, low_memory=False)\n",
    "\n",
    "def pick_first_existing(*candidates: Optional[Path]) -> Optional[Path]:\n",
    "    for c in candidates:\n",
    "        if c and Path(c).exists():\n",
    "            return Path(c)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa3c2c0",
   "metadata": {},
   "source": [
    "## Smoking (SMQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a93e06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SMK_STATUS_CATS = pd.CategoricalDtype([\"NEVER\", \"FORMER\", \"CURRENT\"], ordered=True)\n",
    "\n",
    "def build_smk(cfg: Config = CONFIG) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build smoking covariates with tolerant inputs.\n",
    "\n",
    "    Accepts either a pre-standardized stack (columns already include:\n",
    "      SEQN, SMK_STATUS, CIGS_PER_DAY, PACK_YEARS, FORMER_SMOKER)\n",
    "    OR derives those columns from common legacy/raw fields:\n",
    "      - SMK (1=NEVER, 2=FORMER, 3=CURRENT)\n",
    "      - SMK_AVG (cigs/day)\n",
    "      - PACK_YR (pack-years)\n",
    "      - SMK_YR (years smoked)\n",
    "      - Optionally SMQ020/SMQ040/SMQ050Q/SMQ050U/SMD030 if present\n",
    "    \"\"\"\n",
    "    ensure_dir(cfg.out_dir)\n",
    "\n",
    "    def _read_any(p: Path) -> pd.DataFrame:\n",
    "        return pd.read_parquet(p) if str(p).endswith(\".parquet\") else pd.read_csv(p, low_memory=False)\n",
    "\n",
    "    # 1) Locate sources (prefer combined 99–23; else 99–18 (+ optional 19–23))\n",
    "    smk_9923 = pick_first_existing(\n",
    "        cfg.interim_dir / \"smk_9923.parquet\",\n",
    "        cfg.interim_dir / \"smk_9923.csv\",\n",
    "    )\n",
    "    if smk_9923:\n",
    "        smk = upper_df(_read_any(smk_9923))\n",
    "        src_msg = f\"using {smk_9923.name}\"\n",
    "    else:\n",
    "        p9918 = pick_first_existing(cfg.smk_9918, cfg.interim_dir / \"smk_9918.parquet\", cfg.interim_dir / \"smk_9918.csv\")\n",
    "        p1923 = pick_first_existing(cfg.smk_1923, cfg.interim_dir / \"smk_1923.parquet\", cfg.interim_dir / \"smk_1923.csv\")\n",
    "        if p9918 is None:\n",
    "            raise FileNotFoundError(\"Provide smk_9923 or smk_9918 (csv/parquet) under interim.\")\n",
    "        smk = upper_df(_read_any(p9918))\n",
    "        if p1923:\n",
    "            smk = pd.concat([smk, upper_df(_read_any(p1923))], ignore_index=True)\n",
    "            src_msg = f\"using {Path(p9918).name} + {Path(p1923).name}\"\n",
    "        else:\n",
    "            src_msg = f\"using {Path(p9918).name}\"\n",
    "\n",
    "    # 2) If standardized columns already exist, just use them\n",
    "    needed = {\"SEQN\", \"SMK_STATUS\", \"CIGS_PER_DAY\", \"PACK_YEARS\", \"FORMER_SMOKER\"}\n",
    "    have_std = needed.issubset(set(smk.columns))\n",
    "\n",
    "    if not have_std:\n",
    "        # 3) Derive standardized columns from legacy/raw fields\n",
    "        d = smk.copy()\n",
    "\n",
    "        # Drop obvious junk index cols if present\n",
    "        for junk in [\"UNNAMED: 0\", \"INDEX\"]:\n",
    "            if junk in d.columns:\n",
    "                d = d.drop(columns=[junk])\n",
    "\n",
    "        out = pd.DataFrame({\"SEQN\": d[\"SEQN\"]})\n",
    "\n",
    "        # --- SMK_STATUS ---\n",
    "        # Priority 1: direct SMK_STATUS if it exists but wasn’t part of 'needed'\n",
    "        if \"SMK_STATUS\" in d.columns:\n",
    "            smk_status = d[\"SMK_STATUS\"].astype(\"string\").str.strip().str.upper()\n",
    "        else:\n",
    "            # Priority 2: numeric SMK (1=NEVER,2=FORMER,3=CURRENT)\n",
    "            smk_num = pd.to_numeric(d.get(\"SMK\"), errors=\"coerce\")\n",
    "            smk_status = smk_num.map({1: \"NEVER\", 2: \"FORMER\", 3: \"CURRENT\"}).astype(\"string\")\n",
    "\n",
    "            # Priority 3: derive from SMQ020/SMQ040 if available\n",
    "            if smk_status.isna().all() and ((\"SMQ020\" in d.columns) or (\"SMQ040\" in d.columns)):\n",
    "                ever = d.get(\"SMQ020\")  # 1=Yes (ever 100 cigs), 2=No\n",
    "                if ever is not None:\n",
    "                    ever = pd.to_numeric(ever, errors=\"coerce\").replace({2: 0, 1: 1})\n",
    "                smq040 = pd.to_numeric(d.get(\"SMQ040\"), errors=\"coerce\")  # 1=Every day,2=Some days,3=Not at all\n",
    "\n",
    "                # default NEVER; flip to FORMER/CURRENT based on ever/smq040\n",
    "                smk_status = pd.Series(\"NEVER\", index=d.index, dtype=\"string\")\n",
    "                if ever is not None:\n",
    "                    smk_status = smk_status.mask(ever == 1, \"FORMER\")\n",
    "                if smq040 is not None:\n",
    "                    smk_status = smk_status.mask(smq040.isin([1, 2]), \"CURRENT\")\n",
    "                    smk_status = smk_status.mask(smq040 == 3, \"FORMER\")\n",
    "\n",
    "        out[\"SMK_STATUS\"] = smk_status.astype(\"string\").str.upper()\n",
    "\n",
    "        # --- CIGS_PER_DAY ---\n",
    "        cigs = pd.to_numeric(d.get(\"SMK_AVG\"), errors=\"coerce\")  # your legacy field\n",
    "        if cigs.isna().all():\n",
    "            # try SMQ050Q/U (quantity + unit 1=day,2=week,3=month)\n",
    "            q = pd.to_numeric(d.get(\"SMQ050Q\"), errors=\"coerce\")\n",
    "            u = pd.to_numeric(d.get(\"SMQ050U\"), errors=\"coerce\")\n",
    "            if q is not None and u is not None:\n",
    "                cigs = (\n",
    "                    q.where(u == 1)\n",
    "                    .fillna((q / 7.0).where(u == 2))\n",
    "                    .fillna((q / 30.0).where(u == 3))\n",
    "                )\n",
    "        out[\"CIGS_PER_DAY\"] = cigs\n",
    "\n",
    "        # --- PACK_YEARS ---\n",
    "        pack_years = pd.to_numeric(d.get(\"PACK_YR\"), errors=\"coerce\")\n",
    "        if pack_years.isna().all():\n",
    "            years = pd.to_numeric(d.get(\"SMK_YR\"), errors=\"coerce\")\n",
    "            if years is None or years.isna().all():\n",
    "                years = pd.to_numeric(d.get(\"SMD030\"), errors=\"coerce\")  # years smoked in some SMQ cycles\n",
    "            pack_years = (out[\"CIGS_PER_DAY\"] / 20.0) * years\n",
    "        out[\"PACK_YEARS\"] = pack_years\n",
    "\n",
    "        # --- FORMER_SMOKER ---\n",
    "        out[\"FORMER_SMOKER\"] = out[\"SMK_STATUS\"].eq(\"FORMER\").fillna(False).astype(\"int8\")\n",
    "\n",
    "        # Clean up obviously invalid values\n",
    "        out.loc[out[\"CIGS_PER_DAY\"] < 0, \"CIGS_PER_DAY\"] = np.nan\n",
    "        out.loc[out[\"PACK_YEARS\"] < 0, \"PACK_YEARS\"] = np.nan\n",
    "\n",
    "        smk_std = out\n",
    "\n",
    "    else:\n",
    "        # Already standardized\n",
    "        smk_std = smk[list(needed)].copy()\n",
    "        smk_std[\"SMK_STATUS\"] = smk_std[\"SMK_STATUS\"].astype(\"string\").str.upper()\n",
    "        smk_std[\"FORMER_SMOKER\"] = pd.to_numeric(smk_std[\"FORMER_SMOKER\"], errors=\"coerce\").fillna(0).astype(\"int8\")\n",
    "\n",
    "    # 4) Final tidy + write\n",
    "    smk_std = smk_std.drop_duplicates(\"SEQN\")\n",
    "    smk_std[\"SMK_STATUS\"] = smk_std[\"SMK_STATUS\"].astype(\"category\").cat.set_categories(SMK_STATUS_CATS.categories, ordered=True)\n",
    "    outp = cfg.out_dir / cfg.cov_smk\n",
    "    smk_std.to_parquet(outp, index=False)\n",
    "\n",
    "    # Light telemetry\n",
    "    miss_rate = smk_std[[\"SMK_STATUS\", \"CIGS_PER_DAY\", \"PACK_YEARS\"]].isna().mean().round(3).to_dict()\n",
    "    log(f\"✓ SMK → {outp} ({src_msg}); missing: {miss_rate}\")\n",
    "\n",
    "    return smk_std\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b076035",
   "metadata": {},
   "source": [
    "## Alcohol (ALQ) 99-23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c98addf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Alcohol builder with optional CDC fetch (revised) ----------\n",
    "import pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# --- tiny local helpers (safe fallbacks if your notebook didn't define them) ---\n",
    "def upper_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df.columns = [c.upper() for c in df.columns]\n",
    "    return df\n",
    "\n",
    "def ensure_dir(p: Path | str):\n",
    "    Path(p).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def pick_first_existing(*paths):\n",
    "    for p in paths:\n",
    "        if p and Path(p).exists():\n",
    "            return Path(p)\n",
    "    return None\n",
    "\n",
    "def log(msg: str):  # nice-to-have\n",
    "    print(msg)\n",
    "\n",
    "# --- IO helpers ---\n",
    "def _read_any(p: Path) -> pd.DataFrame:\n",
    "    p = Path(p)\n",
    "    if p.suffix.lower() == \".parquet\":\n",
    "        return pd.read_parquet(p)\n",
    "    return pd.read_csv(p, low_memory=False)\n",
    "\n",
    "def _clean_num(s: pd.Series) -> pd.Series:\n",
    "    NH_MISS = {7, 9, 77, 99, 777, 999, 7777, 9999, 77777, 99999}\n",
    "    s = pd.to_numeric(s, errors=\"coerce\")\n",
    "    return s.mask(s.isin(NH_MISS))\n",
    "\n",
    "def _download(url: str, dest: Path, timeout=90):\n",
    "    import requests\n",
    "    dest.parent.mkdir(parents=True, exist_ok=True)\n",
    "    headers = {\"User-Agent\": \"nhanes-fetch/1.0\"}\n",
    "    with requests.get(url, headers=headers, stream=True, timeout=timeout) as r:\n",
    "        r.raise_for_status()\n",
    "        tmp = dest.with_suffix(dest.suffix + \".downloading\")\n",
    "        with open(tmp, \"wb\") as f:\n",
    "            for chunk in r.iter_content(1 << 15):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "        tmp.rename(dest)\n",
    "    return dest\n",
    "\n",
    "def _read_xpt(p: Path) -> pd.DataFrame:\n",
    "    try:\n",
    "        import pyreadstat\n",
    "        df, _ = pyreadstat.read_xport(p)\n",
    "    except Exception:\n",
    "        df = pd.read_sas(p, format=\"xport\")\n",
    "    df.columns = [c.upper() for c in df.columns]\n",
    "    return df\n",
    "\n",
    "# --- minimal DEMO sex loader (RIAGENDR) ---\n",
    "def _ensure_demo_sex(cfg) -> pd.DataFrame:\n",
    "    \"\"\"Return DEMO with SEQN, RIAGENDR. Try configured paths; else build minimal by fetching.\"\"\"\n",
    "    # 1) Try configured demo\n",
    "    demo_path = None\n",
    "    for cand in [\n",
    "        getattr(cfg, \"demo_9923\", None),\n",
    "        getattr(cfg, \"demo_9918\", None),\n",
    "        cfg.interim_dir / \"demo_9923.parquet\",\n",
    "        cfg.interim_dir / \"demo_9918.parquet\",\n",
    "    ]:\n",
    "        if cand and Path(cand).exists():\n",
    "            demo_path = Path(cand)\n",
    "            break\n",
    "    if demo_path:\n",
    "        demo = _read_any(demo_path)\n",
    "        demo = upper_df(demo)\n",
    "        if \"RIAGENDR\" in demo.columns:\n",
    "            return demo[[\"SEQN\", \"RIAGENDR\"]].drop_duplicates(\"SEQN\")\n",
    "\n",
    "    # 2) Build minimal by fetching RIAGENDR across cycles\n",
    "    DEMO_URLS = {\n",
    "        \"1999-2000\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/1999/DataFiles/DEMO.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/1999-2000/DEMO.XPT\",\n",
    "        ],\n",
    "        \"2001-2002\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2001/DataFiles/DEMO_B.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2001-2002/DEMO_B.XPT\",\n",
    "        ],\n",
    "        \"2003-2004\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2003/DataFiles/DEMO_C.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2003-2004/DEMO_C.XPT\",\n",
    "        ],\n",
    "        \"2005-2006\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2005/DataFiles/DEMO_D.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2005-2006/DEMO_D.XPT\",\n",
    "        ],\n",
    "        \"2007-2008\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2007/DataFiles/DEMO_E.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2007-2008/DEMO_E.XPT\",\n",
    "        ],\n",
    "        \"2009-2010\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2009/DataFiles/DEMO_F.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2009-2010/DEMO_F.XPT\",\n",
    "        ],\n",
    "        \"2011-2012\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2011/DataFiles/DEMO_G.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2011-2012/DEMO_G.XPT\",\n",
    "        ],\n",
    "        \"2013-2014\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2013/DataFiles/DEMO_H.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2013-2014/DEMO_H.XPT\",\n",
    "        ],\n",
    "        \"2015-2016\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2015/DataFiles/DEMO_I.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2015-2016/DEMO_I.XPT\",\n",
    "        ],\n",
    "        \"2017-2018\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2017/DataFiles/DEMO_J.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2017-2018/DEMO_J.XPT\",\n",
    "        ],\n",
    "        \"2017-March 2020 (pre-pandemic)\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2017/DataFiles/P_DEMO.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2017-2020/P_DEMO.XPT\",\n",
    "        ],\n",
    "        \"August 2021–August 2023\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2021/DataFiles/DEMO_L.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2021-2023/DEMO_L.XPT\",\n",
    "        ],\n",
    "    }\n",
    "    store = Path(cfg.interim_dir)\n",
    "    parts = []\n",
    "    for cyc, urls in DEMO_URLS.items():\n",
    "        got = None\n",
    "        for u in urls:\n",
    "            try:\n",
    "                dst = store / Path(u).name\n",
    "                if not dst.exists():\n",
    "                    print(f\"⬇️ DEMO {cyc} → {dst.name}\")\n",
    "                    _download(u, dst)\n",
    "                got = dst\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(\"  ⚠️\", e)\n",
    "        if got is None:\n",
    "            continue\n",
    "        df = _read_xpt(got)\n",
    "        if {\"SEQN\", \"RIAGENDR\"}.issubset(df.columns):\n",
    "            parts.append(df[[\"SEQN\", \"RIAGENDR\"]])\n",
    "    if not parts:\n",
    "        raise RuntimeError(\"Could not build minimal DEMO (RIAGENDR).\")\n",
    "    demo = pd.concat(parts, ignore_index=True).drop_duplicates(\"SEQN\")\n",
    "    out_demo = store / \"demo_riagendr_min.parquet\"\n",
    "    demo.to_parquet(out_demo, index=False)\n",
    "    return demo\n",
    "\n",
    "# --- ALQ downloader/stacker ---\n",
    "def _download_and_stack_alq(cfg) -> pd.DataFrame:\n",
    "    \"\"\"Download ALQ XPTs to interim/alcohol/, stack minimal columns used for dpd.\"\"\"\n",
    "    ALC_STORE = Path(cfg.interim_dir) / \"alcohol\"\n",
    "    ALC_STORE.mkdir(parents=True, exist_ok=True)\n",
    "    ALQ_URLS = {\n",
    "        \"1999-2000\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/1999/DataFiles/ALQ.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/1999-2000/ALQ.XPT\",\n",
    "        ],\n",
    "        \"2001-2002\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2001/DataFiles/ALQ_B.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2001-2002/ALQ_B.XPT\",\n",
    "        ],\n",
    "        \"2003-2004\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2003/DataFiles/ALQ_C.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2003-2004/ALQ_C.XPT\",\n",
    "        ],\n",
    "        \"2005-2006\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2005/DataFiles/ALQ_D.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2005-2006/ALQ_D.XPT\",\n",
    "        ],\n",
    "        \"2007-2008\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2007/DataFiles/ALQ_E.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2007-2008/ALQ_E.XPT\",\n",
    "        ],\n",
    "        \"2009-2010\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2009/DataFiles/ALQ_F.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2009-2010/ALQ_F.XPT\",\n",
    "        ],\n",
    "        \"2011-2012\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2011/DataFiles/ALQ_G.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2011-2012/ALQ_G.XPT\",\n",
    "        ],\n",
    "        \"2013-2014\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2013/DataFiles/ALQ_H.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2013-2014/ALQ_H.XPT\",\n",
    "        ],\n",
    "        \"2015-2016\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2015/DataFiles/ALQ_I.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2015-2016/ALQ_I.XPT\",\n",
    "        ],\n",
    "        \"2017-2018\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2017/DataFiles/ALQ_J.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2017-2018/ALQ_J.XPT\",\n",
    "        ],\n",
    "        \"2017-March 2020 (pre-pandemic)\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2017/DataFiles/P_ALQ.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2017-2020/P_ALQ.XPT\",\n",
    "        ],\n",
    "        \"August 2021–August 2023\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2021/DataFiles/ALQ_L.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2021-2023/ALQ_L.XPT\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2021/DataFiles/ALQ_Q.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2021-2023/ALQ_Q.XPT\",\n",
    "        ],\n",
    "    }\n",
    "    parts = []\n",
    "    for cycle, urls in ALQ_URLS.items():\n",
    "        got = None\n",
    "        for u in urls:\n",
    "            try:\n",
    "                dst = ALC_STORE / Path(u).name\n",
    "                if not dst.exists():\n",
    "                    print(f\"⬇️ ALQ {cycle} → {dst.name}\")\n",
    "                    _download(u, dst)\n",
    "                got = dst\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(\"  ⚠️\", e)\n",
    "        if got is None:\n",
    "            continue\n",
    "        df = _read_xpt(got)\n",
    "        df[\"CYCLE\"] = cycle\n",
    "        keep = [c for c in [\"SEQN\", \"CYCLE\", \"ALQ110\", \"ALQ151\", \"ALQ120Q\", \"ALQ120U\", \"ALQ130\"] if c in df.columns]\n",
    "        parts.append(df[keep])\n",
    "    if not parts:\n",
    "        raise RuntimeError(\"No ALQ data available (download failed).\")\n",
    "    alq = pd.concat(parts, ignore_index=True)\n",
    "    # Persist stacked ALQ for reuse\n",
    "    stacked = Path(cfg.interim_dir) / \"alq_9923.parquet\"\n",
    "    stacked.parent.mkdir(parents=True, exist_ok=True)\n",
    "    alq.to_parquet(stacked, index=False)\n",
    "    return alq\n",
    "\n",
    "# --- derive drinks/day ---\n",
    "def _drinks_per_day_from_alq(alq: pd.DataFrame) -> pd.Series:\n",
    "    d = upper_df(alq)\n",
    "    count = _clean_num(d.get(\"ALQ120Q\", pd.Series(np.nan, index=d.index)))\n",
    "    unit  = d.get(\"ALQ120U\", pd.Series(np.nan, index=d.index))\n",
    "    per_year = pd.Series(np.nan, index=d.index, dtype=\"float\")\n",
    "    per_year = per_year.where(~(unit == 1), 365.0)\n",
    "    per_year = per_year.where(~(unit == 2), 52.142)\n",
    "    per_year = per_year.where(~(unit == 3), 12.0)\n",
    "    per_year = per_year.where(~(unit == 4), 1.0)\n",
    "    occasions_per_year = count * per_year\n",
    "\n",
    "    drinks_per_occasion = _clean_num(d.get(\"ALQ130\", pd.Series(np.nan, index=d.index)))\n",
    "    dpd = (occasions_per_year * drinks_per_occasion) / 365.0\n",
    "    return dpd.where(occasions_per_year.notna() & drinks_per_occasion.notna())\n",
    "\n",
    "# --- categorize with explicit index alignment (fixes FutureWarning) ---\n",
    "def _categorize_alcohol(dpd: pd.Series, sex: pd.Series, lifetime_lt12: pd.Series | None) -> pd.Series:\n",
    "    # Normalize inputs and drop index labels to ensure 1:1 positional alignment\n",
    "    dpd  = pd.to_numeric(dpd, errors=\"coerce\").reset_index(drop=True)\n",
    "    sexM = pd.to_numeric(sex, errors=\"coerce\").map({1: \"M\", 2: \"F\"}).astype(\"string\").reset_index(drop=True)\n",
    "    if lifetime_lt12 is None:\n",
    "        life = pd.Series(pd.NA, index=dpd.index)\n",
    "    else:\n",
    "        life = pd.to_numeric(lifetime_lt12, errors=\"coerce\").reset_index(drop=True)\n",
    "\n",
    "    # Masks (now same RangeIndex → no alignment warning)\n",
    "    none_mask     = dpd.isna() | (dpd < 0.03) | (life == 1)               # <12 lifetime OR <0.03/day\n",
    "    heavy_mask    = ((sexM == \"F\") & (dpd >= 1.0)) | ((sexM == \"M\") & (dpd >= 2.0))\n",
    "    moderate_mask = (~none_mask) & (~heavy_mask)\n",
    "\n",
    "    cat = pd.Series(\"NONE\", index=dpd.index, dtype=\"string\")\n",
    "    cat.loc[moderate_mask] = \"MODERATE\"\n",
    "    cat.loc[heavy_mask]    = \"HEAVY\"\n",
    "    return pd.Categorical(cat, categories=[\"NONE\", \"MODERATE\", \"HEAVY\"], ordered=True)\n",
    "\n",
    "# --- public builder ---\n",
    "def build_alc(cfg: \"Config\" = CONFIG, allow_fetch: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build alcohol covariates.\n",
    "      • Prefer existing output in cfg.out_dir\n",
    "      • Else prefer stacked ALQ in cfg.interim_dir\n",
    "      • Else, if allow_fetch: download ALQ XPTs and stack\n",
    "    Writes {cfg.out_dir}/{cfg.cov_alc}\n",
    "    \"\"\"\n",
    "    ensure_dir(cfg.out_dir)\n",
    "\n",
    "    # A) If already built, return it\n",
    "    out_path = Path(cfg.out_dir) / cfg.cov_alc\n",
    "    if out_path.exists():\n",
    "        return pd.read_parquet(out_path)\n",
    "\n",
    "    # B) Load ALQ stack (prefer 99–23, else 99–18). If missing and allowed, fetch.\n",
    "    alq_path = pick_first_existing(\n",
    "        Path(cfg.interim_dir) / \"alq_9923.parquet\",\n",
    "        Path(cfg.interim_dir) / \"alq_9918.parquet\",\n",
    "    )\n",
    "    if alq_path:\n",
    "        alq = _read_any(alq_path)\n",
    "    else:\n",
    "        if not allow_fetch:\n",
    "            raise FileNotFoundError(\n",
    "                \"ALQ stack not found (alq_9923 / alq_9918). Set allow_fetch=True to download from CDC.\"\n",
    "            )\n",
    "        alq = _download_and_stack_alq(cfg)\n",
    "\n",
    "    alq = upper_df(alq)\n",
    "    dpd = _drinks_per_day_from_alq(alq)\n",
    "\n",
    "    # lifetime <12 drinks indicator\n",
    "    life = None\n",
    "    if \"ALQ110\" in alq.columns:\n",
    "        life = (pd.to_numeric(alq[\"ALQ110\"], errors=\"coerce\") == 2).astype(\"Int8\")\n",
    "    elif \"ALQ151\" in alq.columns:\n",
    "        life = (pd.to_numeric(alq[\"ALQ151\"], errors=\"coerce\") == 2).astype(\"Int8\")\n",
    "\n",
    "    # Bring in sex (1=male, 2=female) aligned to ALQ rows\n",
    "    demo = _ensure_demo_sex(cfg)\n",
    "    demo = upper_df(demo).drop_duplicates(\"SEQN\")\n",
    "    sex = demo.set_index(\"SEQN\").reindex(alq[\"SEQN\"])[\"RIAGENDR\"]\n",
    "\n",
    "    # Categorize with aligned masks\n",
    "    alc_cat = _categorize_alcohol(dpd, sex=sex, lifetime_lt12=life)\n",
    "\n",
    "    out = pd.DataFrame(\n",
    "        {\n",
    "            \"SEQN\": alq[\"SEQN\"].reset_index(drop=True),\n",
    "            \"DRINKS_PER_DAY\": pd.to_numeric(dpd, errors=\"coerce\").reset_index(drop=True),\n",
    "            \"ALCOHOL_CAT\": alc_cat,\n",
    "        }\n",
    "    )\n",
    "    out.to_parquet(out_path, index=False)\n",
    "    log(f\"✓ ALC → {out_path}\")\n",
    "    return out\n",
    "# ---------- end alcohol builder (revised) ----------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45bcd18",
   "metadata": {},
   "source": [
    "## Physical Activity (PA) 99-23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c22e66f-fed3-4a1b-904d-09b3299e9ac1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151837b8-e2ed-4ee2-8594-ed146ec6956a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218b1e79-9503-461c-9567-bc2e76838724",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9376980-913a-436f-bf87-406c98c5e7bc",
   "metadata": {},
   "source": [
    "#### check one cycle PA first "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "677f7ca0-a87a-4eeb-8744-dbfade41b90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cycle: 2011-2012 | shape: (9107, 21)\n",
      "\n",
      "First 20 columns:\n",
      " ['SEQN', 'PAQ706', 'PAQ605', 'PAQ610', 'PAD615', 'PAQ620', 'PAQ625', 'PAD630', 'PAQ635', 'PAQ640', 'PAD645', 'PAQ650', 'PAQ655', 'PAD660', 'PAQ665', 'PAQ670', 'PAD675', 'PAD680', 'PAQ710', 'PAQ715']\n",
      "\n",
      "Present candidates: ['SEQN', 'PAQ605', 'PAQ610', 'PAQ620', 'PAQ655', 'PAQ665', 'PAD615', 'PAD630', 'PAD660', 'PAD675']\n",
      "\n",
      "Non-null counts:\n",
      " PAD615     943\n",
      "PAD630    2059\n",
      "PAD660    1906\n",
      "PAD675    2887\n",
      "PAQ605    6779\n",
      "PAQ610     947\n",
      "PAQ620    6779\n",
      "PAQ655    1909\n",
      "PAQ665    6779\n",
      "SEQN      9107\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEQN</th>\n",
       "      <th>PAQ605</th>\n",
       "      <th>PAQ610</th>\n",
       "      <th>PAQ620</th>\n",
       "      <th>PAQ655</th>\n",
       "      <th>PAQ665</th>\n",
       "      <th>PAD615</th>\n",
       "      <th>PAD630</th>\n",
       "      <th>PAD660</th>\n",
       "      <th>PAD675</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>62161.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62162.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62163.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62164.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60.0</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>62165.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>62166.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>62168.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>62169.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>62170.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>62171.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      SEQN  PAQ605  PAQ610  PAQ620  PAQ655  PAQ665  PAD615  PAD630  PAD660  PAD675\n",
       "0  62161.0     2.0     NaN     2.0     NaN     2.0     NaN     NaN     NaN     NaN\n",
       "1  62162.0     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN\n",
       "2  62163.0     2.0     NaN     2.0     NaN     2.0     NaN     NaN     NaN     NaN\n",
       "3  62164.0     1.0     5.0     2.0     5.0     1.0    60.0     NaN    60.0    45.0\n",
       "4  62165.0     2.0     NaN     2.0     2.0     1.0     NaN     NaN    25.0    25.0\n",
       "5  62166.0     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN\n",
       "6  62168.0     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN\n",
       "7  62169.0     2.0     NaN     2.0     NaN     2.0     NaN     NaN     NaN     NaN\n",
       "8  62170.0     2.0     NaN     1.0     5.0     2.0     NaN    40.0    90.0     NaN\n",
       "9  62171.0     2.0     NaN     2.0     5.0     1.0     NaN     NaN    20.0    30.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Choose a cycle to inspect\n",
    "cycle = \"2011-2012\"\n",
    "urls = [\n",
    "    # PAQ_G\n",
    "    \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2011/DataFiles/PAQ_G.xpt\",\n",
    "    \"https://wwwn.cdc.gov/Nchs/Nhanes/2011-2012/PAQ_G.XPT\",\n",
    "]\n",
    "\n",
    "interim = Path(\"/Users/dengshuyue/Desktop/SDOH/analysis/interim/pa\")\n",
    "interim.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _download(url, dest):\n",
    "    import requests\n",
    "    if dest.exists(): \n",
    "        return dest\n",
    "    with requests.get(url, headers={\"User-Agent\":\"nhanes-pa/1.0\"}, stream=True, timeout=90) as r:\n",
    "        r.raise_for_status()\n",
    "        tmp = dest.with_suffix(dest.suffix + \".downloading\")\n",
    "        with open(tmp, \"wb\") as f:\n",
    "            for chunk in r.iter_content(1<<15):\n",
    "                if chunk: f.write(chunk)\n",
    "        tmp.rename(dest)\n",
    "    return dest\n",
    "\n",
    "# Try the two official links\n",
    "xpt_path = None\n",
    "for u in urls:\n",
    "    try:\n",
    "        cand = interim / Path(u).name\n",
    "        xpt_path = _download(u, cand)\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(\"⚠️\", e)\n",
    "\n",
    "if xpt_path is None:\n",
    "    raise RuntimeError(\"Could not fetch PAQ for the chosen cycle.\")\n",
    "\n",
    "# Read XPT (either pyreadstat or pandas works)\n",
    "try:\n",
    "    import pyreadstat\n",
    "    df, _ = pyreadstat.read_xport(xpt_path)\n",
    "except Exception:\n",
    "    df = pd.read_sas(xpt_path, format=\"xport\")\n",
    "\n",
    "df.columns = [c.upper() for c in df.columns]\n",
    "print(\"Cycle:\", cycle, \"| shape:\", df.shape)\n",
    "print(\"\\nFirst 20 columns:\\n\", list(df.columns[:20]))\n",
    "\n",
    "# Show whether expected candidates are present (days + minutes)\n",
    "cands = [\"SEQN\",\"PAQ605\",\"PAQ610\",\"PAQ620\",\"PAQ655\",\"PAQ665\",\"PAD615\",\"PAD630\",\"PAD660\",\"PAD675\"]\n",
    "present = [c for c in cands if c in df.columns]\n",
    "print(\"\\nPresent candidates:\", present)\n",
    "\n",
    "# Peek at non-null counts for whatever is present\n",
    "nn = df[present].notna().sum().sort_index()\n",
    "print(\"\\nNon-null counts:\\n\", nn)\n",
    "\n",
    "# Quick look\n",
    "display(df[present].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5f8b62-d437-499d-8578-159f8d096969",
   "metadata": {},
   "source": [
    "#### Fetch new cicle 17-20 21-23 and append to 99-18 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1f8d1f56-d596-4922-8ef0-6dbd1358ddb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "… downloading PAQ/DEMO for 2019…\n",
      "   ↪ PAQ_K not available, trying combined P_PAQ (2017)…\n",
      "   ↪ DEMO_K not available, trying combined P_DEMO (2017)…\n",
      "… downloading PAQ/DEMO for 2021…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nx/82zd518d5gb2gsf5fhydbkj00000gn/T/ipykernel_77481/165523215.py:179: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  new_appended = pd.concat([old] + new_parts, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Wrote /Users/dengshuyue/Desktop/SDOH/analysis/data/cov/totalpa_9923_imputed.csv  (shape=(72927, 38))\n"
     ]
    }
   ],
   "source": [
    "# make_totalpa_9923_from_9918.py\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import pyreadstat\n",
    "\n",
    "# ------------ CONFIG (your paths) ------------\n",
    "OLD_FILE = \"/Users/dengshuyue/Desktop/SDOH/analysis/data/cov/totalpa_9918_imputed.csv\"   # existing 99–18 file\n",
    "OUT_FILE = \"/Users/dengshuyue/Desktop/SDOH/analysis/data/cov/totalpa_9923_imputed.csv\"    # new 99–23 output\n",
    "TMP_DIR  = \"/Users/dengshuyue/Desktop/SDOH/analysis/data/cov/_tmp_nhanes_1923\"\n",
    "VERIFY_SSL = True  # set False if your network does SSL inspection\n",
    "\n",
    "# CDC Public URLs (exact stems) — 2019–2020 (K) and 2021–2022 (L)\n",
    "PAQ_URLS = {\n",
    "    2019: \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2019/DataFiles/PAQ_K.xpt\",\n",
    "    2021: \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2021/DataFiles/PAQ_L.xpt\",\n",
    "}\n",
    "DEMO_URLS = {\n",
    "    2019: \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2019/DataFiles/DEMO_K.xpt\",\n",
    "    2021: \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2021/DataFiles/DEMO_L.xpt\",\n",
    "}\n",
    "# Fallback (rare): combined 2017 files\n",
    "PAQ_FALLBACK_2017 = \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2017/DataFiles/P_PAQ.xpt\"\n",
    "DEMO_FALLBACK_2017 = \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2017/DataFiles/P_DEMO.xpt\"\n",
    "\n",
    "os.makedirs(TMP_DIR, exist_ok=True)\n",
    "\n",
    "# ------------ IO helpers ------------\n",
    "def _dl(url: str, dst: str):\n",
    "    r = requests.get(url, timeout=60, verify=VERIFY_SSL)\n",
    "    if not r.ok or r.content is None or len(r.content) < 1024:\n",
    "        raise RuntimeError(f\"Download failed: {url} (status {r.status_code})\")\n",
    "    with open(dst, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "    return dst\n",
    "\n",
    "def _read_xpt(path: str) -> pd.DataFrame:\n",
    "    # tolerant to older pyreadstat and encoding quirks\n",
    "    try:\n",
    "        df, _ = pyreadstat.read_xport(path, apply_value_formats=False, encoding=\"utf-8\")\n",
    "        return df\n",
    "    except TypeError:\n",
    "        try:\n",
    "            df, _ = pyreadstat.read_xport(path)\n",
    "            return df\n",
    "        except UnicodeDecodeError:\n",
    "            pass\n",
    "    except UnicodeDecodeError:\n",
    "        pass\n",
    "    for enc in (\"latin-1\", \"cp1252\"):\n",
    "        try:\n",
    "            df, _ = pyreadstat.read_xport(path, apply_value_formats=False, encoding=enc)\n",
    "            return df\n",
    "        except Exception:\n",
    "            continue\n",
    "    # last resort\n",
    "    return pd.read_sas(path, format=\"xport\", encoding=\"latin-1\")\n",
    "\n",
    "def _upper_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = [c.upper() for c in df.columns]\n",
    "    return df\n",
    "\n",
    "# ------------ LTPA derivation (2007+ schema: PAD615/630/660/675) ------------\n",
    "def derive_ltpa_from_paq(paq: pd.DataFrame) -> pd.DataFrame:\n",
    "    paq = _upper_df(paq)\n",
    "    out = paq[[\"SEQN\"]].copy()\n",
    "\n",
    "    def g(c): return pd.to_numeric(paq[c], errors=\"coerce\") if c in paq.columns else pd.Series(np.nan, index=paq.index)\n",
    "    def clean_days(s): return s.mask(s.isin([77, 99]))\n",
    "    def clean_min(s):  return s.mask(s.isin([7777, 9999]))\n",
    "\n",
    "    vig_days = clean_days(g(\"PAD615\"))\n",
    "    vig_min  = clean_min(g(\"PAD630\"))\n",
    "    mod_days = clean_days(g(\"PAD660\"))\n",
    "    mod_min  = clean_min(g(\"PAD675\"))\n",
    "    any_vig  = g(\"PAQ655\")  # 1 yes / 2 no\n",
    "    any_mod  = g(\"PAQ670\")  # 1 yes / 2 no\n",
    "\n",
    "    vig_met_min = 8.0 * vig_days.fillna(0) * vig_min.fillna(0)\n",
    "    mod_met_min = 4.0 * mod_days.fillna(0) * mod_min.fillna(0)\n",
    "    total_met_min = vig_met_min + mod_met_min\n",
    "\n",
    "    has_components = ((vig_days.notna() & vig_min.notna()) | (mod_days.notna() & mod_min.notna()))\n",
    "    both_no = (any_vig == 2) & (any_mod == 2)\n",
    "\n",
    "    total_met_min = total_met_min.where(has_components, np.nan)\n",
    "    total_met_min = total_met_min.where(~both_no, 0)\n",
    "\n",
    "    out[\"LTPA\"] = (total_met_min / 60.0).clip(lower=0)       # MET-h/week\n",
    "    out[\"METSCORE\"] = total_met_min.clip(lower=0).round(0)   # MET-min/week\n",
    "\n",
    "    imp = pd.Series(0, index=paq.index, dtype=\"int8\")\n",
    "    imp[(~has_components) & both_no] = 1\n",
    "    imp[(~has_components) & (~both_no)] = 1\n",
    "    out[\"IMP\"] = imp\n",
    "    return out\n",
    "\n",
    "def load_demo_keep(demo: pd.DataFrame) -> pd.DataFrame:\n",
    "    demo = _upper_df(demo)\n",
    "    keep = [c for c in [\"SEQN\", \"SDDSRVYR\", \"SDMVPSU\", \"SDMVSTRA\"] if c in demo.columns]\n",
    "    return demo[keep].copy()\n",
    "\n",
    "# ------------ Build only 2019–2023 and append to 9918 ------------\n",
    "def make_9923_from_9918(old_file=OLD_FILE, out_file=OUT_FILE):\n",
    "    if not os.path.exists(old_file):\n",
    "        raise FileNotFoundError(f\"Missing {old_file}. Provide your totalpa_9918_imputed.csv first.\")\n",
    "\n",
    "    old = pd.read_csv(old_file)\n",
    "    old_cols = list(old.columns)\n",
    "\n",
    "    # detect SDDSRVYR column name used in old file (could be 'sddsrvyr')\n",
    "    sdd_oldname = next((c for c in old_cols if c.lower() == \"sddsrvyr\"), None)\n",
    "\n",
    "    new_parts = []\n",
    "    for year in (2019, 2021):\n",
    "        print(f\"… downloading PAQ/DEMO for {year}…\")\n",
    "        # PAQ\n",
    "        paq_url = PAQ_URLS[year]\n",
    "        paq_path = os.path.join(TMP_DIR, os.path.basename(paq_url))\n",
    "        try:\n",
    "            _dl(paq_url, paq_path)\n",
    "        except RuntimeError:\n",
    "            if year == 2019:\n",
    "                print(\"   ↪ PAQ_K not available, trying combined P_PAQ (2017)…\")\n",
    "                paq_fallback = PAQ_FALLBACK_2017\n",
    "                paq_path = os.path.join(TMP_DIR, os.path.basename(paq_fallback))\n",
    "                _dl(paq_fallback, paq_path)\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "        # DEMO\n",
    "        demo_url = DEMO_URLS[year]\n",
    "        demo_path = os.path.join(TMP_DIR, os.path.basename(demo_url))\n",
    "        try:\n",
    "            _dl(demo_url, demo_path)\n",
    "        except RuntimeError:\n",
    "            if year == 2019:\n",
    "                print(\"   ↪ DEMO_K not available, trying combined P_DEMO (2017)…\")\n",
    "                demo_fallback = DEMO_FALLBACK_2017\n",
    "                demo_path = os.path.join(TMP_DIR, os.path.basename(demo_fallback))\n",
    "                _dl(demo_fallback, demo_path)\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "        paq = _read_xpt(paq_path)\n",
    "        demo = _read_xpt(demo_path)\n",
    "\n",
    "        derived = derive_ltpa_from_paq(paq)\n",
    "        dem_keep = load_demo_keep(demo)\n",
    "        merged = derived.merge(dem_keep, on=\"SEQN\", how=\"left\")\n",
    "\n",
    "        # Build a frame exactly like the old schema\n",
    "        part = pd.DataFrame(columns=old_cols).reindex(range(len(merged)))\n",
    "\n",
    "        # map ltpa/metscore/imp into whatever the old file used (case-insensitive)\n",
    "        ltpa_name     = next((c for c in old_cols if c.lower() == \"ltpa\"), None)\n",
    "        metscore_name = next((c for c in old_cols if c.lower() == \"metscore\"), None)\n",
    "        imp_name      = next((c for c in old_cols if c.lower() in (\"imp\", \"imputed\", \"ltpa_imputed_flag\", \"pa_imputed_flag\", \"imputed_flag\")), None)\n",
    "\n",
    "        if \"SEQN\" in part.columns: part[\"SEQN\"] = merged[\"SEQN\"].values\n",
    "        if ltpa_name:              part[ltpa_name] = merged[\"LTPA\"].values\n",
    "        if metscore_name:          part[metscore_name] = merged[\"METSCORE\"].values\n",
    "        if imp_name:               part[imp_name] = merged[\"IMP\"].astype(\"int8\").values\n",
    "\n",
    "        for col in [\"SDMVPSU\", \"SDMVSTRA\"]:\n",
    "            if col in merged.columns and col in part.columns:\n",
    "                part[col] = merged[col].values\n",
    "\n",
    "        if sdd_oldname and \"SDDSRVYR\" in merged.columns and sdd_oldname in part.columns:\n",
    "            part[sdd_oldname] = merged[\"SDDSRVYR\"].values\n",
    "\n",
    "        new_parts.append(part)\n",
    "\n",
    "    if not new_parts:\n",
    "        raise RuntimeError(\"No new cycles were built (2019 & 2021).\")\n",
    "\n",
    "    new_appended = pd.concat([old] + new_parts, ignore_index=True)\n",
    "\n",
    "    # Deduplicate by SEQN (keep first occurrence)\n",
    "    seqn_col = next(c for c in new_appended.columns if c.lower() == \"seqn\")\n",
    "    new_appended = new_appended.drop_duplicates(subset=[seqn_col], keep=\"first\")\n",
    "\n",
    "    new_appended.to_csv(out_file, index=False)\n",
    "    print(f\"✓ Wrote {out_file}  (shape={new_appended.shape})\")\n",
    "    return new_appended\n",
    "\n",
    "# --- run ---\n",
    "if __name__ == \"__main__\":\n",
    "    make_9923_from_9918()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5984a6-6c3b-420f-b3d9-935461c8cd3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ffa13a89-7547-4617-8929-5ff911fb7a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "… downloading PAQ/DEMO for 2019…\n",
      "   ↪ PAQ_K not available, trying combined P_PAQ (2017)…\n",
      "   ↪ DEMO_K not available, trying combined P_DEMO (2017)…\n",
      "… downloading PAQ/DEMO for 2021…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nx/82zd518d5gb2gsf5fhydbkj00000gn/T/ipykernel_77481/1244696013.py:226: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  new_appended = pd.concat([old] + new_parts, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Wrote /Users/dengshuyue/Desktop/SDOH/analysis/data/cov/totalpa_9923_imputed.csv  (shape=(72927, 38))\n"
     ]
    }
   ],
   "source": [
    "# make_totalpa_9923_from_9918.py\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import pyreadstat\n",
    "\n",
    "# ------------ CONFIG (your paths) ------------\n",
    "OLD_FILE = \"/Users/dengshuyue/Desktop/SDOH/analysis/data/cov/totalpa_9918_imputed.csv\"   # existing 99–18 file\n",
    "OUT_FILE = \"/Users/dengshuyue/Desktop/SDOH/analysis/data/cov/totalpa_9923_imputed.csv\"    # new 99–23 output\n",
    "TMP_DIR  = \"/Users/dengshuyue/Desktop/SDOH/analysis/data/cov/_tmp_nhanes_1923\"\n",
    "VERIFY_SSL = True  # set False if your network does SSL inspection\n",
    "\n",
    "# CDC Public URLs — 2019–2020 (K) and 2021–2022 (L)\n",
    "PAQ_URLS = {\n",
    "    2019: \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2019/DataFiles/PAQ_K.xpt\",\n",
    "    2021: \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2021/DataFiles/PAQ_L.xpt\",\n",
    "}\n",
    "DEMO_URLS = {\n",
    "    2019: \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2019/DataFiles/DEMO_K.xpt\",\n",
    "    2021: \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2021/DataFiles/DEMO_L.xpt\",\n",
    "}\n",
    "# Fallback (rare): combined 2017–Mar 2020 files\n",
    "PAQ_FALLBACK_2017 = \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2017/DataFiles/P_PAQ.xpt\"\n",
    "DEMO_FALLBACK_2017 = \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2017/DataFiles/P_DEMO.xpt\"\n",
    "\n",
    "os.makedirs(TMP_DIR, exist_ok=True)\n",
    "\n",
    "# ------------ IO helpers ------------\n",
    "def _dl(url: str, dst: str):\n",
    "    r = requests.get(url, timeout=60, verify=VERIFY_SSL)\n",
    "    if not r.ok or r.content is None or len(r.content) < 1024:\n",
    "        raise RuntimeError(f\"Download failed: {url} (status {r.status_code})\")\n",
    "    with open(dst, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "    return dst\n",
    "\n",
    "def _read_xpt(path: str) -> pd.DataFrame:\n",
    "    # tolerant to older pyreadstat and encoding quirks\n",
    "    try:\n",
    "        df, _ = pyreadstat.read_xport(path, apply_value_formats=False, encoding=\"utf-8\")\n",
    "        return df\n",
    "    except TypeError:\n",
    "        try:\n",
    "            df, _ = pyreadstat.read_xport(path)\n",
    "            return df\n",
    "        except UnicodeDecodeError:\n",
    "            pass\n",
    "    except UnicodeDecodeError:\n",
    "        pass\n",
    "    for enc in (\"latin-1\", \"cp1252\"):\n",
    "        try:\n",
    "            df, _ = pyreadstat.read_xport(path, apply_value_formats=False, encoding=enc)\n",
    "            return df\n",
    "        except Exception:\n",
    "            continue\n",
    "    # last resort\n",
    "    return pd.read_sas(path, format=\"xport\", encoding=\"latin-1\")\n",
    "\n",
    "def _upper_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = [c.upper() for c in df.columns]\n",
    "    return df\n",
    "\n",
    "# ------------ LTPA derivation (supports 2007–2018/P and 2021–2023) ------------\n",
    "def derive_ltpa_from_paq(paq: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Output:\n",
    "      LTPA (MET-h/week), METSCORE (MET-min/week), IMP (0/1)\n",
    "    Handles:\n",
    "      • 2007–2018 & P files: PAQ650/665 (yes/no), PAQ655/670 (days), PAD660/675 (minutes)\n",
    "      • 2021–2023 (PAQ_L): PAD790Q/U + PAD800 (moderate), PAD810Q/U + PAD820 (vigorous)\n",
    "    \"\"\"\n",
    "    paq = paq.copy()\n",
    "    paq.columns = [c.upper() for c in paq.columns]\n",
    "    out = pd.DataFrame({\"SEQN\": paq[\"SEQN\"]})\n",
    "\n",
    "    def num(col):\n",
    "        return pd.to_numeric(paq.get(col, np.nan), errors=\"coerce\")\n",
    "    def clean_days(s):\n",
    "        return s.mask(s.isin([77, 99]))\n",
    "    def clean_min(s):\n",
    "        return s.mask(s.isin([7777, 9999]))\n",
    "\n",
    "    cols = set(paq.columns)\n",
    "\n",
    "    # ===== New schema (PAQ_L 2021–2023) =====\n",
    "    if {\"PAD790Q\",\"PAD790U\",\"PAD800\",\"PAD810Q\",\"PAD810U\",\"PAD820\"}.issubset(cols):\n",
    "\n",
    "        def per_week(q_col, u_col):\n",
    "            q = num(q_col).mask(num(q_col).isin([7777, 9999]))\n",
    "            u = paq[u_col].astype(str).str.strip().str.upper()\n",
    "            # D=day, W=week, M=month, Y=year → convert to times per week\n",
    "            mult = np.where(u==\"D\", 7.0, np.where(u==\"W\", 1.0, np.where(u==\"M\", 52.0/12.0, np.where(u==\"Y\", 52.0, np.nan))))\n",
    "            return q * pd.to_numeric(mult, errors=\"coerce\")\n",
    "\n",
    "        mod_week = per_week(\"PAD790Q\",\"PAD790U\")\n",
    "        vig_week = per_week(\"PAD810Q\",\"PAD810U\")\n",
    "        mod_min_occ = clean_min(num(\"PAD800\"))\n",
    "        vig_min_occ = clean_min(num(\"PAD820\"))\n",
    "\n",
    "        mod_met_min = 4.0 * mod_week.fillna(0) * mod_min_occ.fillna(0)\n",
    "        vig_met_min = 8.0 * vig_week.fillna(0) * vig_min_occ.fillna(0)\n",
    "        total_met_min = mod_met_min + vig_met_min\n",
    "\n",
    "        has_mod = mod_week.notna() & mod_min_occ.notna()\n",
    "        has_vig = vig_week.notna() & vig_min_occ.notna()\n",
    "        has_any = has_mod | has_vig\n",
    "\n",
    "        # true zero: explicit 0 frequency for both domains\n",
    "        zero_mod = (num(\"PAD790Q\") == 0)\n",
    "        zero_vig = (num(\"PAD810Q\") == 0)\n",
    "        true_zero = (zero_mod & zero_vig)\n",
    "\n",
    "        total_met_min = total_met_min.where(has_any, np.nan).where(~true_zero, 0)\n",
    "\n",
    "        imp = pd.Series(0, index=paq.index, dtype=\"int8\")\n",
    "        imp[(~has_any) & (~true_zero)] = 1\n",
    "\n",
    "    # ===== Old schema (2007–2018 & P files) =====\n",
    "    else:\n",
    "        any_vig = num(\"PAQ650\")  # 1 yes / 2 no\n",
    "        any_mod = num(\"PAQ665\")  # 1 yes / 2 no\n",
    "\n",
    "        vig_days = clean_days(num(\"PAQ655\"))\n",
    "        vig_min  = clean_min(num(\"PAD660\"))\n",
    "        mod_days = clean_days(num(\"PAQ670\"))\n",
    "        mod_min  = clean_min(num(\"PAD675\"))\n",
    "\n",
    "        vig_met_min = 8.0 * vig_days.fillna(0) * vig_min.fillna(0)\n",
    "        mod_met_min = 4.0 * mod_days.fillna(0) * mod_min.fillna(0)\n",
    "        total_met_min = vig_met_min + mod_met_min\n",
    "\n",
    "        has_any = ((vig_days.notna() & vig_min.notna()) | (mod_days.notna() & mod_min.notna()))\n",
    "        both_no = (any_vig == 2) & (any_mod == 2)\n",
    "\n",
    "        total_met_min = total_met_min.where(has_any, np.nan).where(~both_no, 0)\n",
    "\n",
    "        imp = pd.Series(0, index=paq.index, dtype=\"int8\")\n",
    "        imp[(~has_any) & (~both_no)] = 1\n",
    "\n",
    "    out[\"LTPA\"] = (total_met_min / 60.0).clip(lower=0)       # MET-h/week\n",
    "    out[\"METSCORE\"] = total_met_min.clip(lower=0).round(0)   # MET-min/week\n",
    "    out[\"IMP\"] = imp\n",
    "    return out\n",
    "\n",
    "def load_demo_keep(demo: pd.DataFrame) -> pd.DataFrame:\n",
    "    demo = _upper_df(demo)\n",
    "    keep = [c for c in [\"SEQN\", \"SDDSRVYR\", \"SDMVPSU\", \"SDMVSTRA\"] if c in demo.columns]\n",
    "    return demo[keep].copy()\n",
    "\n",
    "# ------------ Build only 2019–2023 and append to 9918 ------------\n",
    "def make_9923_from_9918(old_file=OLD_FILE, out_file=OUT_FILE):\n",
    "    if not os.path.exists(old_file):\n",
    "        raise FileNotFoundError(f\"Missing {old_file}. Provide your totalpa_9918_imputed.csv first.\")\n",
    "\n",
    "    old = pd.read_csv(old_file)\n",
    "    old_cols = list(old.columns)\n",
    "\n",
    "    # detect SDDSRVYR column name used in old file (could be 'sddsrvyr')\n",
    "    sdd_oldname = next((c for c in old_cols if c.lower() == \"sddsrvyr\"), None)\n",
    "\n",
    "    new_parts = []\n",
    "    for year in (2019, 2021):\n",
    "        print(f\"… downloading PAQ/DEMO for {year}…\")\n",
    "        # PAQ\n",
    "        paq_url = PAQ_URLS[year]\n",
    "        paq_path = os.path.join(TMP_DIR, os.path.basename(paq_url))\n",
    "        try:\n",
    "            _dl(paq_url, paq_path)\n",
    "        except RuntimeError:\n",
    "            if year == 2019:\n",
    "                print(\"   ↪ PAQ_K not available, trying combined P_PAQ (2017)…\")\n",
    "                paq_fallback = PAQ_FALLBACK_2017\n",
    "                paq_path = os.path.join(TMP_DIR, os.path.basename(paq_fallback))\n",
    "                _dl(paq_fallback, paq_path)\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "        # DEMO\n",
    "        demo_url = DEMO_URLS[year]\n",
    "        demo_path = os.path.join(TMP_DIR, os.path.basename(demo_url))\n",
    "        try:\n",
    "            _dl(demo_url, demo_path)\n",
    "        except RuntimeError:\n",
    "            if year == 2019:\n",
    "                print(\"   ↪ DEMO_K not available, trying combined P_DEMO (2017)…\")\n",
    "                demo_fallback = DEMO_FALLBACK_2017\n",
    "                demo_path = os.path.join(TMP_DIR, os.path.basename(demo_fallback))\n",
    "                _dl(demo_fallback, demo_path)\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "        paq = _read_xpt(paq_path)\n",
    "        demo = _read_xpt(demo_path)\n",
    "\n",
    "        derived = derive_ltpa_from_paq(paq)\n",
    "        dem_keep = load_demo_keep(demo)\n",
    "        merged = derived.merge(dem_keep, on=\"SEQN\", how=\"left\")\n",
    "\n",
    "        # Build a frame exactly like the old schema\n",
    "        part = pd.DataFrame(columns=old_cols).reindex(range(len(merged)))\n",
    "\n",
    "        # map ltpa/metscore/imp into whatever the old file used (case-insensitive)\n",
    "        ltpa_name     = next((c for c in old_cols if c.lower() == \"ltpa\"), None)\n",
    "        metscore_name = next((c for c in old_cols if c.lower() == \"metscore\"), None)\n",
    "        imp_name      = next((c for c in old_cols if c.lower() in (\"imp\", \"imputed\", \"ltpa_imputed_flag\", \"pa_imputed_flag\", \"imputed_flag\")), None)\n",
    "\n",
    "        if \"SEQN\" in part.columns: part[\"SEQN\"] = merged[\"SEQN\"].values\n",
    "        if ltpa_name:              part[ltpa_name] = merged[\"LTPA\"].values\n",
    "        if metscore_name:          part[metscore_name] = merged[\"METSCORE\"].values\n",
    "        if imp_name:               part[imp_name] = merged[\"IMP\"].astype(\"int8\").values\n",
    "\n",
    "        for col in [\"SDMVPSU\", \"SDMVSTRA\"]:\n",
    "            if col in merged.columns and col in part.columns:\n",
    "                part[col] = merged[col].values\n",
    "\n",
    "        if sdd_oldname and \"SDDSRVYR\" in merged.columns and sdd_oldname in part.columns:\n",
    "            part[sdd_oldname] = merged[\"SDDSRVYR\"].values\n",
    "\n",
    "        new_parts.append(part)\n",
    "\n",
    "    if not new_parts:\n",
    "        raise RuntimeError(\"No new cycles were built (2019 & 2021).\")\n",
    "\n",
    "    new_appended = pd.concat([old] + new_parts, ignore_index=True)\n",
    "\n",
    "    # Deduplicate by SEQN (keep first occurrence)\n",
    "    seqn_col = next(c for c in new_appended.columns if c.lower() == \"seqn\")\n",
    "    new_appended = new_appended.drop_duplicates(subset=[seqn_col], keep=\"first\")\n",
    "\n",
    "    new_appended.to_csv(out_file, index=False)\n",
    "    print(f\"✓ Wrote {out_file}  (shape={new_appended.shape})\")\n",
    "    return new_appended\n",
    "\n",
    "# --- run ---\n",
    "if __name__ == \"__main__\":\n",
    "    make_9923_from_9918()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6f9ddc80-7601-432b-b9fb-1e45cdb9ed64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72927, 38)\n",
      "   SEQN  sddsrvyr       ltpa  SDMVPSU  SDMVSTRA   age  sex   re  edu   pir  tchol    hdl    ldl     tg     wc    bmi  dm_self  hba1c       fpg  chf  chd   mi  stroke  cancer  emphysema  bronchitis  \\\n",
      "0     2         1   0.000000      3.0       1.0  77.0  1.0  3.0  5.0  5.00  215.0   54.0  136.0  128.0   98.0  24.90      2.0    4.7   85.1143  2.0  2.0  2.0     2.0     1.0        2.0         2.0   \n",
      "1     5         1  41.066667      2.0       8.0  49.0  1.0  3.0  5.0  5.00  279.0   42.0  168.0  347.0   99.9  29.10      2.0    5.5  101.6869  2.0  2.0  2.0     2.0     0.0        2.0         2.0   \n",
      "2     7         1   3.033333      2.0       4.0  59.0  2.0  4.0  2.0  1.04  245.0  105.0  127.0   62.0   90.7  29.39      2.0    5.8   87.0580  2.0  2.0  2.0     2.0     0.0        2.0         2.0   \n",
      "3    10         1   0.000000      1.0       7.0  43.0  1.0  4.0  3.0  1.59  140.0   51.0   80.0   45.0  108.0  30.94      2.0    5.5   91.3546  2.0  2.0  2.0     2.0     0.0        2.0         2.0   \n",
      "4    12         1   5.600000      2.0       6.0  37.0  1.0  3.0  4.0  4.93  156.0   38.0   89.0  146.0  112.8  30.62      2.0    5.2   84.2959  2.0  2.0  2.0     2.0     0.0        2.0         1.0   \n",
      "\n",
      "   asthma  re2  copd         sbp         dbp  dm_rx  chol_rx  angina_rx  htn_rx  roseQ  metscore  imp  \n",
      "0     2.0  3.0   2.0  100.666667   56.666667    0.0      0.0        0.0     0.0    0.0      60.0    1  \n",
      "1     2.0  3.0   2.0  122.000000   82.666667    0.0      0.0        0.0     1.0    0.0    1920.0    1  \n",
      "2     2.0  4.0   2.0  125.333333   80.000000    0.0      0.0        0.0     0.0    0.0       0.0    1  \n",
      "3     2.0  4.0   2.0  145.333333   96.000000    0.0      0.0        0.0     0.0    0.0    8160.0    1  \n",
      "4     2.0  3.0   2.0  176.666667  102.000000    0.0      0.0        0.0     1.0    0.0       0.0    1  \n",
      "sddsrvyr\n",
      "1     4880\n",
      "2     5411\n",
      "3     5041\n",
      "4     4979\n",
      "5     5935\n",
      "6     6218\n",
      "7     5560\n",
      "8     5769\n",
      "9     5719\n",
      "10    5569\n",
      "12    8153\n",
      "66    9693\n",
      "dtype: int64\n",
      "               ltpa      metscore           imp\n",
      "count  13639.000000  1.363900e+04  13722.000000\n",
      "mean      73.884863  5.812653e+03      0.006122\n",
      "std      991.735103  5.960632e+04      0.078003\n",
      "min        0.000000  0.000000e+00      0.000000\n",
      "25%        0.000000  1.200000e+02      0.000000\n",
      "50%        9.333333  1.040000e+03      0.000000\n",
      "75%       32.000000  3.120000e+03      0.000000\n",
      "max    64896.000000  3.893760e+06      1.000000\n",
      "Cycle 10: rows=5569, LTPA non-null=5569 (100.0%), IMP=1 share=0.0%\n",
      "Cycle 12: rows=8153, LTPA non-null=8070 (99.0%), IMP=1 share=1.0%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "p = Path(\"/Users/dengshuyue/Desktop/SDOH/analysis/data/cov/totalpa_9923_imputed.csv\")\n",
    "df = pd.read_csv(p)\n",
    "\n",
    "# normalize a few dtypes for convenience\n",
    "if \"SEQN\" in df.columns:\n",
    "    df[\"SEQN\"] = df[\"SEQN\"].round().astype(\"Int64\")\n",
    "year_col = next(c for c in df.columns if c.lower() == \"sddsrvyr\")\n",
    "df[year_col] = df[year_col].astype(\"Int64\")\n",
    "\n",
    "# peek\n",
    "print(df.shape)\n",
    "print(df.head())\n",
    "\n",
    "# cycles present and counts\n",
    "print(df.groupby(year_col).size())\n",
    "\n",
    "# Focus on the new cycles: 2017–2020 (=10) and 2021–2023 (=12)\n",
    "new_cycles = [10, 12]\n",
    "sub = df[df[year_col].isin(new_cycles)]\n",
    "\n",
    "# LTPA preview (note: lowercase column names)\n",
    "print(sub[[\"ltpa\", \"metscore\", \"imp\"]].describe())\n",
    "\n",
    "# Missingness by cycle for the new years\n",
    "for cyc in new_cycles:\n",
    "    m = df[df[year_col] == cyc]\n",
    "    print(\n",
    "        f\"Cycle {cyc}: rows={len(m)}, \"\n",
    "        f\"LTPA non-null={m['ltpa'].notna().sum()} \"\n",
    "        f\"({m['ltpa'].notna().mean():.1%}), \"\n",
    "        f\"IMP=1 share={(m['imp'] == 1).mean():.1%}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab043fc-9491-43e5-a0e7-1d9f4288f0d5",
   "metadata": {},
   "source": [
    "#### exam the merged pa file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "441ed888-9d4c-483f-9e2d-f9217aad3b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sddsrvyr\n",
      "1.0     4880\n",
      "2.0     5411\n",
      "3.0     5041\n",
      "4.0     4979\n",
      "5.0     5935\n",
      "6.0     6218\n",
      "7.0     5560\n",
      "8.0     5769\n",
      "9.0     5719\n",
      "10.0    5569\n",
      "12.0    8153\n",
      "66.0    9693\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sddsrvyr</th>\n",
       "      <th>n</th>\n",
       "      <th>ltpa_nonnull</th>\n",
       "      <th>ltpa_mean</th>\n",
       "      <th>ltpa_median</th>\n",
       "      <th>metscore_mean</th>\n",
       "      <th>imp_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4880</td>\n",
       "      <td>4880</td>\n",
       "      <td>15.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3404.29</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>5411</td>\n",
       "      <td>5411</td>\n",
       "      <td>17.49</td>\n",
       "      <td>1.87</td>\n",
       "      <td>3527.64</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>5041</td>\n",
       "      <td>5041</td>\n",
       "      <td>14.68</td>\n",
       "      <td>2.33</td>\n",
       "      <td>3334.97</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4979</td>\n",
       "      <td>4979</td>\n",
       "      <td>16.70</td>\n",
       "      <td>3.73</td>\n",
       "      <td>3593.64</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5935</td>\n",
       "      <td>5935</td>\n",
       "      <td>11.35</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3764.14</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.0</td>\n",
       "      <td>6218</td>\n",
       "      <td>6218</td>\n",
       "      <td>11.56</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3178.56</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.0</td>\n",
       "      <td>5560</td>\n",
       "      <td>5560</td>\n",
       "      <td>13.49</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3062.76</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.0</td>\n",
       "      <td>5769</td>\n",
       "      <td>5769</td>\n",
       "      <td>13.11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3142.25</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.0</td>\n",
       "      <td>5719</td>\n",
       "      <td>5719</td>\n",
       "      <td>13.73</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3668.76</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.0</td>\n",
       "      <td>5569</td>\n",
       "      <td>5569</td>\n",
       "      <td>13.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4182.80</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12.0</td>\n",
       "      <td>8153</td>\n",
       "      <td>8070</td>\n",
       "      <td>115.62</td>\n",
       "      <td>16.00</td>\n",
       "      <td>6937.39</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>66.0</td>\n",
       "      <td>9693</td>\n",
       "      <td>9677</td>\n",
       "      <td>14.69</td>\n",
       "      <td>0.00</td>\n",
       "      <td>881.24</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sddsrvyr     n  ltpa_nonnull  ltpa_mean  ltpa_median  metscore_mean  imp_rate\n",
       "0        1.0  4880          4880      15.23         0.00        3404.29      1.00\n",
       "1        2.0  5411          5411      17.49         1.87        3527.64      1.00\n",
       "2        3.0  5041          5041      14.68         2.33        3334.97      1.00\n",
       "3        4.0  4979          4979      16.70         3.73        3593.64      1.00\n",
       "4        5.0  5935          5935      11.35         0.00        3764.14      0.00\n",
       "5        6.0  6218          6218      11.56         0.00        3178.56      0.00\n",
       "6        7.0  5560          5560      13.49         0.00        3062.76      0.00\n",
       "7        8.0  5769          5769      13.11         0.00        3142.25      0.00\n",
       "8        9.0  5719          5719      13.73         0.00        3668.76      0.00\n",
       "9       10.0  5569          5569      13.40         0.00        4182.80      0.00\n",
       "10      12.0  8153          8070     115.62        16.00        6937.39      0.01\n",
       "11      66.0  9693          9677      14.69         0.00         881.24      0.00"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "PATH = Path(\"/Users/dengshuyue/Desktop/SDOH/analysis/data/cov/totalpa_9923_imputed.csv\")\n",
    "df = pd.read_csv(PATH)\n",
    "\n",
    "# cycle column and quick counts\n",
    "year_col = next(c for c in df.columns if c.lower() == \"sddsrvyr\")\n",
    "print(df.groupby(year_col).size())\n",
    "\n",
    "# tiny summary by cycle\n",
    "summ = (\n",
    "    df.groupby(year_col)\n",
    "      .agg(\n",
    "          n=(\"ltpa\",\"size\"),\n",
    "          ltpa_nonnull=(\"ltpa\",\"count\"),\n",
    "          ltpa_mean=(\"ltpa\",\"mean\"),\n",
    "          ltpa_median=(\"ltpa\",\"median\"),\n",
    "          metscore_mean=(\"metscore\",\"mean\"),\n",
    "          imp_rate=(\"imp\", lambda s: (s==1).mean())\n",
    "      )\n",
    "      .round(2)\n",
    "      .reset_index()\n",
    ")\n",
    "summ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06fbcd7-71f8-4129-b23a-c5b4dfc3ae78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007fc7de-c9d7-4c19-9885-3506ce631fd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eaefbd0-a432-42df-a2c7-2180f951d4ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0365d88-c2a3-4694-a219-9a082d519443",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbcec3e-bb97-40c4-b9b7-98cc8071a2e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "090ac294",
   "metadata": {},
   "source": [
    "## Anthropometrics (BMX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "51d739da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bmx(cfg: Config = CONFIG) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load anthropometrics (BMX), tolerate csv/parquet and 99–18 vs 99–23 stacks,\n",
    "    and compute BMI when BMXBMI is missing.\n",
    "    Writes cov_bmx_1999_2023.parquet to cfg.out_dir.\n",
    "    \"\"\"\n",
    "    ensure_dir(cfg.out_dir)\n",
    "\n",
    "    def _read_any(p: Path) -> pd.DataFrame:\n",
    "        return pd.read_parquet(p) if str(p).lower().endswith(\".parquet\") else pd.read_csv(p, low_memory=False)\n",
    "\n",
    "    # 1) Choose a source\n",
    "    if cfg.bmx_9923 and Path(cfg.bmx_9923).exists():\n",
    "        src = Path(cfg.bmx_9923)\n",
    "    else:\n",
    "        src = pick_first_existing(\n",
    "            cfg.interim_dir / \"bmx_9923.parquet\",\n",
    "            cfg.interim_dir / \"bmx_9923.csv\",\n",
    "            cfg.interim_dir / \"bmx_9918.parquet\",\n",
    "            cfg.interim_dir / \"bmx_9918.csv\",\n",
    "        )\n",
    "    if src is None:\n",
    "        raise FileNotFoundError(\n",
    "            \"BMX not found. Provide bmx_9923.parquet (preferred) or bmx_9918.parquet/csv under \"\n",
    "            f\"{cfg.interim_dir}, or set CONFIG.bmx_9923 to a file.\"\n",
    "        )\n",
    "\n",
    "    # 2) Read + normalize\n",
    "    bmx = upper_df(_read_any(src))\n",
    "    for need in [\"SEQN\", \"BMXWT\", \"BMXHT\"]:\n",
    "        if need not in bmx.columns:\n",
    "            raise ValueError(f\"BMX table missing required column: {need}\")\n",
    "\n",
    "    # helper: NaN series aligned to df\n",
    "    def nan_series(df): \n",
    "        return pd.Series(np.nan, index=df.index, dtype=\"float\")\n",
    "\n",
    "    # 3) BMI: use BMXBMI if present, else compute weight(kg) / (height(m))^2\n",
    "    bmi_src = pd.to_numeric(bmx[\"BMXBMI\"], errors=\"coerce\") if \"BMXBMI\" in bmx.columns else nan_series(bmx)\n",
    "    wt = pd.to_numeric(bmx[\"BMXWT\"], errors=\"coerce\")\n",
    "    ht_cm = pd.to_numeric(bmx[\"BMXHT\"], errors=\"coerce\")\n",
    "\n",
    "    bmi = bmi_src.copy()\n",
    "    missing = bmi.isna()\n",
    "    if missing.any():\n",
    "        bmi.loc[missing] = wt.loc[missing] / (ht_cm.loc[missing] / 100.0) ** 2\n",
    "\n",
    "    out = pd.DataFrame(\n",
    "        {\n",
    "            \"SEQN\": bmx[\"SEQN\"],\n",
    "            \"BMXWT\": wt,\n",
    "            \"BMXHT\": ht_cm,\n",
    "            \"BMI\": pd.to_numeric(bmi, errors=\"coerce\"),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    outp = cfg.out_dir / cfg.cov_bmx\n",
    "    out.to_parquet(outp, index=False)\n",
    "    log(f\"✓ BMX → {outp} (source: {src.name})\")\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d00420",
   "metadata": {},
   "source": [
    "## Clinical & Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc391756",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class ClinicalThresholds:\n",
    "    htn_sbp: float = 140.0\n",
    "    htn_dbp: float = 90.0\n",
    "    a1c_diabetes: float = 6.5\n",
    "    fpg_diabetes: float = 126.0  # mg/dL\n",
    "\n",
    "THR = ClinicalThresholds()\n",
    "\n",
    "def build_clinical(cfg: Config = CONFIG, thr: ClinicalThresholds = THR) -> pd.DataFrame:\n",
    "    ensure_dir(cfg.out_dir)\n",
    "\n",
    "    clin_9923 = pick_first_existing(cfg.interim_dir / \"clinical_9923.parquet\", cfg.interim_dir / \"clinical_9923.csv\")\n",
    "    if clin_9923:\n",
    "        clin = read_any(clin_9923)\n",
    "    else:\n",
    "        p9918 = pick_first_existing(\n",
    "            cfg.clinical_9918,\n",
    "            cfg.interim_dir / \"clinical_9918.parquet\",\n",
    "            cfg.interim_dir / \"clinical_9918.csv\",\n",
    "            cfg.interim_dir / \"nhanes_primary_anal_full_singleimputation_v2.parquet\",\n",
    "            cfg.interim_dir / \"nhanes_primary_anal_full_singleimputation_v2.csv\",\n",
    "        )\n",
    "        p1923 = pick_first_existing(cfg.clinical_1923, cfg.interim_dir / \"clinical_1923.parquet\", cfg.interim_dir / \"clinical_1923.csv\")\n",
    "        if p9918 is None:\n",
    "            raise FileNotFoundError(\"Provide clinical_9923 or clinical_9918 under interim.\")\n",
    "        clin = read_any(p9918)\n",
    "        if p1923:\n",
    "            clin = pd.concat([clin, read_any(p1923)], ignore_index=True)\n",
    "\n",
    "    clin = upper_df(clin)\n",
    "\n",
    "    # Derive BMI_CLAS if missing\n",
    "    if \"BMI_CLAS\" not in clin.columns:\n",
    "        bmi_src = None\n",
    "        bmx_path = cfg.out_dir / cfg.cov_bmx\n",
    "        if bmx_path.exists():\n",
    "            bmx = upper_df(pd.read_parquet(bmx_path))\n",
    "            if {\"SEQN\", \"BMI\"}.issubset(bmx.columns) and \"SEQN\" in clin.columns:\n",
    "                bmi_src = clin[\"SEQN\"].map(bmx.set_index(\"SEQN\")[\"BMI\"]).astype(float)\n",
    "        if bmi_src is None:\n",
    "            bmi_src = pd.to_numeric(clin.get(\"BMI\", np.nan), errors=\"coerce\")\n",
    "\n",
    "        def bmi_class(x):\n",
    "            if pd.isna(x):\n",
    "                return pd.NA\n",
    "            if x < 18.5:\n",
    "                return \"UNDER\"\n",
    "            if x < 25:\n",
    "                return \"NORMAL\"\n",
    "            if x < 30:\n",
    "                return \"OVER\"\n",
    "            return \"OBESE\"\n",
    "\n",
    "        clin[\"BMI_CLAS\"] = pd.Series([bmi_class(v) for v in bmi_src], dtype=\"string\")\n",
    "\n",
    "    # Derive HTN if missing\n",
    "    if \"HTN\" not in clin.columns:\n",
    "        sbp = pd.to_numeric(clin.get(\"SBP\", np.nan), errors=\"coerce\")\n",
    "        dbp = pd.to_numeric(clin.get(\"DBP\", np.nan), errors=\"coerce\")\n",
    "        diag_col = next((c for c in clin.columns if ((\"HTN\" in c or \"HYPERT\" in c) and \"MED\" not in c and c != \"HTN\")), None)\n",
    "        med_col = next((c for c in clin.columns if (\"MED\" in c and (\"BP\" in c or \"HYPER\" in c))), None)\n",
    "        htn = pd.Series(0, index=clin.index, dtype=\"Int8\")\n",
    "        if diag_col:\n",
    "            diag = pd.to_numeric(clin[diag_col], errors=\"coerce\")\n",
    "            htn = ((diag == 1) | (diag > 0)).astype(\"Int8\")\n",
    "        if med_col:\n",
    "            med = pd.to_numeric(clin[med_col], errors=\"coerce\")\n",
    "            htn = ((htn == 1) | (med == 1) | (med > 0)).astype(\"Int8\")\n",
    "        htn = ((htn == 1) | (sbp >= thr.htn_sbp) | (dbp >= thr.htn_dbp)).astype(\"Int8\")\n",
    "        clin[\"HTN\"] = htn\n",
    "\n",
    "    # Derive HIGH_CHOL if missing\n",
    "    if \"HIGH_CHOL\" not in clin.columns:\n",
    "        tch = pd.to_numeric(clin.get(\"TCHOL\", np.nan), errors=\"coerce\")\n",
    "        ldl = pd.to_numeric(clin.get(\"LDL\", np.nan), errors=\"coerce\")\n",
    "        med_col = next((c for c in clin.columns if (\"CHOL\" in c and \"MED\" in c)), None)\n",
    "        high = ((tch >= 240) | (ldl >= 160)).astype(\"Int8\")\n",
    "        if med_col:\n",
    "            med = pd.to_numeric(clin[med_col], errors=\"coerce\")\n",
    "            high = ((high == 1) | (med == 1) | (med > 0)).astype(\"Int8\")\n",
    "        clin[\"HIGH_CHOL\"] = high\n",
    "\n",
    "    keep = [\"SEQN\", \"BMI_CLAS\", \"DIABETES\", \"HTN\", \"HIGH_CHOL\", \"CVD\", \"CANCER\", \"SBP\", \"DBP\", \"TCHOL\", \"HDL\", \"LDL\", \"TG\"]\n",
    "    for k in keep:\n",
    "        if k not in clin.columns:\n",
    "            clin[k] = pd.Series(np.nan, index=clin.index)\n",
    "\n",
    "    out = clin[keep].copy()\n",
    "    for b in [\"DIABETES\", \"HTN\", \"HIGH_CHOL\", \"CVD\", \"CANCER\"]:\n",
    "        out[b] = pd.to_numeric(out[b], errors=\"coerce\").astype(\"Int8\")\n",
    "\n",
    "    outp = cfg.out_dir / cfg.cov_clinical\n",
    "    out.to_parquet(outp, index=False)\n",
    "    log(f\"✓ CLN → {outp}\")\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98831e35",
   "metadata": {},
   "source": [
    "## Household & Survey Core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b08d357",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SURVEY_KEEP = [\"SEQN\", \"SDDSRVYR\", \"SDMVPSU\", \"SDMVSTRA\", \"WTMEC2YR\"]\n",
    "\n",
    "def build_household(cfg: Config = CONFIG) -> pd.DataFrame:\n",
    "    ensure_dir(cfg.out_dir)\n",
    "    demo = upper_df(pd.read_parquet(cfg.demo_9923))\n",
    "    if \"DMDHHSIZ\" not in demo.columns:\n",
    "        raise ValueError(\"DMDHHSIZ not found in DEMO stack.\")\n",
    "    out = demo[[\"SEQN\", \"DMDHHSIZ\"]].copy()\n",
    "    outp = cfg.out_dir / cfg.cov_household\n",
    "    out.to_parquet(outp, index=False)\n",
    "    log(f\"✓ HH  → {outp}\")\n",
    "    return out\n",
    "\n",
    "def get_survey_core(cfg: Config = CONFIG) -> pd.DataFrame:\n",
    "    demo_p = None\n",
    "    if cfg.demo_9923 and Path(cfg.demo_9923).exists():\n",
    "        demo_p = cfg.demo_9923\n",
    "    elif cfg.demo_9918 and Path(cfg.demo_9918).exists():\n",
    "        demo_p = cfg.demo_9918\n",
    "    else:\n",
    "        demo_p = pick_first_existing(cfg.interim_dir / \"demo_9923.parquet\", cfg.interim_dir / \"demo_9918.parquet\")\n",
    "    if demo_p is None:\n",
    "        raise FileNotFoundError(\"Could not find DEMO table.\")\n",
    "\n",
    "    demo = upper_df(pd.read_parquet(demo_p))\n",
    "    miss = [c for c in SURVEY_KEEP if c not in demo.columns]\n",
    "    if miss:\n",
    "        raise ValueError(f\"Missing survey fields in DEMO: {miss}\")\n",
    "    return demo[SURVEY_KEEP].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86702c27",
   "metadata": {},
   "source": [
    "## Merge to Core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5711533",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_core(cfg: Config = CONFIG) -> pd.DataFrame:\n",
    "    ensure_dir(cfg.out_dir)\n",
    "\n",
    "    smk = pd.read_parquet(cfg.out_dir / cfg.cov_smk)\n",
    "    alc = pd.read_parquet(cfg.out_dir / cfg.cov_alc)\n",
    "    pa = pd.read_parquet(cfg.out_dir / cfg.cov_pa)\n",
    "    bmx = pd.read_parquet(cfg.out_dir / cfg.cov_bmx)\n",
    "    clin = pd.read_parquet(cfg.out_dir / cfg.cov_clinical)\n",
    "    hh = pd.read_parquet(cfg.out_dir / cfg.cov_household)\n",
    "    survey = get_survey_core(cfg)\n",
    "\n",
    "    core = survey\n",
    "    for part in [smk, alc, pa, bmx, clin, hh]:\n",
    "        core = core.merge(part, on=\"SEQN\", how=\"left\")\n",
    "\n",
    "    core.columns = [c.upper() for c in core.columns]\n",
    "    outp = cfg.out_dir / cfg.cov_core\n",
    "    core.to_parquet(outp, index=False)\n",
    "    log(f\"✓ CORE → {outp}\")\n",
    "    return core\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566a8a6a",
   "metadata": {},
   "source": [
    "## Orchestrator & Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8512c6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_all(cfg: Config = CONFIG) -> Dict[str, pd.DataFrame]:\n",
    "    ensure_dir(cfg.out_dir)\n",
    "    out: Dict[str, pd.DataFrame] = {}\n",
    "    out[\"smk\"] = build_smk(cfg)\n",
    "    out[\"alc\"] = build_alc(cfg)\n",
    "    out[\"pa\"] = build_pa(cfg)\n",
    "    out[\"bmx\"] = build_bmx(cfg)\n",
    "    out[\"clinical\"] = build_clinical(cfg)\n",
    "    out[\"household\"] = build_household(cfg)\n",
    "    out[\"core\"] = build_core(cfg)\n",
    "    return out\n",
    "\n",
    "def quick_checks(cfg: Config = CONFIG) -> pd.Series:\n",
    "    core = pd.read_parquet(cfg.out_dir / cfg.cov_core)\n",
    "    checks = {\n",
    "        \"n_rows\": int(len(core)),\n",
    "        \"n_unique_seqn\": int(core[\"SEQN\"].nunique()),\n",
    "        \"missing_bmi_pct\": float(core[\"BMI\"].isna().mean()),\n",
    "        \"missing_alcohol_cat_pct\": float(core[\"ALCOHOL_CAT\"].isna().mean()),\n",
    "        \"missing_smk_status_pct\": float(core[\"SMK_STATUS\"].isna().mean()),\n",
    "        \"has_weights\": int(\"WTMEC2YR\" in core.columns),\n",
    "    }\n",
    "    return pd.Series(checks)\n",
    "\n",
    "def missingness_by_era(cfg: Config = CONFIG) -> pd.DataFrame:\n",
    "    core = pd.read_parquet(cfg.out_dir / cfg.cov_core)\n",
    "    era = np.where(core[\"SDDSRVYR\"] <= 10, \"1999–2018\", \"2019–2023\")\n",
    "\n",
    "    def miss(col: str) -> pd.Series:\n",
    "        return core[col].isna().groupby(era).mean().mul(100).round(1)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"SMK_STATUS %NA\": miss(\"SMK_STATUS\"),\n",
    "        \"LTPA_MET_HR_WK %NA\": miss(\"LTPA_MET_HR_WK\"),\n",
    "        \"BMI %NA\": miss(\"BMI\"),\n",
    "        \"WTMEC2YR %NA\": miss(\"WTMEC2YR\"),\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdaad51",
   "metadata": {},
   "source": [
    "## Quick Start (run these)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d3b6194f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ SMK → /Users/dengshuyue/Desktop/SDOH/analysis/output/cov_smk_1999_2023.parquet (using smk_9918.csv); missing: {'SMK_STATUS': 0.001, 'CIGS_PER_DAY': 0.794, 'PACK_YEARS': 0.78}\n",
      "✓ ALC → /Users/dengshuyue/Desktop/SDOH/analysis/output/cov_alc_1999_2023.parquet\n",
      "✓ PA  → /Users/dengshuyue/Desktop/SDOH/analysis/output/cov_pa_1999_2023.parquet\n",
      "✓ BMX → /Users/dengshuyue/Desktop/SDOH/analysis/output/cov_bmx_1999_2023.parquet (source: bmx_9918.csv)\n",
      "✓ CLN → /Users/dengshuyue/Desktop/SDOH/analysis/output/cov_clinical_1999_2023.parquet\n",
      "✓ HH  → /Users/dengshuyue/Desktop/SDOH/analysis/output/cov_household_1999_2023.parquet\n",
      "✓ CORE → /Users/dengshuyue/Desktop/SDOH/analysis/output/cov_core_1999_2023.parquet\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'smk':            SEQN SMK_STATUS  CIGS_PER_DAY  PACK_YEARS  FORMER_SMOKER\n",
       " 0           2.0      NEVER           NaN         NaN              0\n",
       " 1           5.0     FORMER           NaN         NaN              1\n",
       " 2           7.0     FORMER           NaN   8030.0000              1\n",
       " 3          10.0    CURRENT           1.0         NaN              0\n",
       " 4          12.0      NEVER           NaN         NaN              0\n",
       " ...         ...        ...           ...         ...            ...\n",
       " 55076  102950.0     FORMER           NaN   6198.0000              1\n",
       " 55077  102952.0      NEVER           NaN         NaN              0\n",
       " 55078  102953.0     FORMER           NaN    387.8125              1\n",
       " 55079  102954.0      NEVER           NaN         NaN              0\n",
       " 55080  102956.0    CURRENT           2.0         NaN              0\n",
       " \n",
       " [55081 rows x 5 columns],\n",
       " 'alc':            SEQN  DRINKS_PER_DAY ALCOHOL_CAT\n",
       " 0           2.0        0.789041    MODERATE\n",
       " 1           5.0       12.000000       HEAVY\n",
       " 2           7.0             NaN        NONE\n",
       " 3          10.0        0.197260    MODERATE\n",
       " 4          12.0        0.857129    MODERATE\n",
       " ...         ...             ...         ...\n",
       " 68856  142305.0             NaN        NONE\n",
       " 68857  142307.0             NaN        NONE\n",
       " 68858  142308.0             NaN        NONE\n",
       " 68859  142309.0             NaN        NONE\n",
       " 68860  142310.0             NaN        NONE\n",
       " \n",
       " [68861 rows x 3 columns],\n",
       " 'pa':          SEQN  LTPA_MET_HR_WK  LTPA_IMPUTED_FLAG\n",
       " 0           2             NaN                  0\n",
       " 1           5             NaN                  0\n",
       " 2           7             NaN                  0\n",
       " 3          10             NaN                  0\n",
       " 4          12             NaN                  0\n",
       " ...       ...             ...                ...\n",
       " 55076  102950             NaN                  0\n",
       " 55077  102952             NaN                  0\n",
       " 55078  102953             NaN                  0\n",
       " 55079  102954             NaN                  0\n",
       " 55080  102956             NaN                  0\n",
       " \n",
       " [55081 rows x 3 columns],\n",
       " 'bmx':            SEQN  BMXWT  BMXHT        BMI\n",
       " 0           1.0   12.5   91.6  14.897695\n",
       " 1           2.0   75.4  174.0  24.904215\n",
       " 2           3.0   32.9  136.6  17.631713\n",
       " 3           4.0   13.3    NaN        NaN\n",
       " 4           5.0   92.5  178.3  29.096386\n",
       " ...         ...    ...    ...        ...\n",
       " 96761  102952.0   49.0  156.5  20.006329\n",
       " 96762  102953.0   97.4  164.9  35.819345\n",
       " 96763  102954.0   69.1  162.6  26.135870\n",
       " 96764  102955.0  111.9  156.6  45.629590\n",
       " 96765  102956.0  111.5  175.8  36.077557\n",
       " \n",
       " [96766 rows x 4 columns],\n",
       " 'clinical':             SEQN BMI_CLAS  DIABETES  HTN  HIGH_CHOL  CVD  CANCER         SBP        DBP  TCHOL  HDL  LDL   TG\n",
       " 0            1.0    UNDER         0    0          0    0       0   91.333333  56.000000    131   59   54   99\n",
       " 1            2.0   NORMAL         0    0          0    0       1  100.666667  56.666667    215   54  136  128\n",
       " 2            3.0    UNDER         0    0          0    0       0  108.666667  62.000000    129   30   58  202\n",
       " 3            4.0     <NA>         0    0          1    0       0   95.333333  61.333333    211   43  161   37\n",
       " 4            5.0     OVER         0    1          1    0       0  122.000000  82.666667    279   42  168  347\n",
       " ...          ...      ...       ...  ...        ...  ...     ...         ...        ...    ...  ...  ...  ...\n",
       " 101311  102952.0   NORMAL         1    0          0    0       0  139.333300  73.333330    119   60   43   78\n",
       " 101312  102953.0    OBESE         0    0          0    0       0  120.666700  75.333330    182   49   73  299\n",
       " 101313  102954.0     OVER         0    0          0    0       0  116.000000  70.666670    172   54  108   49\n",
       " 101314  102955.0    OBESE         0    0          0    0       0  114.000000  62.000000    150   34   77  193\n",
       " 101315  102956.0    OBESE         0    1          0    0       0  148.000000  96.000000    163   34   75  269\n",
       " \n",
       " [101316 rows x 13 columns],\n",
       " 'household':             SEQN  DMDHHSIZ\n",
       " 0            1.0       3.0\n",
       " 1            2.0       1.0\n",
       " 2            3.0       4.0\n",
       " 3            4.0       7.0\n",
       " 4            5.0       3.0\n",
       " ...          ...       ...\n",
       " 128804  142306.0       2.0\n",
       " 128805  142307.0       5.0\n",
       " 128806  142308.0       3.0\n",
       " 128807  142309.0       5.0\n",
       " 128808  142310.0       2.0\n",
       " \n",
       " [128809 rows x 2 columns],\n",
       " 'core':             SEQN  SDDSRVYR  SDMVPSU  SDMVSTRA      WTMEC2YR SMK_STATUS  CIGS_PER_DAY  PACK_YEARS  FORMER_SMOKER  DRINKS_PER_DAY ALCOHOL_CAT  LTPA_MET_HR_WK  LTPA_IMPUTED_FLAG  BMXWT  BMXHT  \\\n",
       " 0            1.0       1.0      1.0       5.0  10982.898896        NaN           NaN         NaN            NaN             NaN         NaN             NaN                NaN   12.5   91.6   \n",
       " 1            2.0       1.0      3.0       1.0  28325.384898      NEVER           NaN         NaN            0.0        0.789041    MODERATE             NaN                0.0   75.4  174.0   \n",
       " 2            3.0       1.0      2.0       7.0  46192.256945        NaN           NaN         NaN            NaN             NaN         NaN             NaN                NaN   32.9  136.6   \n",
       " 3            4.0       1.0      1.0       2.0  10251.260020        NaN           NaN         NaN            NaN             NaN         NaN             NaN                NaN   13.3    NaN   \n",
       " 4            5.0       1.0      2.0       8.0  99445.065735     FORMER           NaN         NaN            1.0       12.000000       HEAVY             NaN                0.0   92.5  178.3   \n",
       " ...          ...       ...      ...       ...           ...        ...           ...         ...            ...             ...         ...             ...                ...    ...    ...   \n",
       " 128804  142306.0      12.0      1.0     176.0  13459.129019        NaN           NaN         NaN            NaN             NaN         NaN             NaN                NaN    NaN    NaN   \n",
       " 128805  142307.0      12.0      1.0     181.0  64962.328962        NaN           NaN         NaN            NaN             NaN        NONE             NaN                NaN    NaN    NaN   \n",
       " 128806  142308.0      12.0      2.0     183.0  44367.534132        NaN           NaN         NaN            NaN             NaN        NONE             NaN                NaN    NaN    NaN   \n",
       " 128807  142309.0      12.0      1.0     176.0  46249.361849        NaN           NaN         NaN            NaN             NaN        NONE             NaN                NaN    NaN    NaN   \n",
       " 128808  142310.0      12.0      2.0     187.0  49647.225467        NaN           NaN         NaN            NaN             NaN        NONE             NaN                NaN    NaN    NaN   \n",
       " \n",
       "               BMI BMI_CLAS  DIABETES   HTN  HIGH_CHOL   CVD  CANCER         SBP        DBP  TCHOL   HDL    LDL     TG  DMDHHSIZ  \n",
       " 0       14.897695    UNDER         0     0          0     0       0   91.333333  56.000000  131.0  59.0   54.0   99.0       3.0  \n",
       " 1       24.904215   NORMAL         0     0          0     0       1  100.666667  56.666667  215.0  54.0  136.0  128.0       1.0  \n",
       " 2       17.631713    UNDER         0     0          0     0       0  108.666667  62.000000  129.0  30.0   58.0  202.0       4.0  \n",
       " 3             NaN     <NA>         0     0          1     0       0   95.333333  61.333333  211.0  43.0  161.0   37.0       7.0  \n",
       " 4       29.096386     OVER         0     1          1     0       0  122.000000  82.666667  279.0  42.0  168.0  347.0       3.0  \n",
       " ...           ...      ...       ...   ...        ...   ...     ...         ...        ...    ...   ...    ...    ...       ...  \n",
       " 128804        NaN     <NA>      <NA>  <NA>       <NA>  <NA>    <NA>         NaN        NaN    NaN   NaN    NaN    NaN       2.0  \n",
       " 128805        NaN     <NA>      <NA>  <NA>       <NA>  <NA>    <NA>         NaN        NaN    NaN   NaN    NaN    NaN       5.0  \n",
       " 128806        NaN     <NA>      <NA>  <NA>       <NA>  <NA>    <NA>         NaN        NaN    NaN   NaN    NaN    NaN       3.0  \n",
       " 128807        NaN     <NA>      <NA>  <NA>       <NA>  <NA>    <NA>         NaN        NaN    NaN   NaN    NaN    NaN       5.0  \n",
       " 128808        NaN     <NA>      <NA>  <NA>       <NA>  <NA>    <NA>         NaN        NaN    NaN   NaN    NaN    NaN       2.0  \n",
       " \n",
       " [128809 rows x 29 columns]}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Optional: Adjust paths if needed\n",
    "# CONFIG.out_dir = Path(\"/Users/dengshuyue/Desktop/SDOH/analysis/output\")\n",
    "# CONFIG.interim_dir = Path(\"/Users/dengshuyue/Desktop/SDOH/analysis/data/cov\")\n",
    "# CONFIG.demo_9923 = Path(\"/Users/dengshuyue/Desktop/SDOH/analysis/data/cov/demo9923.parquet\")\n",
    "\n",
    "# delete stale alcohol file and rebuild\n",
    "(CONFIG.out_dir / CONFIG.cov_alc).unlink(missing_ok=True)\n",
    "run_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "23332b4c-7df3-4118-8654-8d14029443f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((128809, 29),\n",
       " Index(['SEQN', 'SDDSRVYR', 'SDMVPSU', 'SDMVSTRA', 'WTMEC2YR', 'SMK_STATUS', 'CIGS_PER_DAY', 'PACK_YEARS', 'FORMER_SMOKER', 'DRINKS_PER_DAY', 'ALCOHOL_CAT', 'LTPA_MET_HR_WK', 'LTPA_IMPUTED_FLAG',\n",
       "        'BMXWT', 'BMXHT', 'BMI', 'BMI_CLAS', 'DIABETES', 'HTN', 'HIGH_CHOL', 'CVD', 'CANCER', 'SBP', 'DBP', 'TCHOL', 'HDL', 'LDL', 'TG', 'DMDHHSIZ'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "core.shape, core.columns[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c02ac5a1-8236-4932-9201-aeb2b1996efd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEQN</th>\n",
       "      <th>SDDSRVYR</th>\n",
       "      <th>SDMVPSU</th>\n",
       "      <th>SDMVSTRA</th>\n",
       "      <th>WTMEC2YR</th>\n",
       "      <th>SMK_STATUS</th>\n",
       "      <th>CIGS_PER_DAY</th>\n",
       "      <th>PACK_YEARS</th>\n",
       "      <th>FORMER_SMOKER</th>\n",
       "      <th>DRINKS_PER_DAY</th>\n",
       "      <th>ALCOHOL_CAT</th>\n",
       "      <th>LTPA_MET_HR_WK</th>\n",
       "      <th>LTPA_IMPUTED_FLAG</th>\n",
       "      <th>BMXWT</th>\n",
       "      <th>BMXHT</th>\n",
       "      <th>BMI</th>\n",
       "      <th>BMI_CLAS</th>\n",
       "      <th>DIABETES</th>\n",
       "      <th>HTN</th>\n",
       "      <th>HIGH_CHOL</th>\n",
       "      <th>CVD</th>\n",
       "      <th>CANCER</th>\n",
       "      <th>SBP</th>\n",
       "      <th>DBP</th>\n",
       "      <th>TCHOL</th>\n",
       "      <th>HDL</th>\n",
       "      <th>LDL</th>\n",
       "      <th>TG</th>\n",
       "      <th>DMDHHSIZ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10982.898896</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.5</td>\n",
       "      <td>91.6</td>\n",
       "      <td>14.897695</td>\n",
       "      <td>UNDER</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>91.333333</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>131.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28325.384898</td>\n",
       "      <td>NEVER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.789041</td>\n",
       "      <td>MODERATE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75.4</td>\n",
       "      <td>174.0</td>\n",
       "      <td>24.904215</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>100.666667</td>\n",
       "      <td>56.666667</td>\n",
       "      <td>215.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>46192.256945</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.9</td>\n",
       "      <td>136.6</td>\n",
       "      <td>17.631713</td>\n",
       "      <td>UNDER</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>108.666667</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>129.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10251.260020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>95.333333</td>\n",
       "      <td>61.333333</td>\n",
       "      <td>211.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>99445.065735</td>\n",
       "      <td>FORMER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>HEAVY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>92.5</td>\n",
       "      <td>178.3</td>\n",
       "      <td>29.096386</td>\n",
       "      <td>OVER</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>82.666667</td>\n",
       "      <td>279.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>347.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>39656.600444</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>59.2</td>\n",
       "      <td>162.0</td>\n",
       "      <td>22.557537</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>114.666667</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>153.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>25525.423409</td>\n",
       "      <td>FORMER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8030.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>162.9</td>\n",
       "      <td>29.393577</td>\n",
       "      <td>OVER</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>125.333333</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>245.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>31510.587866</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.7</td>\n",
       "      <td>162.0</td>\n",
       "      <td>15.508307</td>\n",
       "      <td>UNDER</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100.666667</td>\n",
       "      <td>49.333333</td>\n",
       "      <td>162.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7575.870247</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>45.5</td>\n",
       "      <td>156.9</td>\n",
       "      <td>18.482704</td>\n",
       "      <td>UNDER</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>109.333333</td>\n",
       "      <td>53.333333</td>\n",
       "      <td>148.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>22445.808572</td>\n",
       "      <td>CURRENT</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.197260</td>\n",
       "      <td>MODERATE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>111.8</td>\n",
       "      <td>190.1</td>\n",
       "      <td>30.936955</td>\n",
       "      <td>OBESE</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>145.333333</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>140.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SEQN  SDDSRVYR  SDMVPSU  SDMVSTRA      WTMEC2YR SMK_STATUS  CIGS_PER_DAY  PACK_YEARS  FORMER_SMOKER  DRINKS_PER_DAY ALCOHOL_CAT  LTPA_MET_HR_WK  LTPA_IMPUTED_FLAG  BMXWT  BMXHT        BMI  \\\n",
       "0   1.0       1.0      1.0       5.0  10982.898896        NaN           NaN         NaN            NaN             NaN         NaN             NaN                NaN   12.5   91.6  14.897695   \n",
       "1   2.0       1.0      3.0       1.0  28325.384898      NEVER           NaN         NaN            0.0        0.789041    MODERATE             NaN                0.0   75.4  174.0  24.904215   \n",
       "2   3.0       1.0      2.0       7.0  46192.256945        NaN           NaN         NaN            NaN             NaN         NaN             NaN                NaN   32.9  136.6  17.631713   \n",
       "3   4.0       1.0      1.0       2.0  10251.260020        NaN           NaN         NaN            NaN             NaN         NaN             NaN                NaN   13.3    NaN        NaN   \n",
       "4   5.0       1.0      2.0       8.0  99445.065735     FORMER           NaN         NaN            1.0       12.000000       HEAVY             NaN                0.0   92.5  178.3  29.096386   \n",
       "5   6.0       1.0      2.0       2.0  39656.600444        NaN           NaN         NaN            NaN             NaN         NaN             NaN                NaN   59.2  162.0  22.557537   \n",
       "6   7.0       1.0      2.0       4.0  25525.423409     FORMER           NaN      8030.0            1.0             NaN        NONE             NaN                0.0   78.0  162.9  29.393577   \n",
       "7   8.0       1.0      1.0       6.0  31510.587866        NaN           NaN         NaN            NaN             NaN         NaN             NaN                NaN   40.7  162.0  15.508307   \n",
       "8   9.0       1.0      2.0       9.0   7575.870247        NaN           NaN         NaN            NaN             NaN         NaN             NaN                NaN   45.5  156.9  18.482704   \n",
       "9  10.0       1.0      1.0       7.0  22445.808572    CURRENT           1.0         NaN            0.0        0.197260    MODERATE             NaN                0.0  111.8  190.1  30.936955   \n",
       "\n",
       "  BMI_CLAS  DIABETES  HTN  HIGH_CHOL  CVD  CANCER         SBP        DBP  TCHOL    HDL    LDL     TG  DMDHHSIZ  \n",
       "0    UNDER         0    0          0    0       0   91.333333  56.000000  131.0   59.0   54.0   99.0       3.0  \n",
       "1   NORMAL         0    0          0    0       1  100.666667  56.666667  215.0   54.0  136.0  128.0       1.0  \n",
       "2    UNDER         0    0          0    0       0  108.666667  62.000000  129.0   30.0   58.0  202.0       4.0  \n",
       "3     <NA>         0    0          1    0       0   95.333333  61.333333  211.0   43.0  161.0   37.0       7.0  \n",
       "4     OVER         0    1          1    0       0  122.000000  82.666667  279.0   42.0  168.0  347.0       3.0  \n",
       "5   NORMAL         0    0          0    0       0  114.666667  68.000000  153.0   61.0   59.0  181.0       2.0  \n",
       "6     OVER         0    0          1    0       0  125.333333  80.000000  245.0  105.0  127.0   62.0       1.0  \n",
       "7    UNDER         0    0          0    0       0  100.666667  49.333333  162.0   67.0   88.0   33.0       7.0  \n",
       "8    UNDER         0    0          0    0       0  109.333333  53.333333  148.0   58.0   79.0   56.0       4.0  \n",
       "9    OBESE         0    1          0    0       0  145.333333  96.000000  140.0   51.0   80.0   45.0       1.0  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "core.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "37cd9236-c057-4048-a153-517cbc050d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ SMK → /Users/dengshuyue/Desktop/SDOH/analysis/output/cov_smk_1999_2023.parquet (using smk_9918.csv); missing: {'SMK_STATUS': 0.001, 'CIGS_PER_DAY': 0.794, 'PACK_YEARS': 0.78}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEQN</th>\n",
       "      <th>SMK_STATUS</th>\n",
       "      <th>CIGS_PER_DAY</th>\n",
       "      <th>PACK_YEARS</th>\n",
       "      <th>FORMER_SMOKER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>NEVER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>FORMER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.0</td>\n",
       "      <td>FORMER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8030.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0</td>\n",
       "      <td>CURRENT</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.0</td>\n",
       "      <td>NEVER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SEQN SMK_STATUS  CIGS_PER_DAY  PACK_YEARS  FORMER_SMOKER\n",
       "0   2.0      NEVER           NaN         NaN              0\n",
       "1   5.0     FORMER           NaN         NaN              1\n",
       "2   7.0     FORMER           NaN      8030.0              1\n",
       "3  10.0    CURRENT           1.0         NaN              0\n",
       "4  12.0      NEVER           NaN         NaN              0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only (re)build smoking\n",
    "smk = build_smk(CONFIG)          # writes cov_smk_1999_2023.parquet\n",
    "smk.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e37b16-a0f2-4e2a-b605-770cff407320",
   "metadata": {},
   "source": [
    "<h2> Merge Mortality </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "062801bf-cbe5-45cf-900c-787162c4914a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /Users/dengshuyue/Desktop/SDOH/analysis/output/core_mort_1999_2018.parquet (59064, 32)\n",
      "Age non-missing: 1.0\n",
      "Events (%): 15.66\n",
      "TIME_Y median (y): 9.416666666666666\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "BASE = Path(\"/Users/dengshuyue/Desktop/SDOH/analysis\")\n",
    "DATA = BASE / \"data\" / \"cov\"\n",
    "OUT  = BASE / \"output\"\n",
    "\n",
    "# 1) Load\n",
    "core = pd.read_parquet(OUT / \"cov_core_1999_2023.parquet\")\n",
    "mort_with_demo = pd.read_parquet(OUT / \"mort_with_demo.parquet\")  # built earlier\n",
    "demo_age = pd.read_parquet(DATA / \"demo9923.parquet\")[[\"SEQN\", \"RIDAGEYR\"]].drop_duplicates(\"SEQN\")\n",
    "\n",
    "# 2) Merge mortality fields you need\n",
    "need_from_mort = [\"SEQN\", \"TIME_Y\", \"EVENT\"]\n",
    "missing = [c for c in need_from_mort if c not in mort_with_demo.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"mort_with_demo is missing columns: {missing}\")\n",
    "\n",
    "core_mort = core.merge(mort_with_demo[need_from_mort], on=\"SEQN\", how=\"inner\")\n",
    "\n",
    "# 3) Add age (RIDAGEYR) from DEMO\n",
    "core_mort = core_mort.merge(demo_age, on=\"SEQN\", how=\"left\")\n",
    "\n",
    "# 4) Keep mortality-linked cycles using SDDSRVYR from *core*\n",
    "core_mort_9918 = core_mort[core_mort[\"SDDSRVYR\"].between(1, 10)].copy()\n",
    "\n",
    "# 5) Save\n",
    "out_path = OUT / \"core_mort_1999_2018.parquet\"\n",
    "core_mort_9918.to_parquet(out_path, index=False)\n",
    "\n",
    "print(\"Saved:\", out_path, core_mort_9918.shape)\n",
    "print(\"Age non-missing:\", core_mort_9918[\"RIDAGEYR\"].notna().mean().round(3))\n",
    "print(\"Events (%):\", round(100 * core_mort_9918[\"EVENT\"].mean(), 2))\n",
    "print(\"TIME_Y median (y):\", core_mort_9918[\"TIME_Y\"].median())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "886fe36a-80f9-45a4-afa2-24ca16408f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEQN</th>\n",
       "      <th>SDDSRVYR</th>\n",
       "      <th>SDMVPSU</th>\n",
       "      <th>SDMVSTRA</th>\n",
       "      <th>WTMEC2YR</th>\n",
       "      <th>SMK_STATUS</th>\n",
       "      <th>CIGS_PER_DAY</th>\n",
       "      <th>PACK_YEARS</th>\n",
       "      <th>FORMER_SMOKER</th>\n",
       "      <th>DRINKS_PER_DAY</th>\n",
       "      <th>ALCOHOL_CAT</th>\n",
       "      <th>LTPA_MET_HR_WK</th>\n",
       "      <th>LTPA_IMPUTED_FLAG</th>\n",
       "      <th>BMXWT</th>\n",
       "      <th>BMXHT</th>\n",
       "      <th>BMI</th>\n",
       "      <th>BMI_CLAS</th>\n",
       "      <th>DIABETES</th>\n",
       "      <th>HTN</th>\n",
       "      <th>HIGH_CHOL</th>\n",
       "      <th>CVD</th>\n",
       "      <th>CANCER</th>\n",
       "      <th>SBP</th>\n",
       "      <th>DBP</th>\n",
       "      <th>TCHOL</th>\n",
       "      <th>HDL</th>\n",
       "      <th>LDL</th>\n",
       "      <th>TG</th>\n",
       "      <th>DMDHHSIZ</th>\n",
       "      <th>TIME_Y</th>\n",
       "      <th>EVENT</th>\n",
       "      <th>RIDAGEYR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28325.384898</td>\n",
       "      <td>NEVER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.789041</td>\n",
       "      <td>MODERATE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75.4</td>\n",
       "      <td>174.0</td>\n",
       "      <td>24.904215</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>100.666667</td>\n",
       "      <td>56.666667</td>\n",
       "      <td>215.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.750000</td>\n",
       "      <td>1</td>\n",
       "      <td>77.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>99445.065735</td>\n",
       "      <td>FORMER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>HEAVY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>92.5</td>\n",
       "      <td>178.3</td>\n",
       "      <td>29.096386</td>\n",
       "      <td>OVER</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>82.666667</td>\n",
       "      <td>279.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>347.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>20.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>39656.600444</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>59.2</td>\n",
       "      <td>162.0</td>\n",
       "      <td>22.557537</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>114.666667</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>153.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20.416667</td>\n",
       "      <td>0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>25525.423409</td>\n",
       "      <td>FORMER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8030.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>162.9</td>\n",
       "      <td>29.393577</td>\n",
       "      <td>OVER</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>125.333333</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>245.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.666667</td>\n",
       "      <td>0</td>\n",
       "      <td>59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>22445.808572</td>\n",
       "      <td>CURRENT</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.197260</td>\n",
       "      <td>MODERATE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>111.8</td>\n",
       "      <td>190.1</td>\n",
       "      <td>30.936955</td>\n",
       "      <td>OBESE</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>145.333333</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>140.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.250000</td>\n",
       "      <td>1</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>95494.214052</td>\n",
       "      <td>NEVER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.857129</td>\n",
       "      <td>MODERATE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.2</td>\n",
       "      <td>180.0</td>\n",
       "      <td>30.617284</td>\n",
       "      <td>OBESE</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>176.666667</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>156.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>19.666667</td>\n",
       "      <td>0</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1843.950828</td>\n",
       "      <td>FORMER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1095.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>HEAVY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63.6</td>\n",
       "      <td>157.7</td>\n",
       "      <td>25.573710</td>\n",
       "      <td>OVER</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>133.333333</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>314.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>1</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>19486.733790</td>\n",
       "      <td>CURRENT</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>MODERATE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75.5</td>\n",
       "      <td>166.2</td>\n",
       "      <td>27.332850</td>\n",
       "      <td>OVER</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>59.333333</td>\n",
       "      <td>174.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.333333</td>\n",
       "      <td>1</td>\n",
       "      <td>81.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>114519.177908</td>\n",
       "      <td>CURRENT</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>HEAVY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>81.6</td>\n",
       "      <td>174.9</td>\n",
       "      <td>26.675375</td>\n",
       "      <td>OVER</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>68.666667</td>\n",
       "      <td>199.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>19.250000</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>12565.995924</td>\n",
       "      <td>NEVER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.5</td>\n",
       "      <td>144.2</td>\n",
       "      <td>19.958026</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>147.333333</td>\n",
       "      <td>60.666667</td>\n",
       "      <td>164.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.166667</td>\n",
       "      <td>1</td>\n",
       "      <td>85.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SEQN  SDDSRVYR  SDMVPSU  SDMVSTRA       WTMEC2YR SMK_STATUS  CIGS_PER_DAY  PACK_YEARS  FORMER_SMOKER  DRINKS_PER_DAY ALCOHOL_CAT  LTPA_MET_HR_WK  LTPA_IMPUTED_FLAG  BMXWT  BMXHT        BMI  \\\n",
       "0   2.0       1.0      3.0       1.0   28325.384898      NEVER           NaN         NaN            0.0        0.789041    MODERATE             NaN                0.0   75.4  174.0  24.904215   \n",
       "1   5.0       1.0      2.0       8.0   99445.065735     FORMER           NaN         NaN            1.0       12.000000       HEAVY             NaN                0.0   92.5  178.3  29.096386   \n",
       "2   6.0       1.0      2.0       2.0   39656.600444        NaN           NaN         NaN            NaN             NaN         NaN             NaN                NaN   59.2  162.0  22.557537   \n",
       "3   7.0       1.0      2.0       4.0   25525.423409     FORMER           NaN      8030.0            1.0             NaN        NONE             NaN                0.0   78.0  162.9  29.393577   \n",
       "4  10.0       1.0      1.0       7.0   22445.808572    CURRENT           1.0         NaN            0.0        0.197260    MODERATE             NaN                0.0  111.8  190.1  30.936955   \n",
       "5  12.0       1.0      2.0       6.0   95494.214052      NEVER           NaN         NaN            0.0        0.857129    MODERATE             NaN                0.0   99.2  180.0  30.617284   \n",
       "6  13.0       1.0      2.0      13.0    1843.950828     FORMER           NaN      1095.0            1.0        2.000000       HEAVY             NaN                0.0   63.6  157.7  25.573710   \n",
       "7  14.0       1.0      1.0      12.0   19486.733790    CURRENT           1.0         NaN            0.0        1.000000    MODERATE             NaN                0.0   75.5  166.2  27.332850   \n",
       "8  15.0       1.0      2.0      11.0  114519.177908    CURRENT           2.0         NaN            0.0        8.000000       HEAVY             NaN                0.0   81.6  174.9  26.675375   \n",
       "9  16.0       1.0      1.0      11.0   12565.995924      NEVER           NaN         NaN            0.0             NaN        NONE             NaN                0.0   41.5  144.2  19.958026   \n",
       "\n",
       "  BMI_CLAS  DIABETES  HTN  HIGH_CHOL  CVD  CANCER         SBP         DBP  TCHOL    HDL    LDL     TG  DMDHHSIZ     TIME_Y  EVENT  RIDAGEYR  \n",
       "0   NORMAL         0    0          0    0       1  100.666667   56.666667  215.0   54.0  136.0  128.0       1.0  14.750000      1      77.0  \n",
       "1     OVER         0    1          1    0       0  122.000000   82.666667  279.0   42.0  168.0  347.0       3.0  20.333333      0      49.0  \n",
       "2   NORMAL         0    0          0    0       0  114.666667   68.000000  153.0   61.0   59.0  181.0       2.0  20.416667      0      19.0  \n",
       "3     OVER         0    0          1    0       0  125.333333   80.000000  245.0  105.0  127.0   62.0       1.0  19.666667      0      59.0  \n",
       "4    OBESE         0    1          0    0       0  145.333333   96.000000  140.0   51.0   80.0   45.0       1.0  19.250000      1      43.0  \n",
       "5    OBESE         0    1          0    0       0  176.666667  102.000000  156.0   38.0   89.0  146.0       4.0  19.666667      0      37.0  \n",
       "6     OVER         1    1          1    0       0  133.333333   70.000000  314.0   49.0  260.0   16.0       2.0   1.333333      1      70.0  \n",
       "7     OVER         0    0          0    0       0  138.000000   59.333333  174.0   40.0  123.0   49.0       1.0  11.333333      1      81.0  \n",
       "8     OVER         0    0          0    0       0  108.000000   68.666667  199.0   58.0  131.0   54.0       2.0  19.250000      0      38.0  \n",
       "9   NORMAL         0    1          0    0       0  147.333333   60.666667  164.0   55.0   73.0  187.0       1.0   5.166667      1      85.0  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "core_mort_9918.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c951427e-bb99-48f7-ac72-b8818b4be732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "core_mort_9918[\"RIDAGEYR\"].describe()\n",
    "core_mort_9918[\"RIDAGEYR\"].isna().mean().round(3)  # missing rate\n",
    "core_mort_9918.query(\"RIDAGEYR < 0 or RIDAGEYR > 120\").shape  # out-of-range?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dfbf538a-1ce6-42bb-804f-5cfcbe7ce99f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    59064.000000\n",
       "mean        47.734119\n",
       "std         19.504749\n",
       "min         18.000000\n",
       "25%         31.000000\n",
       "50%         47.000000\n",
       "75%         64.000000\n",
       "max         85.000000\n",
       "Name: RIDAGEYR, dtype: float64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "core_mort_9918[\"RIDAGEYR\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6e9a77-0172-449f-8cb6-d94ce5fb1917",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "437cffe5-9c2b-4bbf-a4e4-e530e0fe9bb2",
   "metadata": {},
   "source": [
    "<h2>Sanity Checks </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a9688bb-e27b-4b99-8c18-5e5c52e4185d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "n_rows                     128809.000000\n",
       "n_unique_seqn              128809.000000\n",
       "missing_bmi_pct                 0.318378\n",
       "missing_alcohol_cat_pct         0.465402\n",
       "missing_smk_status_pct          0.572903\n",
       "has_weights                     1.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity checks\n",
    "quick_checks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d5279902-ca98-425a-9d67-aceaa3031f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SMK_STATUS %NA</th>\n",
       "      <th>LTPA_MET_HR_WK %NA</th>\n",
       "      <th>BMI %NA</th>\n",
       "      <th>WTMEC2YR %NA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1999–2018</th>\n",
       "      <td>45.7</td>\n",
       "      <td>100.0</td>\n",
       "      <td>13.3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019–2023</th>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>56.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           SMK_STATUS %NA  LTPA_MET_HR_WK %NA  BMI %NA  WTMEC2YR %NA\n",
       "1999–2018            45.7               100.0     13.3           0.0\n",
       "2019–2023           100.0               100.0    100.0          56.6"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Missingness by era table\n",
    "missingness_by_era()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2c2c7c83-8862-4e32-8647-51f822553018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.6815517549239571)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) Basic peek\n",
    "core = pd.read_parquet(CONFIG.out_dir / CONFIG.cov_core)\n",
    "core.head(3)\n",
    "\n",
    "# 2) Unique participants and NA overview\n",
    "core[\"SEQN\"].nunique(), core.isna().mean().round(3).sort_values().head(10)\n",
    "\n",
    "# 3) Alcohol category distribution\n",
    "core[\"ALCOHOL_CAT\"].value_counts(dropna=False)\n",
    "\n",
    "# 4) Smoking status distribution\n",
    "core[\"SMK_STATUS\"].value_counts(dropna=False)\n",
    "\n",
    "# 5) Survey design fields present?\n",
    "set([\"SDDSRVYR\",\"SDMVPSU\",\"SDMVSTRA\",\"WTMEC2YR\"]).issubset(core.columns)\n",
    "\n",
    "# 6) BMI sanity (no negatives and in a plausible range)\n",
    "core[\"BMI\"].describe()\n",
    "((core[\"BMI\"] >= 10) & (core[\"BMI\"] <= 80)).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "70d8bce1-ed42-4c4b-8fe4-7ea07ba8d600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ALCOHOL_CAT\n",
       "NaN         59948\n",
       "NONE        42792\n",
       "MODERATE    14843\n",
       "HEAVY       11226\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3) Alcohol category distribution\n",
    "core[\"ALCOHOL_CAT\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c2e7691b-dbae-41be-9807-3145e1977c87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SMK_STATUS\n",
       "NaN        0.573\n",
       "NEVER      0.233\n",
       "FORMER     0.106\n",
       "CURRENT    0.089\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distribution checks\n",
    "core[\"ALCOHOL_CAT\"].value_counts(dropna=False, normalize=True).round(3)\n",
    "core[\"SMK_STATUS\"].value_counts(dropna=False, normalize=True).round(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3b8191bc-afd0-4c27-a64f-a412e2cdee77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7804f8ef-d3e8-4b3f-9ca3-3e22270c5028",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
