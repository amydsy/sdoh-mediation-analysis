{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23e4e0a1",
   "metadata": {},
   "source": [
    "<h1> 00 — Bootstrap (paths, cycles, helpers)</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24edfeb",
   "metadata": {},
   "source": [
    "<h2>Shared environment and helper functions used across notebooks.</h2>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88aa7d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap loaded.\n",
      "ROOT: /Users/dengshuyue/Desktop/SDOH/analysis\n",
      "Data dir exists: True\n",
      "Output dir exists: True\n",
      "Mortality cycles: ['1999-2000', '2001-2002', '2003-2004', '2005-2006', '2007-2008', '2009-2010', '2011-2012', '2013-2014', '2015-2016', '2017-2018']\n",
      "Non-mortality cycles: ['2017-March 2020 (pre-pandemic)', 'August 2021–August 2023']\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os, sys, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------------\n",
    "# Root & existing folders (NO mkdir here)\n",
    "# -------------------------\n",
    "ROOT = Path(os.environ.get(\"SDOH_ROOT\", \"/Users/dengshuyue/Desktop/SDOH/analysis\"))\n",
    "\n",
    "CODE   = ROOT / \"code\"\n",
    "DATA   = ROOT / \"data\"\n",
    "OUT    = ROOT / \"output\"\n",
    "\n",
    "# Data subfolders that you already have\n",
    "NH_DEIT      = DATA / \"nhanes_deit\"\n",
    "NH_BY_MOD    = DATA / \"nhanes_by_module\"\n",
    "FPED_DIR     = DATA / \"fped\"\n",
    "FNDDDS_DIR   = DATA / \"fndds\"\n",
    "BPQ_DIR      = DATA / \"bpq\"\n",
    "HEALTH_ACC   = DATA / \"health_access\"\n",
    "HH_SIZE_DIR  = DATA / \"household_size\"\n",
    "TMP_NORM_XPT = DATA / \"tmp_norm_xpt\"\n",
    "LESS_IMP     = DATA / \"less_important\"\n",
    "\n",
    "# Common files already present\n",
    "FILES = {\n",
    "    \"demoall_csv\":          DATA / \"demoall.csv\",\n",
    "    \"demoall_pkl\":          DATA / \"demoall.pkl\",\n",
    "    \"hei9918_sas7bdat\":     DATA / \"hei9918.sas7bdat\",\n",
    "    \"sodh_diet_mort_sas\":   DATA / \"sodh_diet_mort.sas7bdat\",\n",
    "    \"sodh_diet_mort_pkl\":   DATA / \"SODH_diet_mort.pkl\",\n",
    "    # multiple CSV variants exist; we’ll glob when needed\n",
    "}\n",
    "\n",
    "# Output files/folders (already exist in your tree)\n",
    "OUT_FILES = {\n",
    "    \"demo_summary_csv\":     OUT / \"demo_summary.csv\",\n",
    "    \"demo_summary_r_csv\":   OUT / \"demo_summary_r.csv\",\n",
    "    \"ahei_combined_csv\":    DATA / \"ahei_combined.csv\",  # lives under data/\n",
    "}\n",
    "TABLES_FIGS = OUT  # you keep tables directly in output/\n",
    "\n",
    "# -------------------------\n",
    "# NHANES cycles\n",
    "# - Keep it simple: Cox is in R; use explicit lists\n",
    "# -------------------------\n",
    "CYCLES_MORTALITY = [\n",
    "    \"1999-2000\",\"2001-2002\",\"2003-2004\",\"2005-2006\",\"2007-2008\",\n",
    "    \"2009-2010\",\"2011-2012\",\"2013-2014\",\"2015-2016\",\"2017-2018\",\n",
    "]\n",
    "CYCLES_NONMORT = [\n",
    "    \"2017-March 2020 (pre-pandemic)\",  # P_DEMO.xpt style\n",
    "    \"August 2021–August 2023\",         # DEMO_L.xpt\n",
    "]\n",
    "CYCLES_ALL = CYCLES_MORTALITY + CYCLES_NONMORT\n",
    "\n",
    "# Optional suffix/prefix hints (use only if you need to load DEMO files by pattern)\n",
    "CYCLE_SUFFIX = {\n",
    "    \"1999-2000\": \"\",\n",
    "    \"2001-2002\": \"_B\", \"2003-2004\": \"_C\", \"2005-2006\": \"_D\",\n",
    "    \"2007-2008\": \"_E\", \"2009-2010\": \"_F\", \"2011-2012\": \"_G\",\n",
    "    \"2013-2014\": \"_H\", \"2015-2016\": \"_I\", \"2017-2018\": \"_J\",\n",
    "    \"2017-March 2020 (pre-pandemic)\": \"P_\",  # e.g., P_DEMO.xpt\n",
    "    \"August 2021–August 2023\": \"_L\",         # e.g., DEMO_L.xpt\n",
    "}\n",
    "def cycle_suffix(label: str) -> str:\n",
    "    return CYCLE_SUFFIX.get(label, \"\")\n",
    "\n",
    "# -------------------------\n",
    "# Small helpers\n",
    "# -------------------------\n",
    "def z(x):\n",
    "    x = pd.Series(x, dtype=\"float64\")\n",
    "    return (x - x.mean(skipna=True)) / x.std(skipna=True)\n",
    "\n",
    "def combine_wtmec(w, n_cycles: int):\n",
    "    \"\"\"\n",
    "    When stacking cycles, divide 2-year MEC weights by the number of\n",
    "    2-year cycles actually included in the stack you’re building.\n",
    "    \"\"\"\n",
    "    return w / float(n_cycles)\n",
    "\n",
    "def list_existing(paths):\n",
    "    \"\"\"Quickly check which paths exist (debug helper).\"\"\"\n",
    "    return {k: (p if p.exists() else None) for k, p in paths.items()}\n",
    "\n",
    "# -------------------------\n",
    "# Display prefs\n",
    "# -------------------------\n",
    "pd.options.display.max_rows = 60\n",
    "pd.options.display.max_columns = 120\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"Bootstrap loaded.\")\n",
    "print(\"ROOT:\", ROOT)\n",
    "print(\"Data dir exists:\", DATA.exists())\n",
    "print(\"Output dir exists:\", OUT.exists())\n",
    "print(\"Mortality cycles:\", CYCLES_MORTALITY)\n",
    "print(\"Non-mortality cycles:\", CYCLES_NONMORT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b53c4c7-ff92-45cb-b330-d34c7102b204",
   "metadata": {},
   "source": [
    "<h2>preview sas code and data </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68500809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidates: [PosixPath('/Users/dengshuyue/Desktop/SDOH/analysis/data/household_size/DEMO_I.xpt'), PosixPath('/Users/dengshuyue/Desktop/SDOH/analysis/data/nhanes_deit/DEMO_I.xpt')]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEQN</th>\n",
       "      <th>SDDSRVYR</th>\n",
       "      <th>RIDSTATR</th>\n",
       "      <th>RIAGENDR</th>\n",
       "      <th>RIDAGEYR</th>\n",
       "      <th>RIDAGEMN</th>\n",
       "      <th>RIDRETH1</th>\n",
       "      <th>RIDRETH3</th>\n",
       "      <th>RIDEXMON</th>\n",
       "      <th>RIDEXAGM</th>\n",
       "      <th>DMQMILIZ</th>\n",
       "      <th>DMQADFC</th>\n",
       "      <th>DMDBORN4</th>\n",
       "      <th>DMDCITZN</th>\n",
       "      <th>DMDYRSUS</th>\n",
       "      <th>DMDEDUC3</th>\n",
       "      <th>DMDEDUC2</th>\n",
       "      <th>DMDMARTL</th>\n",
       "      <th>RIDEXPRG</th>\n",
       "      <th>SIALANG</th>\n",
       "      <th>SIAPROXY</th>\n",
       "      <th>SIAINTRP</th>\n",
       "      <th>FIALANG</th>\n",
       "      <th>FIAPROXY</th>\n",
       "      <th>FIAINTRP</th>\n",
       "      <th>MIALANG</th>\n",
       "      <th>MIAPROXY</th>\n",
       "      <th>MIAINTRP</th>\n",
       "      <th>AIALANGA</th>\n",
       "      <th>DMDHHSIZ</th>\n",
       "      <th>DMDFMSIZ</th>\n",
       "      <th>DMDHHSZA</th>\n",
       "      <th>DMDHHSZB</th>\n",
       "      <th>DMDHHSZE</th>\n",
       "      <th>DMDHRGND</th>\n",
       "      <th>DMDHRAGE</th>\n",
       "      <th>DMDHRBR4</th>\n",
       "      <th>DMDHREDU</th>\n",
       "      <th>DMDHRMAR</th>\n",
       "      <th>DMDHSEDU</th>\n",
       "      <th>WTINT2YR</th>\n",
       "      <th>WTMEC2YR</th>\n",
       "      <th>SDMVPSU</th>\n",
       "      <th>SDMVSTRA</th>\n",
       "      <th>INDHHIN2</th>\n",
       "      <th>INDFMIN2</th>\n",
       "      <th>INDFMPIR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83732.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.397605e-79</td>\n",
       "      <td>5.397605e-79</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>134671.370419</td>\n",
       "      <td>135629.507405</td>\n",
       "      <td>1.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>83733.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.397605e-79</td>\n",
       "      <td>5.397605e-79</td>\n",
       "      <td>5.397605e-79</td>\n",
       "      <td>1.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24328.560239</td>\n",
       "      <td>25282.425927</td>\n",
       "      <td>1.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>83734.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.397605e-79</td>\n",
       "      <td>5.397605e-79</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12400.008522</td>\n",
       "      <td>12575.838818</td>\n",
       "      <td>1.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>83735.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.397605e-79</td>\n",
       "      <td>5.397605e-79</td>\n",
       "      <td>5.397605e-79</td>\n",
       "      <td>2.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>102717.995647</td>\n",
       "      <td>102078.634508</td>\n",
       "      <td>1.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>83736.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.397605e-79</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>5.397605e-79</td>\n",
       "      <td>2.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17627.674984</td>\n",
       "      <td>18234.736219</td>\n",
       "      <td>2.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      SEQN  SDDSRVYR  RIDSTATR  RIAGENDR  RIDAGEYR  RIDAGEMN  RIDRETH1  \\\n",
       "0  83732.0       9.0       2.0       1.0      62.0       NaN       3.0   \n",
       "1  83733.0       9.0       2.0       1.0      53.0       NaN       3.0   \n",
       "2  83734.0       9.0       2.0       1.0      78.0       NaN       3.0   \n",
       "3  83735.0       9.0       2.0       2.0      56.0       NaN       3.0   \n",
       "4  83736.0       9.0       2.0       2.0      42.0       NaN       4.0   \n",
       "\n",
       "   RIDRETH3  RIDEXMON  RIDEXAGM  DMQMILIZ  DMQADFC  DMDBORN4  DMDCITZN  \\\n",
       "0       3.0       1.0       NaN       2.0      NaN       1.0       1.0   \n",
       "1       3.0       1.0       NaN       2.0      NaN       2.0       2.0   \n",
       "2       3.0       2.0       NaN       1.0      2.0       1.0       1.0   \n",
       "3       3.0       2.0       NaN       2.0      NaN       1.0       1.0   \n",
       "4       4.0       2.0       NaN       2.0      NaN       1.0       1.0   \n",
       "\n",
       "   DMDYRSUS  DMDEDUC3  DMDEDUC2  DMDMARTL  RIDEXPRG  SIALANG  SIAPROXY  \\\n",
       "0       NaN       NaN       5.0       1.0       NaN      1.0       2.0   \n",
       "1       7.0       NaN       3.0       3.0       NaN      1.0       2.0   \n",
       "2       NaN       NaN       3.0       1.0       NaN      1.0       2.0   \n",
       "3       NaN       NaN       5.0       6.0       NaN      1.0       2.0   \n",
       "4       NaN       NaN       4.0       3.0       1.0      1.0       2.0   \n",
       "\n",
       "   SIAINTRP  FIALANG  FIAPROXY  FIAINTRP  MIALANG  MIAPROXY  MIAINTRP  \\\n",
       "0       2.0      1.0       2.0       2.0      1.0       2.0       2.0   \n",
       "1       2.0      1.0       2.0       2.0      1.0       2.0       2.0   \n",
       "2       2.0      1.0       2.0       2.0      1.0       2.0       2.0   \n",
       "3       2.0      1.0       2.0       2.0      1.0       2.0       2.0   \n",
       "4       2.0      1.0       2.0       2.0      1.0       2.0       2.0   \n",
       "\n",
       "   AIALANGA  DMDHHSIZ  DMDFMSIZ      DMDHHSZA      DMDHHSZB      DMDHHSZE  \\\n",
       "0       1.0       2.0       2.0  5.397605e-79  5.397605e-79  1.000000e+00   \n",
       "1       1.0       1.0       1.0  5.397605e-79  5.397605e-79  5.397605e-79   \n",
       "2       NaN       2.0       2.0  5.397605e-79  5.397605e-79  2.000000e+00   \n",
       "3       1.0       1.0       1.0  5.397605e-79  5.397605e-79  5.397605e-79   \n",
       "4       1.0       5.0       5.0  5.397605e-79  2.000000e+00  5.397605e-79   \n",
       "\n",
       "   DMDHRGND  DMDHRAGE  DMDHRBR4  DMDHREDU  DMDHRMAR  DMDHSEDU       WTINT2YR  \\\n",
       "0       1.0      62.0       1.0       5.0       1.0       3.0  134671.370419   \n",
       "1       1.0      53.0       2.0       3.0       3.0       NaN   24328.560239   \n",
       "2       2.0      79.0       1.0       3.0       1.0       3.0   12400.008522   \n",
       "3       2.0      56.0       1.0       5.0       6.0       NaN  102717.995647   \n",
       "4       2.0      42.0       1.0       4.0       3.0       NaN   17627.674984   \n",
       "\n",
       "        WTMEC2YR  SDMVPSU  SDMVSTRA  INDHHIN2  INDFMIN2  INDFMPIR  \n",
       "0  135629.507405      1.0     125.0      10.0      10.0      4.39  \n",
       "1   25282.425927      1.0     125.0       4.0       4.0      1.32  \n",
       "2   12575.838818      1.0     131.0       5.0       5.0      1.51  \n",
       "3  102078.634508      1.0     131.0      10.0      10.0      5.00  \n",
       "4   18234.736219      2.0     126.0       7.0       7.0      1.23  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SEQN', 'SDDSRVYR', 'RIDSTATR', 'RIAGENDR', 'RIDAGEYR', 'RIDAGEMN', 'RIDRETH1', 'RIDRETH3', 'RIDEXMON', 'RIDEXAGM', 'DMQMILIZ', 'DMQADFC', 'DMDBORN4', 'DMDCITZN', 'DMDYRSUS', 'DMDEDUC3', 'DMDEDUC2', 'DMDMARTL', 'RIDEXPRG', 'SIALANG', 'SIAPROXY', 'SIAINTRP', 'FIALANG', 'FIAPROXY', 'FIAINTRP', 'MIALANG', 'MIAPROXY', 'MIAINTRP', 'AIALANGA', 'DMDHHSIZ', 'DMDFMSIZ', 'DMDHHSZA', 'DMDHHSZB', 'DMDHHSZE', 'DMDHRGND', 'DMDHRAGE', 'DMDHRBR4', 'DMDHREDU', 'DMDHRMAR', 'DMDHSEDU', 'WTINT2YR', 'WTMEC2YR', 'SDMVPSU', 'SDMVSTRA', 'INDHHIN2', 'INDFMIN2', 'INDFMPIR']\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "BASE = Path(\"/Users/dengshuyue/Desktop/SDOH/analysis/data\")\n",
    "\n",
    "# Find DEMO for 2015–2016 (\"I\")\n",
    "cands = list(BASE.rglob(\"DEMO_I.*\"))\n",
    "print(\"Candidates:\", cands)\n",
    "\n",
    "if not cands:\n",
    "    # fallback: list any DEMO files you have\n",
    "    print(\"Any DEMO files I can see:\")\n",
    "    print(list(BASE.rglob(\"DEMO*.*\")))\n",
    "else:\n",
    "    p = cands[0]\n",
    "    if p.suffix.lower() == \".xpt\":\n",
    "        df = pd.read_sas(p, format=\"xport\")  # NHANES XPT format\n",
    "    elif p.suffix.lower() == \".sas7bdat\":\n",
    "        import pyreadstat\n",
    "        df, _ = pyreadstat.read_sas7bdat(p)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown extension: {p.suffix}\")\n",
    "\n",
    "    pd.set_option(\"display.max_columns\", None)\n",
    "    display(df.head())\n",
    "    print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef01a718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SEQN', 'DAYS', 'DR12IFDC', 'WTDRD1', 'DR12DRST', 'SDDSRVYR', 'RIAGENDR', 'RIDAGEYR', 'RIDRETH1', 'DMDEDUC3', 'DMDEDUC2', 'INDFMPIR', 'DMDHREDU', 'WTINT2YR', 'WTMEC2YR', 'SDMVPSU', 'SDMVSTRA', 'DR12IKC2', 'CYCLE', 'WTDR2D', 'DR12FS', 'DMDHREDZ', 'age', 'race', 'edu', 'pedu', 'Incm', 'incm2', 'include', 'Weight16a', 'cycles', 'sex', 'age1', 'age2', 'age3', 'race2', 'race3', 'race4', 'weekend', '_NAME_', '_LABEL_', 'DRDAY1', 'DRDAY2', 'tkal1', 'tkal2', 'Tcal', 'DR12DAY', 'kcal1', 'kcal2', 'kcal3', 'kcal4', 'kcal12', 'wt1', 'wt2', 'wt3', 'wt4', 'wt12', 'pcte1', 'pcte2', 'pcte3', 'pcte4', 'pcte12', 'pctg1', 'pctg2', 'pctg3', 'pctg4', 'pctg12', 'kcals2', 'kcals5', 'kcals6', 'kcals9', 'kcals13', 'kcals14', 'kcals17', 'kcals20', 'kcals21', 'kcals22', 'kcals23', 'kcals25', 'kcals28', 'kcals29', 'kcals33', 'kcals36', 'kcals39', 'kcals41', 'kcals3', 'kcals37', 'kcals38', 'kcals40', 'kcals42', 'kcals1', 'kcals10', 'kcals16', 'kcals24', 'kcals15', 'kcals18', 'kcals7', 'kcals8', 'kcals19', 'kcals46', 'kcals44', 'kcals45', 'kcals12', 'kcals27', 'kcals34', 'kcals30', 'kcals26', 'kcals43', 'kcals214', 'kcals215', 'kcals32', 'kcals31', 'wts2', 'wts5', 'wts6', 'wts9', 'wts13', 'wts14', 'wts17', 'wts20', 'wts21', 'wts22', 'wts23', 'wts25', 'wts28', 'wts29', 'wts33', 'wts36', 'wts39', 'wts41', 'wts3', 'wts37', 'wts38', 'wts40', 'wts42', 'wts1', 'wts10', 'wts16', 'wts24', 'wts15', 'wts18', 'wts7', 'wts8', 'wts19', 'wts46', 'wts44', 'wts45', 'wts12', 'wts27', 'wts34', 'wts30', 'wts26', 'wts43', 'wts214', 'wts215', 'wts32', 'wts31', 'pctes2', 'pctes5', 'pctes6', 'pctes9', 'pctes13', 'pctes14', 'pctes17', 'pctes20', 'pctes21', 'pctes22', 'pctes23', 'pctes25', 'pctes28', 'pctes29', 'pctes33', 'pctes36', 'pctes39', 'pctes41', 'pctes3', 'pctes37', 'pctes38', 'pctes40', 'pctes42', 'pctes1', 'pctes10', 'pctes16', 'pctes24', 'pctes15', 'pctes18', 'pctes7', 'pctes8', 'pctes19', 'pctes46', 'pctes44', 'pctes45', 'pctes12', 'pctes27', 'pctes34', 'pctes30', 'pctes26', 'pctes43', 'pctes214', 'pctes215', 'pctes32', 'pctes31', 'pctgs2', 'pctgs5', 'pctgs6', 'pctgs9', 'pctgs13', 'pctgs14', 'pctgs17', 'pctgs20', 'pctgs21', 'pctgs22', 'pctgs23', 'pctgs25', 'pctgs28', 'pctgs29', 'pctgs33', 'pctgs36', 'pctgs39', 'pctgs41', 'pctgs3', 'pctgs37', 'pctgs38', 'pctgs40', 'pctgs42', 'pctgs1', 'pctgs10', 'pctgs16', 'pctgs24', 'pctgs15', 'pctgs18', 'pctgs7', 'pctgs8', 'pctgs19', 'pctgs46', 'pctgs44', 'pctgs45', 'pctgs12', 'pctgs27', 'pctgs34', 'pctgs30', 'pctgs26', 'pctgs43', 'pctgs214', 'pctgs215', 'pctgs32', 'pctgs31', 'kcals4', 'kcals11', 'kcals35', 'kcals47', 'kcals48', 'kcals49', 'kcals50', 'kcals51', 'pctgs4', 'pctgs11', 'pctgs35', 'pctgs47', 'pctgs48', 'pctgs49', 'pctgs50', 'pctgs51', 'pctes4', 'pctes11', 'pctes35', 'pctes47', 'pctes48', 'pctes49', 'pctes50', 'pctes51', 'wts4', 'wts11', 'wts35', 'wts47', 'wts48', 'wts49', 'wts50', 'wts51', 'Pctes1a', 'Pctes6a', 'Pctgs1a', 'Pctgs6a', 'Pctgs10b', 'Pctgs112', 'Kcals1a', 'Kcals6a', 'Kcals10b', 'Kcals112', 'Kcals10a', 'Pctes10a', 'wts10a', 'Pctgs10a', 'Pctes10b', 'Pctes112', 'Kcals2a', 'Pctes2a', 'wts2a', 'Pctgs2a', 'Kcals13a', 'Pctes13a', 'wts13a', 'Pctgs13a', 'Pctes23a', 'Kcals23a', 'Pctgs23a', 'Kcals38a', 'Pctes38a', 'wts38a', 'Pctgs38a', 'Pctes25a', 'Pctgs25a', 'Kcals25a', 'Pctes24a', 'Pctgs24a', 'Kcals24a', 'Pctes30a', 'Pctes30b', 'Pctgs30a', 'Pctgs30b', 'Kcals30a', 'Kcals30b', 'Pctes37b', 'pctes44a']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Path to your .sas7bdat file\n",
    "file_path = \"/Users/dengshuyue/Desktop/SDOH/analysis/data/less_important/gg.sas7bdat\"\n",
    "\n",
    "# Load the dataset using pandas (requires pyreadstat)\n",
    "df = pd.read_sas(file_path, format=\"sas7bdat\")\n",
    "\n",
    "# Preview the data\n",
    "pd.set_option('display.max_columns', None)\n",
    "df.head()\n",
    "\n",
    "# List all column names\n",
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a1ea600",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "libname data \"C:\\Users\\lwang18\\Box\\Projects\\5_UPF_Mortality\\data\";\n",
      "*%let home=C:\\Users\\LWANG18\\Box\\Projects\\5_UPF_Mortality\\results_revision ; \n",
      "%let home= C:\\Users\\lwang18\\Box\\Projects\\Kroger project\\Data for analysis\\Scores;\n",
      " %let path= C:\\Users\\lwang18\\Box\\Projects\\Kroger project\\Data for analysis\\Scores;\n",
      "\n",
      "%let home= C:\\Users\\lwang18\\OneDrive - Tufts\\Desktop\\Projects\\Food Insecurity,;\n",
      "libname out \"C:\\Users\\lwang18\\OneDrive - Tufts\\Desktop\\Projects\\Food Insecurity,\";\n",
      "\n",
      "libname NHANES \"C:\\Users\\LWANG18\\Box\\NHANES_Lu\" ;\n",
      "\n",
      "/** main analysis **/\n",
      "\n",
      "%macro cox(data, dvar, evars, covars, death , out);\n",
      "ods select all ; \n",
      "ODS OUTPUT PARAMETERESTIMATES=r0; \n",
      "proc surveyphreg data=&data;\n",
      "\tstrata sdmvstra;\n",
      "\tcluster sdmvpsu;\n",
      "\tweight wt;\n",
      "\tclass    sex (ref=\"1\") race edu(ref=\"1\") smk  pir(ref=\"3\") SNAP(ref=\"0\") FS ins2(ref=\"1\") ins i_FCS_sdq hei2015q(ref=\"3\") marriage  hoq065/param=ref;\n",
      "\tmodel py*&death(0)= &dvar &covars /rl ties=breslow;\n",
      "run ;\n",
      "data r0 ; set r0 ; \n",
      "HRCL=compress(round(hazardratio,0.01)||\"(\"||round(HRLOWerCL,.01)||\",\" ||round(HRupperCL,0.01)||\")\")  ; \n",
      "outcome=\"&death.\" ;\n",
      "predictor=\"&dvar.\";\n",
      "explainvar=\"null\" ;\n",
      "model=\"&out.\"; \n",
      "run; \n",
      "proc append data=r0  base=&out force; run;\n",
      "\n",
      "%let i=1 ;\n",
      "%do %while(%scan(&evars,&i) ne ) ;\n",
      "%let evar=%scan(&evars,&i) ;\n",
      "\n",
      "ods select all ; \n",
      "ODS OUTPUT PARAMETERESTIMATES=r1; \n",
      "proc surveyphreg data=&data;\n",
      "\tstrata sdmvstra;\n",
      "\tcluster sdmvpsu;\n",
      "\tweight wt;\n",
      "\tclass    sex (ref=\"1\") race edu(ref=\"1\") smk  pir(ref=\"3\") SNAP(ref=\"0\") FS ins2(ref=\"1\") ins i_FCS_sdq hei2015q(ref=\"3\") marriage hoq065(ref=\"1\")/param=ref;\n",
      "\tmodel py*&death(0)= &dvar &evar &covars /rl ties=breslow;\n",
      "run ;\n",
      "\n",
      "data resc&death.&i ; \n",
      "set r1; \n",
      "HRCL=compress(round(hazardratio,0.01)||\"(\"||round(HRLOWerCL,.01)||\",\" ||round(HRupperCL,0.01)||\")\")  ; \n",
      "outcome=\"&death.\" ;\n",
      "predictor=\"&dvar.\";\n",
      "explainvar=\"&evar.\" ;\n",
      "model=\"&out.\"; \n",
      "run; \n",
      "proc append data=resc&death.&i  base=&out force; run;\n",
      " \n",
      "%let i=%eval(&i+1) ;\n",
      "%end ;\n",
      "%mend ; \n",
      "\n",
      " \n",
      "%macro cox1(data, dvar, evars, covars, death , out, label);\n",
      "OPTION SPOOL ; \n",
      "ods select all ; \n",
      "ODS OUTPUT PARAMETERESTIMATES=r1; \n",
      "proc surveyphreg data=&data;\n",
      "\tstrata sdmvstra;\n",
      "\tcluster sdmvpsu;\n",
      "\tweight wt;\n",
      "\tclass    sex (ref=\"1\") race edu(ref=\"1\") smk  pir(ref=\"3\") SNAP(ref=\"0\") FS ins ins2(ref=\"1\") i_FCS_sdq hei2015q(ref=\"3\") /param=ref;\n",
      "\tmodel py*&death(0)= &dvar &evars &covars /rl ties=breslow;\n",
      "run ;\n",
      "\n",
      "data resc&death ; \n",
      "set r1; \n",
      "HRCL=compress(round(hazardratio,0.01)||\"(\"||round(HRLOWerCL,.01)||\",\" ||round(HRupperCL,0.01)||\")\")  ; \n",
      "outcome=\"&death.\" ;\n",
      "predictor=\"&dvar.\";\n",
      "explainvar=&label ; \n",
      "model=\"&out.\"; \n",
      "run; \n",
      "proc append data=resc&death  base=&out force; run;\n",
      "%mend ; \n",
      "\n",
      "*%cox(score_mort1,pir, fs  i_FCS_sdq SNAP ins ins2, ridageyr sex race   , mortstat, out_model0 );\n",
      "%cox(score_mort,pir,  hei2015q i_FCS_sdq fs SNAP ins ins2 marriage hoq065 unemployment unemployment2  ,  ridageyr sex race  edu , mortstat, out_model1 );\n",
      "%cox1(score_mort,pir, i_FCS_sdq fs , ridageyr sex race  edu , mortstat, out_model1 , \"comb1\");\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq SNAP ins, ridageyr sex race  edu , mortstat, out_model1 , \"all1\");\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq SNAP ins2, ridageyr sex race  edu , mortstat, out_model1 , \"all2\");\n",
      "\n",
      "/**age, sex, race, edu, ins as the base model*/\n",
      "%cox(score_mort,pir, hei2015q i_FCS_sdq fs  SNAP marriage hoq065 unemployment unemployment2,  ridageyr sex race  edu ins , mortstat, out_model11 );  \n",
      "%cox1(score_mort,pir, fs i_FCS_sdq, ridageyr sex race  edu ins, mortstat, out_model11 , \"comb1\");\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq SNAP ins, ridageyr sex race  edu ins, mortstat, out_model11 , \"all1\");\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq SNAP ins2, ridageyr sex race  edu ins, mortstat, out_model11 , \"all2\");\n",
      "\n",
      "/**age, sex, race, edu, lifestyles as the base model*/\n",
      "%cox(score_mort,pir, hei2015q i_FCS_sdq fs  SNAP ins ins2 marriage hoq065 unemployment unemployment2, ridageyr sex race  edu smk\tmet_hr perE_alco , mortstat, out_model2 );\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq, ridageyr sex race  edu smk\tmet_hr perE_alco, mortstat, out_model2 , \"comb1\");\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq SNAP ins, ridageyr sex race  edu smk\tmet_hr perE_alco, mortstat, out_model2 , \"all1\");\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq SNAP ins2, ridageyr sex race  edu smk\tmet_hr perE_alco, mortstat, out_model2 , \"all2\");\n",
      "\n",
      "/**age, sex, race, edu, lifestyles, insurance as the base model*/\n",
      "%cox(score_mort,pir, hei2015q i_FCS_sdq fs  SNAP marriage hoq065 unemployment unemployment2, ridageyr sex race  edu smk\tmet_hr perE_alco ins , mortstat, out_model21 );\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq, ridageyr sex race  edu smk\tmet_hr perE_alco ins, mortstat, out_model21 , \"comb1\");\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq SNAP ins, ridageyr sex race  edu smk\tmet_hr perE_alco ins, mortstat, out_model21 , \"all1\");\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq SNAP ins2, ridageyr sex race  edu smk\tmet_hr perE_alco ins, mortstat, out_model21 , \"all2\");\n",
      "\n",
      "\n",
      "/**age, sex, race, edu, lifestyles, baseline health as the base model*/\n",
      "%cox(score_mort,pir, hei2015q i_FCS_sdq fs  SNAP ins ins2 marriage hoq065 unemployment unemployment2, ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl , mortstat, out_model3 );\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq, ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl , mortstat, out_model3 , \"comb1\");\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq SNAP ins, ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl , mortstat, out_model3 , \"all1\");\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq SNAP ins2, ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl , mortstat, out_model3 , \"all2\");\n",
      "\n",
      "\n",
      "/**age, sex, race, edu, lifestyles, baseline health, insurance, as the base model*/\n",
      "%cox(score_mort,pir, hei2015q i_FCS_sdq fs  SNAP marriage hoq065 unemployment unemployment2, ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl ins, mortstat , out_model31 );\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq, ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl ins, mortstat, out_model31 , \"comb1\");\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq SNAP ins, ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl ins, mortstat, out_model31 , \"all1\");\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq SNAP ins2, ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl ins, mortstat, out_model31 , \"all2\");\n",
      "\n",
      "\n",
      "data out.out_models_pir ; set out_model1 out_model11 out_model2 out_model21 out_model3 out_model31; run; \n",
      "\n",
      "\n",
      "\n",
      "PROC expORT data= WORK.out_models_pir \n",
      "            outFILE= \"&home\\allmodelspir.xlsx\" \n",
      "            DBMS=xlsx REPLACE;\n",
      " *    GETNAMES=YES;\n",
      "RUN;\n",
      "\n",
      "/***Predictor=FS **/\n",
      "\n",
      "*%cox(score_mort1,pir, fs  i_FCS_sdq SNAP ins ins2, ridageyr sex race   , mortstat, out_model0 );\n",
      "%cox(score_mort,fs, hei2015q i_FCS_sdq pir SNAP ins ins2 marriage hoq065 unemployment unemployment2,  ridageyr sex race  edu , mortstat, out_model1a );\n",
      "\n",
      "%cox(score_mort,fs, hei2015q i_FCS_sdq, ridageyr sex race edu ins marriage unemployment2 hoq065 pir snap , mortstat, out_model11a );  \n",
      "\n",
      "%cox(score_mort,fs, hei2015q i_FCS_sdq pir SNAP ins ins2 marriage hoq065 unemployment unemployment2, ridageyr sex race edu smk met_hr perE_alco , mortstat, out_model2a );  \n",
      "\n",
      "%cox(score_mort,fs, hei2015q i_FCS_sdq, ridageyr sex race edu smk met_hr perE_alco ins marriage unemployment2 hoq065 pir snap , mortstat, out_model21a);  \n",
      "/**age, sex, race, edu, lifestyles, baseline health, insurance, as the base model*/\n",
      "\n",
      "%cox(score_mort,fs, hei2015q i_FCS_sdq pir  SNAP ins ins2 marriage hoq065 unemployment unemployment2, ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl , mortstat , out_model3a );\n",
      "\n",
      "%cox1(score_mort,fs, hei2015q pir  SNAP ins marriage hoq065 unemployment , ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl , mortstat, out_model4 , \"all1\");\n",
      "\n",
      "\n",
      "data out_models_fs ; set out_model1a out_model11a out_model2a out_model21a  out_model3a out_model4; run; \n",
      "\n",
      "\n",
      "PROC expORT data= WORK.out_models_fs \n",
      "            outFILE= \"&home\\allmodelsfs.xlsx\" \n",
      "            DBMS=xlsx REPLACE;\n",
      " *    GETNAMES=YES;\n",
      "RUN;\n",
      "\n",
      "\n",
      "%cox(score_mort, hei2015q, fs pir SNAP ins ins2 marriage hoq065 unemployment unemployment2,  ridageyr sex race  edu , mortstat, out_model1b );\n",
      "\n",
      "%cox(score_mort, hei2015q, fs pir SNAP ins ins2 marriage hoq065 unemployment unemployment2, ridageyr sex race edu smk met_hr perE_alco , mortstat, out_model2b );  \n",
      "\n",
      "%cox(score_mort, hei2015q, fs pir  SNAP ins ins2 marriage hoq065 unemployment unemployment2, ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl , mortstat , out_model3b );\n",
      "\n",
      "%cox1(score_mort, hei2015q, fs pir  SNAP ins  , ridageyr sex race  edu smk\tmet_hr perE_alco , mortstat, out_model2b , \"all1\");\n",
      "\n",
      "\n",
      "%cox1(score_mort, hei2015q, fs pir  SNAP ins marriage hoq065 unemployment , ridageyr sex race  edu smk\tmet_hr perE_alco , mortstat, out_model2b , \"all2\");\n",
      "\n",
      "%cox1(score_mort, hei2015q, fs pir  SNAP ins  , ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl , mortstat, out_model3b , \"all1\");\n",
      "\n",
      "%cox1(score_mort, hei2015q, fs pir  SNAP ins marriage hoq065 unemployment , ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl , mortstat, out_model3b , \"all2\");\n",
      "\n",
      "%cox1(score_mort, hei2015q, fs pir  SNAP ins marriage hoq065 unemployment , ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl , mortstat, out_model3b , \"all3\");\n",
      "\n",
      "\n",
      "data data.out_models_hei ; set out_model1b out_model2b out_model3b; run; \n",
      "\n",
      "\n",
      "PROC expORT data= WORK.out_models_hei \n",
      "            outFILE= \"&home\\allmodelshei.xlsx\" \n",
      "            DBMS=xlsx REPLACE;\n",
      " *    GETNAMES=YES;\n",
      "RUN;\n",
      "\n",
      "\n",
      "%cox(score_mort, SNAP, hei2015q fs pir ins ins2 marriage hoq065 unemployment unemployment2,  ridageyr sex race  edu , mortstat, out_model1c );\n",
      "\n",
      "%cox(score_mort, SNAP, hei2015q fs pir ins ins2 marriage hoq065 unemployment unemployment2, ridageyr sex race edu smk met_hr perE_alco , mortstat, out_model2c );  \n",
      "\n",
      "%cox(score_mort, SNAP, hei2015q fs pir  ins ins2 marriage hoq065 unemployment unemployment2, ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl , mortstat , out_model3c );\n",
      "\n",
      "%cox1(score_mort, SNAP, hei2015q fs pir  ins  , ridageyr sex race  edu smk\tmet_hr perE_alco , mortstat, out_model2c , \"all1\");\n",
      "\n",
      "%cox1(score_mort, SNAP, hei2015q fs pir  ins marriage hoq065 unemployment , ridageyr sex race  edu smk\tmet_hr perE_alco , mortstat, out_model2c , \"all2\");\n",
      "\n",
      "%cox1(score_mort, SNAP, hei2015q fs pir  ins  , ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl , mortstat, out_model3c , \"all1\");\n",
      "\n",
      "%cox1(score_mort, SNAP, hei2015q fs pir  ins marriage hoq065 unemployment , ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl , mortstat, out_model3c , \"all2\");\n",
      "\n",
      "%cox1(score_mort, SNAP, hei2015q fs pir  ins marriage hoq065 unemployment2 , ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl , mortstat, out_model3c , \"all3\");\n",
      "\n",
      "\n",
      "data data.out_models_SNAP ; set out_model1c out_model2c out_model3c; run; \n",
      "\n",
      "\n",
      "PROC expORT data= data.out_models_snap \n",
      "            outFILE= \"&home\\allmodelssnap.xlsx\" \n",
      "            DBMS=xlsx REPLACE;\n",
      " *    GETNAMES=YES;\n",
      "RUN;\n",
      "\n",
      "\n",
      "\n",
      "/*PROC expORT data= WORK.out_model0*/\n",
      "/*            outFILE= \"&home\\model0.xlsx\" */\n",
      "/*            DBMS=xlsx REPLACE;*/\n",
      "/* *    GETNAMES=YES;*/\n",
      "/*RUN;*/\n",
      "/*PROC expORT data= WORK.out_model1*/\n",
      "/*            outFILE= \"&home\\model1.xlsx\" */\n",
      "/*            DBMS=xlsx REPLACE;*/\n",
      "/* *    GETNAMES=YES;*/\n",
      "/*RUN;*/\n",
      "/*PROC expORT data= WORK.out_model2*/\n",
      "/*            outFILE= \"&home\\model2.xlsx\" */\n",
      "/*            DBMS=xlsx REPLACE;*/\n",
      "/* *    GETNAMES=YES;*/\n",
      "/*RUN;*/\n",
      "/**/\n",
      "/*PROC expORT data= WORK.out_model3*/\n",
      "/*            outFILE= \"&home\\model3.xlsx\" */\n",
      "/*            DBMS=xlsx REPLACE;*/\n",
      "/* *    GETNAMES=YES;*/\n",
      "/*RUN;*/\n",
      "\n",
      "\n",
      "******************************************************************************************************;\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to your .sas script file (SAS code, not dataset)\n",
    "file_path = \"/Users/dengshuyue/Desktop/SDOH/analysis/code/code_lu/Analysis1_COX_allcause.sas\"\n",
    "\n",
    "# Load the SAS script as plain text\n",
    "with open(file_path, \"r\") as file:\n",
    "    sas_code = file.read()\n",
    "\n",
    "# Optionally preview the first 30 lines\n",
    "sas_code_lines = sas_code.splitlines()\n",
    "for line in sas_code_lines[:300]:\n",
    "    print(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7908d2b4-cccd-4a07-bcae-4a6bed5e87b5",
   "metadata": {},
   "source": [
    "<h2>Step 1: NHANES demographic data (fetch and merge)</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bdf967b3-dd39-49f5-b7e1-d1cdfc7f9026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total rows: 128809\n",
      "✅ Unique SEQN: 128809\n",
      "Columns: ['SEQN', 'RIDAGEYR', 'SDDSRVYR', 'CYCLE', 'MARRIAGE', 'MARRIAGE3']\n",
      "\n",
      "2021–2023 MARRIAGE3 counts:\n",
      " MARRIAGE3\n",
      "<NA>    4150\n",
      "1       4136\n",
      "2       2022\n",
      "3       1625\n",
      "Name: count, dtype: Int64\n"
     ]
    }
   ],
   "source": [
    "# %% Step 1: Paths and cycle mapping (matches your project)\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "BASE = Path(\"/Users/dengshuyue/Desktop/SDOH/analysis\")\n",
    "DATA: Path = BASE / \"data\"\n",
    "(DATA / \"nhanes_by_module\" / \"DEMO\").mkdir(parents=True, exist_ok=True)  # where we’ll save downloads\n",
    "\n",
    "# --- Candidate upstream URLs for DEMO files (multiple fallbacks per cycle) ---\n",
    "# Includes the “classic” /Nhanes/<cycle>/ path and the /Nchs/Data/Nhanes/Public/<year>/DataFiles/ path.\n",
    "def nhanes_url_candidates():\n",
    "    return {\n",
    "        \"1999-2000\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/1999/DataFiles/DEMO.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/1999-2000/DEMO.XPT\",\n",
    "        ],\n",
    "        \"2001-2002\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2001/DataFiles/DEMO_B.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2001-2002/DEMO_B.XPT\",\n",
    "        ],\n",
    "        \"2003-2004\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2003/DataFiles/DEMO_C.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2003-2004/DEMO_C.XPT\",\n",
    "        ],\n",
    "        \"2005-2006\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2005/DataFiles/DEMO_D.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2005-2006/DEMO_D.XPT\",\n",
    "        ],\n",
    "        \"2007-2008\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2007/DataFiles/DEMO_E.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2007-2008/DEMO_E.XPT\",\n",
    "        ],\n",
    "        \"2009-2010\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2009/DataFiles/DEMO_F.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2009-2010/DEMO_F.XPT\",\n",
    "        ],\n",
    "        \"2011-2012\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2011/DataFiles/DEMO_G.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2011-2012/DEMO_G.XPT\",\n",
    "        ],\n",
    "        \"2013-2014\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2013/DataFiles/DEMO_H.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2013-2014/DEMO_H.XPT\",\n",
    "        ],\n",
    "        \"2015-2016\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2015/DataFiles/DEMO_I.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2015-2016/DEMO_I.XPT\",\n",
    "        ],\n",
    "        \"2017-2018\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2017/DataFiles/DEMO_J.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2017-2018/DEMO_J.XPT\",\n",
    "        ],\n",
    "        # Special combined pre-pandemic release\n",
    "        \"2017-March 2020 (pre-pandemic)\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2017/DataFiles/P_DEMO.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2017-2020/P_DEMO.XPT\",\n",
    "        ],\n",
    "        # August 2021–August 2023 (DEMO_L; include Q as fallback just in case)\n",
    "        \"August 2021–August 2023\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2021/DataFiles/DEMO_L.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2021-2023/DEMO_L.XPT\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2021/DataFiles/DEMO_Q.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2021-2023/DEMO_Q.XPT\",\n",
    "        ],\n",
    "    }\n",
    "\n",
    "NHANES_URLS = nhanes_url_candidates()\n",
    "\n",
    "# Build local filename candidates per cycle (try these names before downloading)\n",
    "def candidates_from_urls(urls):\n",
    "    out, seen = [], set()\n",
    "    for url in urls:\n",
    "        fname = Path(url).name\n",
    "        for v in (fname, fname.upper(), fname.lower(), fname.capitalize()):\n",
    "            if v not in seen:\n",
    "                seen.add(v)\n",
    "                out.append(v)\n",
    "    return out\n",
    "\n",
    "LOCAL_CANDIDATES = {cycle: candidates_from_urls(urls)\n",
    "                    for cycle, urls in NHANES_URLS.items()}\n",
    "\n",
    "# %% Step 2: helpers (local search + download + reader)\n",
    "def find_first_under_data(patterns):\n",
    "    \"\"\"Search recursively under DATA for any of the provided filenames (case-sensitive per pattern list).\"\"\"\n",
    "    for pattern in patterns:\n",
    "        hits = list(DATA.rglob(pattern))\n",
    "        if hits:\n",
    "            return hits[0]\n",
    "    return None\n",
    "\n",
    "# define fuction to fetch from CDC web\n",
    "def download_to(path: Path, url: str, timeout=90):\n",
    "    \"\"\"Download URL -> path with a basic retry; return local path.\"\"\"\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    headers = {\"User-Agent\": \"nhanes-fetch/1.0 (+https://cdc.gov)\"}\n",
    "    last_err = None\n",
    "    for attempt in range(2):  # two tries per URL\n",
    "        try:\n",
    "            resp = requests.get(url, headers=headers, timeout=timeout, stream=True)\n",
    "            resp.raise_for_status()\n",
    "            tmp = path.with_suffix(path.suffix + \".downloading\")\n",
    "            with open(tmp, \"wb\") as f:\n",
    "                for chunk in resp.iter_content(chunk_size=1 << 15):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "            tmp.rename(path)\n",
    "            return path\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "    raise last_err\n",
    "\n",
    "def ensure_demo_file(cycle_label: str) -> Path:\n",
    "    \"\"\"Return a local DEMO file path for the cycle (search first, then download from known URLs).\"\"\"\n",
    "    local = find_first_under_data(LOCAL_CANDIDATES[cycle_label])\n",
    "    if local:\n",
    "        return local\n",
    "    for url in NHANES_URLS[cycle_label]:\n",
    "        out = DATA / \"nhanes_by_module\" / \"DEMO\" / Path(url).name\n",
    "        try:\n",
    "            print(f\"⬇️  Downloading {cycle_label} from {url}\")\n",
    "            return download_to(out, url)\n",
    "        except Exception as e:\n",
    "            print(f\"   ⚠️ Download failed from {url}: {e}\")\n",
    "    raise FileNotFoundError(f\"No DEMO file found or downloaded for {cycle_label}\")\n",
    "\n",
    "def read_demo_file(p: Path) -> pd.DataFrame:\n",
    "    \"\"\"Read DEMO file (.xpt preferred, .sas7bdat fallback).\"\"\"\n",
    "    if p.suffix.lower() == \".xpt\":\n",
    "        # Prefer pyreadstat if available (faster/labels), else pandas\n",
    "        try:\n",
    "            import pyreadstat\n",
    "            df, _ = pyreadstat.read_xport(p)\n",
    "        except Exception:\n",
    "            df = pd.read_sas(p, format=\"xport\")\n",
    "    elif p.suffix.lower() == \".sas7bdat\":\n",
    "        try:\n",
    "            import pyreadstat\n",
    "            df, _ = pyreadstat.read_sas7bdat(p)\n",
    "        except Exception:\n",
    "            df = pd.read_sas(p, format=\"sas7bdat\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {p.suffix}\")\n",
    "    return df\n",
    "\n",
    "# Which cycles to include in this DEMO build (OK to include through 2023 here)\n",
    "DEMO_CYCLES = [\n",
    "    \"1999-2000\",\"2001-2002\",\"2003-2004\",\"2005-2006\",\"2007-2008\",\n",
    "    \"2009-2010\",\"2011-2012\",\"2013-2014\",\"2015-2016\",\"2017-2018\",\n",
    "    \"2017-March 2020 (pre-pandemic)\",\"August 2021–August 2023\",\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- helpers to recode marital status ---\n",
    "def recode_L_to_4(s):\n",
    "    # DMDMARTL codes: 1=Married, 2=Never, 3=Widowed, 4=Divorced, 5=Separated, 6=Living with partner, 77/99=DK/Ref\n",
    "    s = pd.to_numeric(s, errors=\"coerce\")\n",
    "    out = pd.Series(pd.NA, index=s.index, dtype=\"Int64\")\n",
    "    out = out.where(~s.isin([1, 6]), 1)   # married/partner\n",
    "    out = out.where(~s.isin([3, 4]), 2)   # widowed/divorced\n",
    "    out = out.where(~(s == 2), 3)         # never married\n",
    "    out = out.where(~(s == 5), 4)         # separated\n",
    "    out = out.where(~s.isin([77, 99]), pd.NA)\n",
    "    return out\n",
    "\n",
    "def recode_L_to_3(s):\n",
    "    # Collapse L into 3 categories to match Z:\n",
    "    # 1 = married/partner; 2 = previously married (widowed/divorced/separated); 3 = never\n",
    "    s = pd.to_numeric(s, errors=\"coerce\")\n",
    "    out = pd.Series(pd.NA, index=s.index, dtype=\"Int64\")\n",
    "    out = out.where(~s.isin([1, 6]), 1)\n",
    "    out = out.where(~s.isin([3, 4, 5]), 2)\n",
    "    out = out.where(~(s == 2), 3)\n",
    "    out = out.where(~s.isin([77, 99]), pd.NA)\n",
    "    return out\n",
    "\n",
    "def recode_Z_to_3(s):\n",
    "    # DMDMARTZ codes (2021–2023): 1=married/partner, 2=previously married, 3=never, 77/99=DK/Ref\n",
    "    s = pd.to_numeric(s, errors=\"coerce\")\n",
    "    out = pd.Series(pd.NA, index=s.index, dtype=\"Int64\")\n",
    "    out = out.where(~(s == 1), 1)\n",
    "    out = out.where(~(s == 2), 2)\n",
    "    out = out.where(~(s == 3), 3)\n",
    "    out = out.where(~s.isin([77, 99]), pd.NA)\n",
    "    return out\n",
    "\n",
    "def compute_marriage_cols(df_upper: pd.DataFrame):\n",
    "    \"\"\"Return (MARRIAGE, MARRIAGE3) given an uppercase-column DataFrame.\"\"\"\n",
    "    hasL = \"DMDMARTL\" in df_upper.columns\n",
    "    hasZ = \"DMDMARTZ\" in df_upper.columns\n",
    "    M4 = pd.Series(pd.NA, index=df_upper.index, dtype=\"Int64\")\n",
    "    M3 = pd.Series(pd.NA, index=df_upper.index, dtype=\"Int64\")\n",
    "    if hasL:\n",
    "        M4 = recode_L_to_4(df_upper[\"DMDMARTL\"])\n",
    "        M3 = recode_L_to_3(df_upper[\"DMDMARTL\"])\n",
    "    elif hasZ:\n",
    "        # Can't reconstruct a 4-cat from Z; leave M4 as NA\n",
    "        M3 = recode_Z_to_3(df_upper[\"DMDMARTZ\"])\n",
    "    return M4, M3\n",
    "\n",
    "# %% Step 3: Load (local-or-download), tag cycle, recode marriage per cycle, and stack\n",
    "demo_dfs, missing = [], []\n",
    "for cycle in DEMO_CYCLES:\n",
    "    try:\n",
    "        p = ensure_demo_file(cycle)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"⚠️ {e}\")\n",
    "        missing.append(cycle)\n",
    "        continue\n",
    "\n",
    "    df = read_demo_file(p)\n",
    "    df.columns = [c.upper() for c in df.columns]\n",
    "    df[\"CYCLE\"] = cycle\n",
    "\n",
    "    # Recode marital status for this cycle using whichever column exists\n",
    "    M4, M3 = compute_marriage_cols(df)\n",
    "    df[\"MARRIAGE\"]  = M4          # 4-category where available; NA for 2021–2023\n",
    "    df[\"MARRIAGE3\"] = M3          # 3-category available for ALL cycles\n",
    "\n",
    "    # Keep only relevant columns that exist\n",
    "    keep = [c for c in [\"SEQN\",\"RIDAGEYR\",\"SDDSRVYR\",\"CYCLE\",\"MARRIAGE\",\"MARRIAGE3\"] if c in df.columns]\n",
    "    demo_dfs.append(df[keep])\n",
    "\n",
    "if missing:\n",
    "    print(\"⚠️ Missing cycles (not found and not downloaded):\", missing)\n",
    "if not demo_dfs:\n",
    "    raise FileNotFoundError(\"No DEMO files were found or downloaded under your data/ directory.\")\n",
    "\n",
    "# %% Step 4: Combine\n",
    "demo9923 = pd.concat(demo_dfs, ignore_index=True).copy()\n",
    "\n",
    "# %% Step 5/6 merged: we already kept just the needed columns; print summary\n",
    "print(f\"✅ Total rows: {demo9923.shape[0]}\")\n",
    "print(f\"✅ Unique SEQN: {demo9923['SEQN'].nunique()}\")\n",
    "print(\"Columns:\", demo9923.columns.tolist())\n",
    "\n",
    "# Optional quick check: 2021–2023 now has MARRIAGE3 populated\n",
    "vc = demo9923.loc[demo9923[\"CYCLE\"]==\"August 2021–August 2023\",\"MARRIAGE3\"].value_counts(dropna=False)\n",
    "print(\"\\n2021–2023 MARRIAGE3 counts:\\n\", vc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562ed918-6c96-4412-acba-827f652d9fc6",
   "metadata": {},
   "source": [
    "<h3>save and check</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "473fea1a-e695-4b8a-877e-2329915dc8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /Users/dengshuyue/Desktop/SDOH/analysis/data/demo9923.pkl\n",
      "Saved: /Users/dengshuyue/Desktop/SDOH/analysis/data/demo9923.csv\n"
     ]
    }
   ],
   "source": [
    "# Simple saves without pyarrow (plain CSV)\n",
    "out_pkl = DATA / \"demo9923.pkl\"   # fastest round-trip in Python\n",
    "out_csv = DATA / \"demo9923.csv\"   # plain CSV (max compatibility)\n",
    "\n",
    "demo9923.to_pickle(out_pkl)\n",
    "demo9923.to_csv(out_csv, index=False)  # no compression\n",
    "\n",
    "print(\"Saved:\", out_pkl)\n",
    "print(\"Saved:\", out_csv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af7ba73c-db7a-4696-b68a-14fdea35b5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Counts by CYCLE:\n",
      "CYCLE\n",
      "2017-March 2020 (pre-pandemic)    15560\n",
      "August 2021–August 2023           11933\n",
      "2001-2002                         11039\n",
      "2009-2010                         10537\n",
      "2005-2006                         10348\n",
      "2013-2014                         10175\n",
      "2007-2008                         10149\n",
      "2003-2004                         10122\n",
      "2015-2016                          9971\n",
      "1999-2000                          9965\n",
      "2011-2012                          9756\n",
      "2017-2018                          9254\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts by SDDSRVYR (raw numeric code, may be NaN for newer releases):\n",
      "SDDSRVYR\n",
      "1.0      9965\n",
      "2.0     11039\n",
      "3.0     10122\n",
      "4.0     10348\n",
      "5.0     10149\n",
      "6.0     10537\n",
      "7.0      9756\n",
      "8.0     10175\n",
      "9.0      9971\n",
      "10.0     9254\n",
      "12.0    11933\n",
      "66.0    15560\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCounts by CYCLE:\")\n",
    "print(demo9923[\"CYCLE\"].value_counts(dropna=False))\n",
    "\n",
    "print(\"\\nCounts by SDDSRVYR (raw numeric code, may be NaN for newer releases):\")\n",
    "print(demo9923[\"SDDSRVYR\"].value_counts(dropna=False).sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d124b00c-1b5e-4047-80dc-1f19a12132c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEQN</th>\n",
       "      <th>RIDAGEYR</th>\n",
       "      <th>SDDSRVYR</th>\n",
       "      <th>CYCLE</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>MARRIAGE3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>128793</th>\n",
       "      <td>142295.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>August 2021–August 2023</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128794</th>\n",
       "      <td>142296.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>August 2021–August 2023</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128795</th>\n",
       "      <td>142297.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>August 2021–August 2023</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128796</th>\n",
       "      <td>142298.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>August 2021–August 2023</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128797</th>\n",
       "      <td>142299.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>August 2021–August 2023</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128798</th>\n",
       "      <td>142300.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>August 2021–August 2023</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128799</th>\n",
       "      <td>142301.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>August 2021–August 2023</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128800</th>\n",
       "      <td>142302.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>August 2021–August 2023</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128801</th>\n",
       "      <td>142303.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>August 2021–August 2023</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128802</th>\n",
       "      <td>142304.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>August 2021–August 2023</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128803</th>\n",
       "      <td>142305.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>August 2021–August 2023</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128804</th>\n",
       "      <td>142306.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>August 2021–August 2023</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128805</th>\n",
       "      <td>142307.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>August 2021–August 2023</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128806</th>\n",
       "      <td>142308.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>August 2021–August 2023</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128807</th>\n",
       "      <td>142309.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>August 2021–August 2023</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128808</th>\n",
       "      <td>142310.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>August 2021–August 2023</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            SEQN  RIDAGEYR  SDDSRVYR                    CYCLE  MARRIAGE  \\\n",
       "128793  142295.0      80.0      12.0  August 2021–August 2023      <NA>   \n",
       "128794  142296.0       1.0      12.0  August 2021–August 2023      <NA>   \n",
       "128795  142297.0      76.0      12.0  August 2021–August 2023      <NA>   \n",
       "128796  142298.0      60.0      12.0  August 2021–August 2023      <NA>   \n",
       "128797  142299.0      33.0      12.0  August 2021–August 2023      <NA>   \n",
       "128798  142300.0      46.0      12.0  August 2021–August 2023      <NA>   \n",
       "128799  142301.0      80.0      12.0  August 2021–August 2023      <NA>   \n",
       "128800  142302.0      70.0      12.0  August 2021–August 2023      <NA>   \n",
       "128801  142303.0      69.0      12.0  August 2021–August 2023      <NA>   \n",
       "128802  142304.0      14.0      12.0  August 2021–August 2023      <NA>   \n",
       "128803  142305.0      76.0      12.0  August 2021–August 2023      <NA>   \n",
       "128804  142306.0       9.0      12.0  August 2021–August 2023      <NA>   \n",
       "128805  142307.0      49.0      12.0  August 2021–August 2023      <NA>   \n",
       "128806  142308.0      50.0      12.0  August 2021–August 2023      <NA>   \n",
       "128807  142309.0      40.0      12.0  August 2021–August 2023      <NA>   \n",
       "128808  142310.0      80.0      12.0  August 2021–August 2023      <NA>   \n",
       "\n",
       "        MARRIAGE3  \n",
       "128793          2  \n",
       "128794       <NA>  \n",
       "128795          2  \n",
       "128796          1  \n",
       "128797          1  \n",
       "128798          1  \n",
       "128799          2  \n",
       "128800          3  \n",
       "128801          2  \n",
       "128802       <NA>  \n",
       "128803          2  \n",
       "128804       <NA>  \n",
       "128805          3  \n",
       "128806          1  \n",
       "128807          2  \n",
       "128808          1  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo9923.tail(16)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4fceb15f-f051-4f4d-bb66-0936de39864f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate columns for marital status: ['DMDMARTZ']\n",
      "\n",
      " DMDMARTZ\n",
      "DMDMARTZ\n",
      "NaN     4141\n",
      "1.0     4136\n",
      "2.0     2022\n",
      "3.0     1625\n",
      "99.0       5\n",
      "77.0       4\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50aff7dc-77ed-4d06-be26-be021a58156a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a9ad7e-d9e0-4488-a042-b17f884dbb95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72df42b6-ac05-4e45-9520-070a01e153fe",
   "metadata": {},
   "source": [
    "<h2> Step 2: merge demo with mortality </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5dd8e708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mortality file: /Users/dengshuyue/Desktop/SDOH/analysis/data/less_important/mortality9918.sas7bdat\n",
      "\n",
      "📊 NHANES cycles in mortality-linked data:\n",
      "SDDSRVYR\n",
      "1999–2000    4973\n",
      "2001–2002    5586\n",
      "2003–2004    5293\n",
      "2005–2006    5332\n",
      "2007–2008    5989\n",
      "2009–2010    6346\n",
      "2011–2012    5603\n",
      "2013–2014    5913\n",
      "2015–2016    5720\n",
      "2017–2018    5498\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total records: 56253\n",
      "\n",
      "⏱️  Survival summary:\n",
      "  N (unique SEQN): 56253\n",
      "  Events (%): 14.87\n",
      "  TIME_Y (min / median / max): 0.0 9.416666666666666 20.75\n"
     ]
    }
   ],
   "source": [
    "# %% Mortality: locate, read, prep, and merge with DEMO (uses demo9923)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Paths -------------------------------------------------------\n",
    "ROOT = Path(\"/Users/dengshuyue/Desktop/SDOH/analysis\")\n",
    "DATA = ROOT / \"data\"\n",
    "\n",
    "# --- Locate mortality file (search recursively under data/) -----\n",
    "MORT_PATTERNS = [\n",
    "    \"*mortality*.sas7bdat\", \"*mort*.sas7bdat\",\n",
    "    \"*mortality*.xpt\",      \"*mort*.xpt\",\n",
    "    \"*mortality*.csv\",      \"*mort*.csv\"\n",
    "]\n",
    "\n",
    "def find_first(base: Path, patterns):\n",
    "    for pat in patterns:\n",
    "        hits = list(base.rglob(pat))\n",
    "        if hits:\n",
    "            return hits[0]\n",
    "    return None\n",
    "\n",
    "mort_path = find_first(DATA, MORT_PATTERNS)\n",
    "if mort_path is None:\n",
    "    raise FileNotFoundError(\n",
    "        f\"No mortality file found under {DATA}. \"\n",
    "        \"Place a file like 'NHANES_1999_2019_LMF_public.csv' or 'mortality9918.sas7bdat' there.\"\n",
    "    )\n",
    "print(\"Using mortality file:\", mort_path)\n",
    "\n",
    "# --- Load mortality (handles .xpt / .sas7bdat / .csv) ----------\n",
    "suffix = mort_path.suffix.lower()\n",
    "if suffix == \".xpt\":\n",
    "    try:\n",
    "        import pyreadstat\n",
    "        mort, _ = pyreadstat.read_xport(mort_path)\n",
    "    except Exception:\n",
    "        mort = pd.read_sas(mort_path, format=\"xport\")\n",
    "elif suffix == \".sas7bdat\":\n",
    "    try:\n",
    "        import pyreadstat\n",
    "        mort, _ = pyreadstat.read_sas7bdat(mort_path)\n",
    "    except Exception:\n",
    "        mort = pd.read_sas(mort_path, format=\"sas7bdat\")\n",
    "elif suffix == \".csv\":\n",
    "    mort = pd.read_csv(mort_path)\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported mortality file type: {suffix}\")\n",
    "\n",
    "mort.columns = mort.columns.str.upper()\n",
    "\n",
    "# --- Keep relevant columns if present ---------------------------\n",
    "keep_cols = [c for c in [\n",
    "    \"SEQN\",\"ELIGSTAT\",\"MORTSTAT\",\"PERMTH_EXM\",\"PERMTH_INT\",\n",
    "    \"UCOD_LEADING\",\"DIABETES\",\"HYPERTEN\",\"DODQTR\",\"DODYEAR\"\n",
    "] if c in mort.columns]\n",
    "mort = mort[keep_cols].copy()\n",
    "\n",
    "# --- Eligibility and time/event construction --------------------\n",
    "if \"ELIGSTAT\" in mort.columns:\n",
    "    mort = mort[mort[\"ELIGSTAT\"] == 1].copy()\n",
    "\n",
    "# default: exam-based time; switch to interview by changing TIME_COL\n",
    "TIME_COL = \"PERMTH_EXM\" if \"PERMTH_EXM\" in mort.columns else \"PERMTH_INT\"\n",
    "if TIME_COL not in mort.columns:\n",
    "    raise ValueError(\"Neither PERMTH_EXM nor PERMTH_INT found in mortality file.\")\n",
    "\n",
    "mort[\"TIME_Y\"] = pd.to_numeric(mort[TIME_COL], errors=\"coerce\") / 12.0\n",
    "mort[\"EVENT\"]  = (mort[\"MORTSTAT\"] == 1).astype(int)\n",
    "mort = mort[(mort[\"TIME_Y\"].notna()) & (mort[\"TIME_Y\"] >= 0)].copy()\n",
    "\n",
    "# --- Ensure DEMO columns & types (demo9923) ---------------------\n",
    "demo9923.columns = demo9923.columns.str.upper()\n",
    "if \"SDDSRVYR\" not in demo9923.columns:\n",
    "    raise ValueError(\"SDDSRVYR not found in demo9923. Ensure your DEMO build carries SDDSRVYR.\")\n",
    "demo9923[\"SDDSRVYR\"] = pd.to_numeric(demo9923[\"SDDSRVYR\"], errors=\"coerce\")\n",
    "\n",
    "# --- Merge age + cycle into mortality ---------------------------\n",
    "cols_to_merge = [c for c in [\"SEQN\",\"RIDAGEYR\",\"SDDSRVYR\"] if c in demo9923.columns]\n",
    "mort_with_demo = mort.merge(demo9923[cols_to_merge], on=\"SEQN\", how=\"left\")\n",
    "\n",
    "# Optional: adults (≥20)\n",
    "# mort_with_demo = mort_with_demo.dropna(subset=[\"RIDAGEYR\"])\n",
    "# mort_with_demo = mort_with_demo[mort_with_demo[\"RIDAGEYR\"] >= 20]\n",
    "\n",
    "# --- Cycle counts and summary -----------------------------------\n",
    "cycle_counts = mort_with_demo[\"SDDSRVYR\"].value_counts(dropna=False).sort_index()\n",
    "\n",
    "cycle_map = {\n",
    "    1: \"1999–2000\",  2: \"2001–2002\",  3: \"2003–2004\",\n",
    "    4: \"2005–2006\",  5: \"2007–2008\",  6: \"2009–2010\",\n",
    "    7: \"2011–2012\",  8: \"2013–2014\",  9: \"2015–2016\",\n",
    "    10: \"2017–2018\", 11: \"2017–Mar 2020 (pre-pandemic)\", 12: \"Aug 2021–Aug 2023\"\n",
    "}\n",
    "\n",
    "print(\"\\n📊 NHANES cycles in mortality-linked data:\")\n",
    "print(cycle_counts.rename(index=cycle_map))\n",
    "print(f\"\\nTotal records: {int(cycle_counts.sum())}\")\n",
    "\n",
    "print(\"\\n⏱️  Survival summary:\")\n",
    "print(\"  N (unique SEQN):\", mort_with_demo[\"SEQN\"].nunique())\n",
    "print(\"  Events (%):\", round(100 * mort_with_demo[\"EVENT\"].mean(), 2))\n",
    "print(\"  TIME_Y (min / median / max):\",\n",
    "      np.nanmin(mort_with_demo[\"TIME_Y\"]),\n",
    "      np.nanmedian(mort_with_demo[\"TIME_Y\"]),\n",
    "      np.nanmax(mort_with_demo[\"TIME_Y\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df6670d-1b16-413f-a629-93b05ac9befb",
   "metadata": {},
   "source": [
    "<h2>Step 3: merge demo_mort with sodh info (BEGIN HERE !! )</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176e6fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# -----------------------------\n",
    "# Step 0: Setup (NEW PATHS)\n",
    "# -----------------------------\n",
    "base_module_path = Path(\"/Users/dengshuyue/Desktop/SDOH/analysis/data/nhanes_by_module\")\n",
    "\n",
    "ocq_path = base_module_path / \"ocq\" / \"ocq.sas7bdat\"     # main OCQ (2003–2018)\n",
    "hoq_path = base_module_path / \"hoq\" / \"hoq.sas7bdat\"     # main HOQ (2003–2018)\n",
    "hiq_path = base_module_path / \"hiq\" / \"hiqs.sas7bdat\"    # insurance (HIQS for later years)\n",
    "fsq_path = base_module_path / \"fsq\" / \"fsqs.sas7bdat\"    # food security (FSQS for later years)\n",
    "\n",
    "# Step 1: Load demoall (unchanged)\n",
    "demoall = pd.read_pickle(\"/Users/dengshuyue/Desktop/SDOH/analysis/data/demoall.pkl\")\n",
    "if \"SEQN\" not in demoall.columns or \"RIDAGEYR\" not in demoall.columns:\n",
    "    raise ValueError(\"demoall is missing SEQN or RIDAGEYR\")\n",
    "\n",
    "# Step 2: Filter demoall to adults (age ≥ 20)\n",
    "age_df = demoall[[\"SEQN\", \"RIDAGEYR\"]].dropna()\n",
    "age_df = age_df[age_df[\"RIDAGEYR\"] >= 20]\n",
    "\n",
    "# Helper: Filter any df to age ≥ 20 using demoall\n",
    "def filter_adults(df):\n",
    "    return df.merge(age_df, on=\"SEQN\", how=\"inner\")\n",
    "\n",
    "# -----------------------------\n",
    "# Step 3: Employment (OCQ)\n",
    "# -----------------------------\n",
    "if not ocq_path.exists():\n",
    "    raise FileNotFoundError(f\"OCQ file not found at: {ocq_path}\")\n",
    "\n",
    "ocq = filter_adults(pd.read_sas(str(ocq_path), format=\"sas7bdat\", encoding=\"latin1\"))\n",
    "ocq['employ'] = np.nan\n",
    "ocq.loc[ocq['OCD150'] == 1, 'employ'] = 1\n",
    "ocq.loc[(ocq['OCD150'] == 3) | (ocq['OCQ380'] == 5), 'employ'] = 2\n",
    "ocq.loc[ocq['OCQ380'] == 3, 'employ'] = 3\n",
    "ocq.loc[ocq['OCQ380'].isin([4, 6]), 'employ'] = 4\n",
    "ocq.loc[ocq['OCQ380'].isin([1, 2, 7]), 'employ'] = 5\n",
    "ocq['unemployment'] = (ocq['employ'] == 2).astype(int)\n",
    "ocq = ocq[['SEQN', 'employ', 'unemployment']]\n",
    "\n",
    "# -----------------------------\n",
    "# Step 4: Housing (HOQ)\n",
    "# -----------------------------\n",
    "if not hoq_path.exists():\n",
    "    raise FileNotFoundError(f\"HOQ file not found at: {hoq_path}\")\n",
    "\n",
    "hoq = filter_adults(pd.read_sas(str(hoq_path), format=\"sas7bdat\", encoding=\"latin1\"))\n",
    "if \"HOQ065\" in hoq.columns:\n",
    "    hoq.loc[hoq['HOQ065'].isin([7, 9]), 'HOQ065'] = np.nan\n",
    "keep_hoq = [c for c in ['SEQN', 'HOD050', 'HOQ065'] if c in hoq.columns]\n",
    "hoq = hoq[keep_hoq]\n",
    "\n",
    "# -----------------------------\n",
    "# Step 5: Insurance (HIQS)\n",
    "# -----------------------------\n",
    "if not hiq_path.exists():\n",
    "    raise FileNotFoundError(f\"HIQS file not found at: {hiq_path}\")\n",
    "\n",
    "hiqs = filter_adults(pd.read_sas(str(hiq_path), format=\"sas7bdat\", encoding=\"latin1\"))\n",
    "ins = pd.DataFrame({'SEQN': hiqs['SEQN']})\n",
    "ins['ins'] = np.nan\n",
    "\n",
    "# Private\n",
    "if 'HIQ031A' in hiqs: ins.loc[(hiqs['HIQ031A'] == 14), 'ins'] = 1\n",
    "if 'HID030A' in hiqs: ins.loc[(hiqs['HID030A'] == 1), 'ins'] = 1\n",
    "\n",
    "# Medicare\n",
    "cond_med = False\n",
    "if set(['HIQ031B','HIQ031D','HIQ031E']).issubset(hiqs.columns):\n",
    "    cond_med = ((hiqs['HIQ031B'] == 15) & (hiqs['HIQ031D'] != 17) & (hiqs['HIQ031E'] != 18))\n",
    "if set(['HID030B','HID030C']).issubset(hiqs.columns):\n",
    "    cond_med = cond_med | ((hiqs['HID030B'] == 1) & (hiqs['HID030C'] != 1)) if isinstance(cond_med, pd.Series) else ((hiqs['HID030B'] == 1) & (hiqs['HID030C'] != 1))\n",
    "ins.loc[cond_med, 'ins'] = 2\n",
    "\n",
    "# Medicaid\n",
    "cond_mcaid = False\n",
    "if set(['HIQ031B','HIQ031D','HIQ031E']).issubset(hiqs.columns):\n",
    "    cond_mcaid = (((hiqs['HIQ031D'] == 17) | (hiqs['HIQ031E'] == 18)) & (hiqs['HIQ031B'] != 15))\n",
    "if set(['HID030B','HID030C']).issubset(hiqs.columns):\n",
    "    cond_mcaid = cond_mcaid | ((hiqs['HID030B'] != 1) & (hiqs['HID030C'] == 1)) if isinstance(cond_mcaid, pd.Series) else ((hiqs['HID030B'] != 1) & (hiqs['HID030C'] == 1))\n",
    "ins.loc[cond_mcaid, 'ins'] = 3\n",
    "\n",
    "# Medicaid (both present)\n",
    "if set(['HIQ031B','HIQ031D']).issubset(hiqs.columns):\n",
    "    ins.loc[((hiqs['HIQ031B'] == 15) & (hiqs['HIQ031D'] == 17)), 'ins'] = 3\n",
    "if set(['HID030B','HID030C']).issubset(hiqs.columns):\n",
    "    ins.loc[((hiqs['HID030B'] == 1) & (hiqs['HID030C'] == 1)), 'ins'] = 3\n",
    "\n",
    "# Other insurance\n",
    "cols_other = [c for c in ['HIQ031C','HIQ031F','HIQ031G','HIQ031H','HIQ031I'] if c in hiqs.columns]\n",
    "cond_other = hiqs[cols_other].eq(1).any(axis=1) if cols_other else False\n",
    "if 'HID030D' in hiqs:\n",
    "    cond_other = cond_other | (hiqs['HID030D'] == 1) if isinstance(cond_other, pd.Series) else (hiqs['HID030D'] == 1)\n",
    "ins.loc[cond_other, 'ins'] = 5\n",
    "\n",
    "# No insurance\n",
    "conds_none = []\n",
    "if 'HIQ011' in hiqs: conds_none.append(hiqs['HIQ011'] == 2)\n",
    "if 'HID010' in hiqs: conds_none.append(hiqs['HID010'] == 2)\n",
    "if conds_none:\n",
    "    ins.loc[np.logical_or.reduce(conds_none), 'ins'] = 0\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Step 6: SNAP & Food Security (FSQS)\n",
    "# -----------------------------\n",
    "if not fsq_path.exists():\n",
    "    raise FileNotFoundError(f\"FSQS file not found: {fsq_path}\")\n",
    "\n",
    "fsqs = filter_adults(pd.read_sas(str(fsq_path), format=\"sas7bdat\", encoding=\"latin1\"))\n",
    "snap = pd.DataFrame({'SEQN': fsqs['SEQN']})\n",
    "if 'FSDHH' in fsqs: snap['FSDHH'] = fsqs['FSDHH']\n",
    "\n",
    "snap['SNAP'] = np.nan\n",
    "if 'FSQ165' in fsqs: snap.loc[fsqs['FSQ165'] == 2, 'SNAP'] = 0\n",
    "if 'FSQ012' in fsqs:\n",
    "    snap.loc[fsqs['FSQ012'] == 1, 'SNAP'] = 1\n",
    "    snap.loc[fsqs['FSQ012'] == 2, 'SNAP'] = 0\n",
    "if 'FSQ171' in fsqs:\n",
    "    snap.loc[fsqs['FSQ171'] == 1, 'SNAP'] = 1\n",
    "    snap.loc[fsqs['FSQ171'] == 2, 'SNAP'] = 0\n",
    "if 'FSD170N' in fsqs: snap.loc[fsqs['FSD170N'] >= 1, 'SNAP'] = 1\n",
    "if 'FSQ170' in fsqs:\n",
    "    snap.loc[fsqs['FSQ170'] == 1, 'SNAP'] = 1\n",
    "    snap.loc[(fsqs['FSQ170'] == 2) & (fsqs.get('FSD170N', pd.Series(index=fsqs.index)) < 1), 'SNAP'] = 0\n",
    "if 'FSD200' in fsqs: snap.loc[fsqs['FSD200'] == 1, 'SNAP'] = 1\n",
    "\n",
    "snap['FS'] = np.nan\n",
    "if 'FSDHH' in fsqs:\n",
    "    snap.loc[fsqs['FSDHH'].isin([1, 2]), 'FS'] = 1\n",
    "    snap.loc[fsqs['FSDHH'] > 2, 'FS'] = 0\n",
    "\n",
    "snap = snap[['SEQN', 'SNAP', 'FSDHH', 'FS']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e57d417",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# merge previous 99-03 ir 99-01  first check their SEQN\n",
    "\n",
    "import pyreadstat\n",
    "from pathlib import Path\n",
    "\n",
    "ocq_dir = Path(\"/Users/dengshuyue/Desktop/SDOH/analysis/data/nhanes_by_module/ocq\")\n",
    "\n",
    "files = [\n",
    "    (\"OCQ.xpt\",   \"1999–2000\"),\n",
    "    (\"OCQ_B.xpt\", \"2001–2002\"),\n",
    "    (\"OCQ_C.xpt\", \"2003–2004\"),\n",
    "]\n",
    "\n",
    "for fname, years in files:\n",
    "    path = ocq_dir / fname\n",
    "    if not path.exists():\n",
    "        print(f\"⚠️ File not found: {path}\")\n",
    "        continue\n",
    "    \n",
    "    df, meta = pyreadstat.read_xport(path)\n",
    "    df.columns = df.columns.str.upper()\n",
    "    \n",
    "    print(f\"\\n✅ Loaded {fname} ({years})\")\n",
    "    print(\"Shape:\", df.shape)\n",
    "    print(\"SEQN min:\", df['SEQN'].min(), \"SEQN max:\", df['SEQN'].max())\n",
    "    print(\"Unique SEQN:\", df['SEQN'].nunique())\n",
    "    print(\"First 5 SEQN:\", df['SEQN'].head().tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a4590c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first start with ocq \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pyreadstat\n",
    "\n",
    "# --- Paths ---\n",
    "ocq_dir = Path(\"/Users/dengshuyue/Desktop/SDOH/analysis/data/nhanes_by_module/ocq\")\n",
    "ocq_main_path = ocq_dir / \"ocq.sas7bdat\"                 # 2003–2018 consolidated\n",
    "ocq_early_paths = [(ocq_dir/\"OCQ.xpt\", 1), (ocq_dir/\"OCQ_B.xpt\", 2)]  # 1999–2002\n",
    "\n",
    "def recode_employment(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = df.columns.str.upper()\n",
    "    df[\"EMPLOY\"] = np.nan\n",
    "    if \"OCD150\" in df:\n",
    "        df.loc[df[\"OCD150\"] == 1, \"EMPLOY\"] = 1\n",
    "        df.loc[df[\"OCD150\"] == 3, \"EMPLOY\"] = 2\n",
    "    if \"OCQ380\" in df:\n",
    "        df.loc[df[\"OCQ380\"] == 5, \"EMPLOY\"] = 2\n",
    "        df.loc[df[\"OCQ380\"] == 3, \"EMPLOY\"] = 3\n",
    "        df.loc[df[\"OCQ380\"].isin([4, 6]), \"EMPLOY\"] = 4\n",
    "        df.loc[df[\"OCQ380\"].isin([1, 2, 7]), \"EMPLOY\"] = 5\n",
    "    df[\"UNEMPLOYMENT\"] = (df[\"EMPLOY\"] == 2).astype(int)\n",
    "    keep = [c for c in [\"SEQN\",\"EMPLOY\",\"UNEMPLOYMENT\",\"SDDSRVYR\"] if c in df.columns]\n",
    "    return df[keep]\n",
    "\n",
    "# --- A) Read early cycles (1999–2002) ---\n",
    "early_parts = []\n",
    "for p, cyc in ocq_early_paths:\n",
    "    if p.exists():\n",
    "        df, _ = pyreadstat.read_xport(str(p))\n",
    "        df.columns = df.columns.str.upper()\n",
    "        df[\"SDDSRVYR\"] = cyc\n",
    "        early_parts.append(recode_employment(df))\n",
    "    else:\n",
    "        print(f\"⚠️ Missing early OCQ file: {p}\")\n",
    "ocq_early = pd.concat(early_parts, ignore_index=True) if early_parts else pd.DataFrame()\n",
    "\n",
    "# --- B) Read main consolidated OCQ (2003–2018) from the SAME folder ---\n",
    "if not ocq_main_path.exists():\n",
    "    raise FileNotFoundError(f\"Main OCQ not found: {ocq_main_path}\")\n",
    "ocq_main_raw = pd.read_sas(str(ocq_main_path), format=\"sas7bdat\", encoding=\"latin1\")\n",
    "ocq_main_raw.columns = ocq_main_raw.columns.str.upper()\n",
    "\n",
    "# Optional adult filter if you have it defined\n",
    "try:\n",
    "    ocq_main_raw = filter_adults(ocq_main_raw)\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "# If SDDSRVYR isn't present in the main file, you can skip it or infer later via SEQN.\n",
    "ocq_main = recode_employment(ocq_main_raw)\n",
    "\n",
    "# --- C) Combine to final OCQ (1999–2018) ---\n",
    "ocq = pd.concat([ocq_early, ocq_main], ignore_index=True)\n",
    "\n",
    "# Hygiene\n",
    "ocq[\"SEQN\"] = pd.to_numeric(ocq[\"SEQN\"], errors=\"coerce\").astype(\"Int64\")\n",
    "if \"SDDSRVYR\" in ocq.columns:\n",
    "    ocq[\"SDDSRVYR\"] = pd.to_numeric(ocq[\"SDDSRVYR\"], errors=\"coerce\").astype(\"Int64\")\n",
    "ocq = ocq.dropna(subset=[\"SEQN\"]).drop_duplicates(subset=[\"SEQN\"])\n",
    "\n",
    "# --- D) Quick checks ---\n",
    "print(\"Final OCQ shape:\", ocq.shape)\n",
    "print(\"SEQN range:\", ocq[\"SEQN\"].min(), \"→\", ocq[\"SEQN\"].max())\n",
    "if \"SDDSRVYR\" in ocq.columns:\n",
    "    print(\"Cycles present:\\n\", ocq[\"SDDSRVYR\"].value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be472780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hoq (housing)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Paths ---\n",
    "hoq_dir = Path(\"/Users/dengshuyue/Desktop/SDOH/analysis/data/nhanes_by_module/hoq\")\n",
    "hoq_main_path = hoq_dir / \"hoq.sas7bdat\"  # your 2003–2018 consolidated file\n",
    "early_candidates = [\n",
    "    (hoq_dir / \"HOQ.xpt\",   1),  # 1999–2000\n",
    "    (hoq_dir / \"HOQ_B.xpt\", 2),  # 2001–2002\n",
    "    # if you happen to have SAS versions instead of XPTs, we’ll try those too:\n",
    "    (hoq_dir / \"hoq.sas7bdat\",   1),  # fallback for 99–00 (rare)\n",
    "    (hoq_dir / \"hoq_b.sas7bdat\", 2),\n",
    "]\n",
    "\n",
    "def read_any(path: Path) -> pd.DataFrame:\n",
    "    if path.suffix.lower() == \".xpt\":\n",
    "        import pyreadstat\n",
    "        df, _ = pyreadstat.read_xport(str(path))\n",
    "    elif path.suffix.lower() == \".sas7bdat\":\n",
    "        df = pd.read_sas(str(path), format=\"sas7bdat\", encoding=\"latin1\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file: {path}\")\n",
    "    df.columns = df.columns.str.upper()\n",
    "    return df\n",
    "\n",
    "def preprocess_hoq(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    # Clean HOQ065: set 7/9 to NaN where present\n",
    "    if \"HOQ065\" in df.columns:\n",
    "        df.loc[df[\"HOQ065\"].isin([7, 9]), \"HOQ065\"] = np.nan\n",
    "    # Keep the core columns that exist\n",
    "    keep = [c for c in [\"SEQN\", \"HOD050\", \"HOQ065\", \"SDDSRVYR\"] if c in df.columns]\n",
    "    return df[keep]\n",
    "\n",
    "# --- A) Early cycles (99–02) ---\n",
    "early_parts = []\n",
    "seen_cycles = set()\n",
    "for p, cyc in early_candidates:\n",
    "    if p.exists() and cyc not in seen_cycles:\n",
    "        df = read_any(p)\n",
    "        df[\"SDDSRVYR\"] = cyc\n",
    "        early_parts.append(preprocess_hoq(df))\n",
    "        seen_cycles.add(cyc)\n",
    "hoq_early = pd.concat(early_parts, ignore_index=True) if early_parts else pd.DataFrame()\n",
    "\n",
    "# --- B) Main consolidated HOQ (2003–2018) ---\n",
    "if not hoq_main_path.exists():\n",
    "    raise FileNotFoundError(f\"Main HOQ not found: {hoq_main_path}\")\n",
    "hoq_main_raw = read_any(hoq_main_path)\n",
    "# (Optional) If you have an adult filter function:\n",
    "try:\n",
    "    hoq_main_raw = filter_adults(hoq_main_raw)  # your function, if defined\n",
    "except NameError:\n",
    "    pass\n",
    "hoq_main = preprocess_hoq(hoq_main_raw)\n",
    "\n",
    "# --- C) Combine to final HOQ (1999–2018) ---\n",
    "hoq_all = pd.concat([hoq_early, hoq_main], ignore_index=True)\n",
    "\n",
    "# Hygiene\n",
    "if \"SEQN\" in hoq_all:\n",
    "    hoq_all[\"SEQN\"] = pd.to_numeric(hoq_all[\"SEQN\"], errors=\"coerce\").astype(\"Int64\")\n",
    "if \"SDDSRVYR\" in hoq_all:\n",
    "    hoq_all[\"SDDSRVYR\"] = pd.to_numeric(hoq_all[\"SDDSRVYR\"], errors=\"coerce\").astype(\"Int64\")\n",
    "hoq_all = hoq_all.dropna(subset=[\"SEQN\"]).drop_duplicates(subset=[\"SEQN\"])\n",
    "\n",
    "# --- D) Quick checks ---\n",
    "print(\"Final HOQ shape:\", hoq_all.shape)\n",
    "print(\"SEQN range:\", hoq_all[\"SEQN\"].min(), \"→\", hoq_all[\"SEQN\"].max())\n",
    "if \"SDDSRVYR\" in hoq_all:\n",
    "    cycle_map = {1:\"1999–2000\",2:\"2001–2002\",3:\"2003–2004\",4:\"2005–2006\",5:\"2007–2008\",\n",
    "                 6:\"2009–2010\",7:\"2011–2012\",8:\"2013–2014\",9:\"2015–2016\",10:\"2017–2018\"}\n",
    "    counts = hoq_all[\"SDDSRVYR\"].value_counts().sort_index()\n",
    "    print(\"Cycles present:\\n\", counts.rename(index=cycle_map))\n",
    "\n",
    "# (Optional) Save for reuse\n",
    "hoq_all.to_csv(\"/Users/dengshuyue/Desktop/SDOH/analysis/data/nhanes_by_module/hoq/hoq_99_18.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b977e570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hiqs \n",
    "\n",
    "# -----------------------------\n",
    "# Insurance (HIQ/HIQS) — add 1999–2002\n",
    "# -----------------------------\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "base_module_path = Path(\"/Users/dengshuyue/Desktop/SDOH/analysis/data/nhanes_by_module\")\n",
    "hiq_dir = base_module_path / \"hiq\"\n",
    "\n",
    "# Main consolidated later years (HIQS for 2003–2018)\n",
    "hiq_main_path = hiq_dir / \"hiqs.sas7bdat\"\n",
    "\n",
    "# Early cycles (prefer XPT; fallback to sas7bdat if that’s what you have)\n",
    "early_candidates = [\n",
    "    (hiq_dir / \"HIQ.xpt\",       1),  # 1999–2000\n",
    "    (hiq_dir / \"HIQ_B.xpt\",     2),  # 2001–2002\n",
    "    (hiq_dir / \"hiq.sas7bdat\",  1),  # fallback (rare)\n",
    "    (hiq_dir / \"hiq_b.sas7bdat\",2),\n",
    "]\n",
    "\n",
    "def read_any(p: Path) -> pd.DataFrame:\n",
    "    if p.suffix.lower() == \".xpt\":\n",
    "        import pyreadstat\n",
    "        df, _ = pyreadstat.read_xport(str(p))\n",
    "    elif p.suffix.lower() == \".sas7bdat\":\n",
    "        df = pd.read_sas(str(p), format=\"sas7bdat\", encoding=\"latin1\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file: {p}\")\n",
    "    df.columns = df.columns.str.upper()\n",
    "    return df\n",
    "\n",
    "# A) Early cycles (99–02)\n",
    "early_parts, seen = [], set()\n",
    "for p, cyc in early_candidates:\n",
    "    if p.exists() and cyc not in seen:\n",
    "        df = read_any(p)\n",
    "        df[\"SDDSRVYR\"] = cyc\n",
    "        early_parts.append(df)\n",
    "        seen.add(cyc)\n",
    "hiq_early_raw = pd.concat(early_parts, ignore_index=True) if early_parts else pd.DataFrame()\n",
    "\n",
    "# B) Main consolidated (03–18)\n",
    "if not hiq_main_path.exists():\n",
    "    raise FileNotFoundError(f\"HIQS file not found at: {hiq_main_path}\")\n",
    "hiq_main_raw = read_any(hiq_main_path)\n",
    "\n",
    "# C) Filter to adults using your helper\n",
    "hiq_early = filter_adults(hiq_early_raw) if not hiq_early_raw.empty else hiq_early_raw\n",
    "hiqs       = filter_adults(hiq_main_raw)\n",
    "\n",
    "# D) Stack early + main\n",
    "hiq_all = pd.concat([hiq_early, hiqs], ignore_index=True)\n",
    "hiq_all.columns = hiq_all.columns.str.upper()\n",
    "\n",
    "# E) Compute insurance 'ins' on the combined data (guards for varying columns)\n",
    "ins = pd.DataFrame({\"SEQN\": hiq_all[\"SEQN\"]})\n",
    "if \"SDDSRVYR\" in hiq_all.columns:\n",
    "    ins[\"SDDSRVYR\"] = hiq_all[\"SDDSRVYR\"]\n",
    "ins[\"ins\"] = np.nan\n",
    "\n",
    "# Private\n",
    "if \"HIQ031A\" in hiq_all: ins.loc[hiq_all[\"HIQ031A\"] == 14, \"ins\"] = 1\n",
    "if \"HID030A\" in hiq_all: ins.loc[hiq_all[\"HID030A\"] == 1,  \"ins\"] = 1\n",
    "\n",
    "# Medicare\n",
    "cond_med = False\n",
    "if {\"HIQ031B\",\"HIQ031D\",\"HIQ031E\"}.issubset(hiq_all.columns):\n",
    "    cond_med = (hiq_all[\"HIQ031B\"] == 15) & (hiq_all[\"HIQ031D\"] != 17) & (hiq_all[\"HIQ031E\"] != 18)\n",
    "if {\"HID030B\",\"HID030C\"}.issubset(hiq_all.columns):\n",
    "    cond_med = cond_med | ((hiq_all[\"HID030B\"] == 1) & (hiq_all[\"HID030C\"] != 1)) if isinstance(cond_med, pd.Series) else ((hiq_all[\"HID030B\"] == 1) & (hiq_all[\"HID030C\"] != 1))\n",
    "ins.loc[cond_med, \"ins\"] = 2\n",
    "\n",
    "# Medicaid\n",
    "cond_mcaid = False\n",
    "if {\"HIQ031B\",\"HIQ031D\",\"HIQ031E\"}.issubset(hiq_all.columns):\n",
    "    cond_mcaid = (((hiq_all[\"HIQ031D\"] == 17) | (hiq_all[\"HIQ031E\"] == 18)) & (hiq_all[\"HIQ031B\"] != 15))\n",
    "if {\"HID030B\",\"HID030C\"}.issubset(hiq_all.columns):\n",
    "    cond_mcaid = cond_mcaid | ((hiq_all[\"HID030B\"] != 1) & (hiq_all[\"HID030C\"] == 1)) if isinstance(cond_mcaid, pd.Series) else ((hiq_all[\"HID030B\"] != 1) & (hiq_all[\"HID030C\"] == 1))\n",
    "ins.loc[cond_mcaid, \"ins\"] = 3\n",
    "\n",
    "# Medicaid when both present\n",
    "if {\"HIQ031B\",\"HIQ031D\"}.issubset(hiq_all.columns):\n",
    "    ins.loc[(hiq_all[\"HIQ031B\"] == 15) & (hiq_all[\"HIQ031D\"] == 17), \"ins\"] = 3\n",
    "if {\"HID030B\",\"HID030C\"}.issubset(hiq_all.columns):\n",
    "    ins.loc[(hiq_all[\"HID030B\"] == 1) & (hiq_all[\"HID030C\"] == 1), \"ins\"] = 3\n",
    "\n",
    "# Other insurance\n",
    "other_cols = [c for c in [\"HIQ031C\",\"HIQ031F\",\"HIQ031G\",\"HIQ031H\",\"HIQ031I\"] if c in hiq_all.columns]\n",
    "cond_other = hiq_all[other_cols].eq(1).any(axis=1) if other_cols else False\n",
    "if \"HID030D\" in hiq_all:\n",
    "    cond_other = cond_other | (hiq_all[\"HID030D\"] == 1) if isinstance(cond_other, pd.Series) else (hiq_all[\"HID030D\"] == 1)\n",
    "ins.loc[cond_other, \"ins\"] = 5\n",
    "\n",
    "# No insurance\n",
    "conds_none = []\n",
    "if \"HIQ011\" in hiq_all: conds_none.append(hiq_all[\"HIQ011\"] == 2)\n",
    "if \"HID010\" in hiq_all: conds_none.append(hiq_all[\"HID010\"] == 2)\n",
    "if conds_none:\n",
    "    ins.loc[np.logical_or.reduce(conds_none), \"ins\"] = 0\n",
    "\n",
    "# Tidy up\n",
    "ins[\"SEQN\"] = pd.to_numeric(ins[\"SEQN\"], errors=\"coerce\").astype(\"Int64\")\n",
    "if \"SDDSRVYR\" in ins.columns:\n",
    "    ins[\"SDDSRVYR\"] = pd.to_numeric(ins[\"SDDSRVYR\"], errors=\"coerce\").astype(\"Int64\")\n",
    "ins = ins.dropna(subset=[\"SEQN\"]).drop_duplicates(subset=[\"SEQN\"])\n",
    "\n",
    "print(\"Insurance table shape:\", ins.shape)\n",
    "if \"SDDSRVYR\" in ins.columns:\n",
    "    print(\"Cycles present:\\n\", ins[\"SDDSRVYR\"].value_counts().sort_index())\n",
    "\n",
    "# print(ins[\"SEQN\"])\n",
    "\n",
    "# (Optional) Save for reuse\n",
    "ins.to_csv(\"/Users/dengshuyue/Desktop/SDOH/analysis/data/nhanes_by_module/hiq/ins_99_18.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d426bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fsq\n",
    "# -----------------------------\n",
    "# Step 6: SNAP & Food Security (FSQ/FSQS) — add 1999–2002\n",
    "# -----------------------------\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "base_module_path = Path(\"/Users/dengshuyue/Desktop/SDOH/analysis/data/nhanes_by_module\")\n",
    "fsq_dir = base_module_path / \"fsq\"\n",
    "\n",
    "# Main later-years file (FSQS for 2003–2018)\n",
    "fsq_main_path = fsq_dir / \"fsqs.sas7bdat\"\n",
    "\n",
    "# Early cycles (prefer XPT; fallback to sas7bdat if that’s what you have)\n",
    "early_candidates = [\n",
    "    (fsq_dir / \"FSQ.xpt\",         1),  # 1999–2000\n",
    "    (fsq_dir / \"FSQ_B.xpt\",       2),  # 2001–2002\n",
    "    (fsq_dir / \"fsq.sas7bdat\",    1),  # fallback (rare)\n",
    "    (fsq_dir / \"fsq_b.sas7bdat\",  2),\n",
    "]\n",
    "\n",
    "def read_any(p: Path) -> pd.DataFrame:\n",
    "    if p.suffix.lower() == \".xpt\":\n",
    "        import pyreadstat\n",
    "        df, _ = pyreadstat.read_xport(str(p))\n",
    "    elif p.suffix.lower() == \".sas7bdat\":\n",
    "        df = pd.read_sas(str(p), format=\"sas7bdat\", encoding=\"latin1\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file: {p}\")\n",
    "    df.columns = df.columns.str.upper()\n",
    "    return df\n",
    "\n",
    "# A) Early cycles (99–02)\n",
    "early_parts, seen = [], set()\n",
    "for p, cyc in early_candidates:\n",
    "    if p.exists() and cyc not in seen:\n",
    "        df = read_any(p)\n",
    "        df[\"SDDSRVYR\"] = cyc\n",
    "        early_parts.append(df)\n",
    "        seen.add(cyc)\n",
    "fsq_early_raw = pd.concat(early_parts, ignore_index=True) if early_parts else pd.DataFrame()\n",
    "\n",
    "# B) Main consolidated (03–18)\n",
    "if not fsq_main_path.exists():\n",
    "    raise FileNotFoundError(f\"FSQS file not found: {fsq_main_path}\")\n",
    "fsq_main_raw = read_any(fsq_main_path)\n",
    "\n",
    "# C) Filter to adults using your helper\n",
    "fsq_early = filter_adults(fsq_early_raw) if not fsq_early_raw.empty else fsq_early_raw\n",
    "fsqs       = filter_adults(fsq_main_raw)\n",
    "\n",
    "# D) Stack early + main\n",
    "fsq_all = pd.concat([fsq_early, fsqs], ignore_index=True)\n",
    "fsq_all.columns = fsq_all.columns.str.upper()\n",
    "\n",
    "# E) Build SNAP/FS outputs (guards for varying columns)\n",
    "snap = pd.DataFrame({\"SEQN\": fsq_all[\"SEQN\"]})\n",
    "if \"SDDSRVYR\" in fsq_all: snap[\"SDDSRVYR\"] = fsq_all[\"SDDSRVYR\"]\n",
    "if \"FSDHH\" in fsq_all:   snap[\"FSDHH\"] = fsq_all[\"FSDHH\"]\n",
    "\n",
    "snap[\"SNAP\"] = np.nan\n",
    "if \"FSQ165\" in fsq_all: snap.loc[fsq_all[\"FSQ165\"] == 2, \"SNAP\"] = 0\n",
    "if \"FSQ012\" in fsq_all:\n",
    "    snap.loc[fsq_all[\"FSQ012\"] == 1, \"SNAP\"] = 1\n",
    "    snap.loc[fsq_all[\"FSQ012\"] == 2, \"SNAP\"] = 0\n",
    "if \"FSQ171\" in fsq_all:\n",
    "    snap.loc[fsq_all[\"FSQ171\"] == 1, \"SNAP\"] = 1\n",
    "    snap.loc[fsq_all[\"FSQ171\"] == 2, \"SNAP\"] = 0\n",
    "if \"FSD170N\" in fsq_all: snap.loc[fsq_all[\"FSD170N\"] >= 1, \"SNAP\"] = 1\n",
    "if \"FSQ170\" in fsq_all:\n",
    "    snap.loc[fsq_all[\"FSQ170\"] == 1, \"SNAP\"] = 1\n",
    "    snap.loc[(fsq_all[\"FSQ170\"] == 2) & (fsq_all.get(\"FSD170N\", pd.Series(index=fsq_all.index)) < 1), \"SNAP\"] = 0\n",
    "if \"FSD200\" in fsq_all: snap.loc[fsq_all[\"FSD200\"] == 1, \"SNAP\"] = 1\n",
    "\n",
    "snap[\"FS\"] = np.nan\n",
    "if \"FSDHH\" in fsq_all:\n",
    "    snap.loc[fsq_all[\"FSDHH\"].isin([1, 2]), \"FS\"] = 1\n",
    "    snap.loc[fsq_all[\"FSDHH\"] > 2, \"FS\"] = 0\n",
    "\n",
    "# Final columns\n",
    "snap = snap[[c for c in [\"SEQN\", \"SNAP\", \"FSDHH\", \"FS\", \"SDDSRVYR\"] if c in snap.columns]]\n",
    "\n",
    "# Hygiene\n",
    "snap[\"SEQN\"] = pd.to_numeric(snap[\"SEQN\"], errors=\"coerce\").astype(\"Int64\")\n",
    "if \"SDDSRVYR\" in snap.columns:\n",
    "    snap[\"SDDSRVYR\"] = pd.to_numeric(snap[\"SDDSRVYR\"], errors=\"coerce\").astype(\"Int64\")\n",
    "snap = snap.dropna(subset=[\"SEQN\"]).drop_duplicates(subset=[\"SEQN\"])\n",
    "\n",
    "# Quick checks\n",
    "print(\"SNAP table shape:\", snap.shape)\n",
    "if \"SDDSRVYR\" in snap.columns:\n",
    "    print(\"Cycles present:\\n\", snap[\"SDDSRVYR\"].value_counts().sort_index())\n",
    "\n",
    "print(snap[\"SEQN\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82abd698",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  SEQN check — i.scores, covar, covariates, gg (dietwt), mortality9918\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Set your folder\n",
    "folder_path = \"/Users/dengshuyue/Desktop/SDOH/analysis/data/less_important\"\n",
    "\n",
    "def _standardize_seqn(df):\n",
    "    # make columns case-insensitive and return a Series of SEQN (nullable Int64)\n",
    "    df.columns = df.columns.str.upper()\n",
    "    if \"SEQN\" not in df.columns:\n",
    "        return None\n",
    "    s = pd.to_numeric(df[\"SEQN\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    return s\n",
    "\n",
    "def summarize_seqn(name, seqn):\n",
    "    print(f\"\\n{name}\")\n",
    "    if seqn is None:\n",
    "        print(\"  ❌ SEQN column not found\")\n",
    "        return\n",
    "    print(f\"  Rows: {len(seqn)}\")\n",
    "    print(f\"  Null SEQN: {seqn.isna().sum()}\")\n",
    "    if seqn.notna().any():\n",
    "        print(f\"  dtype: {seqn.dtype}\")\n",
    "        print(f\"  Min / Max: {seqn.min()} / {seqn.max()}\")\n",
    "        nunq = seqn.nunique(dropna=True)\n",
    "        dups = (seqn.notna().sum() - nunq)\n",
    "        print(f\"  Unique SEQN: {nunq}  |  Duplicates: {dups}\")\n",
    "        # small sample\n",
    "        print(f\"  Head SEQN: {list(seqn.dropna().head(5))}\")\n",
    "        print(f\"  Tail SEQN: {list(seqn.dropna().tail(5))}\")\n",
    "\n",
    "# 1) i.scores.xlsx\n",
    "try:\n",
    "    scores = pd.read_excel(os.path.join(folder_path, \"i.scores.xlsx\"), engine=\"openpyxl\")\n",
    "    # handle seqn casing\n",
    "    if \"seqn\" in scores.columns and \"SEQN\" not in scores.columns:\n",
    "        scores = scores.rename(columns={\"seqn\": \"SEQN\"})\n",
    "    summarize_seqn(\"i.scores.xlsx\", _standardize_seqn(scores))\n",
    "except Exception as e:\n",
    "    print(\"\\n[i.scores.xlsx] ⚠️\", e)\n",
    "\n",
    "# 2) covar.sas7bdat\n",
    "try:\n",
    "    covar = pd.read_sas(os.path.join(folder_path, \"covar.sas7bdat\"),\n",
    "                        format=\"sas7bdat\", encoding=\"latin1\")\n",
    "    summarize_seqn(\"covar.sas7bdat\", _standardize_seqn(covar))\n",
    "except Exception as e:\n",
    "    print(\"\\n[covar.sas7bdat] ⚠️\", e)\n",
    "\n",
    "# 3) covariates.csv\n",
    "try:\n",
    "    covariates1_raw = pd.read_csv(os.path.join(folder_path, \"covariates.csv\"))\n",
    "    if \"seqn\" in covariates1_raw.columns and \"SEQN\" not in covariates1_raw.columns:\n",
    "        covariates1_raw = covariates1_raw.rename(columns={\"seqn\": \"SEQN\"})\n",
    "    summarize_seqn(\"covariates.csv\", _standardize_seqn(covariates1_raw))\n",
    "except Exception as e:\n",
    "    print(\"\\n[covariates.csv] ⚠️\", e)\n",
    "\n",
    "# 4) gg.sas7bdat (diet weights)\n",
    "try:\n",
    "    dietwt = pd.read_sas(os.path.join(folder_path, \"gg.sas7bdat\"),\n",
    "                         format=\"sas7bdat\", encoding=\"latin1\")\n",
    "    summarize_seqn(\"gg.sas7bdat\", _standardize_seqn(dietwt))\n",
    "except Exception as e:\n",
    "    print(\"\\n[gg.sas7bdat] ⚠️\", e)\n",
    "\n",
    "# 5) mortality9918.sas7bdat\n",
    "try:\n",
    "    mort = pd.read_sas(os.path.join(folder_path, \"mortality9918.sas7bdat\"),\n",
    "                       format=\"sas7bdat\", encoding=\"latin1\")\n",
    "    summarize_seqn(\"mortality9918.sas7bdat\", _standardize_seqn(mort))\n",
    "except Exception as e:\n",
    "    print(\"\\n[mortality9918.sas7bdat] ⚠️\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56f5761",
   "metadata": {},
   "outputs": [],
   "source": [
    "SODH_diet_mort = pd.read_pickle(os.path.join(folder_path, \"SODH_diet_mort.pkl\"))\n",
    "\n",
    "score_mort.to_csv(\"/Users/dengshuyue/Desktop/SDOH/analysis/data/SODH_diet_mort_depr2.csv\", index=False)\n",
    "\n",
    "# Number of rows\n",
    "total_rows = SODH_diet_mort.shape[0]\n",
    "\n",
    "# Number of unique SEQN values\n",
    "unique_ids = SODH_diet_mort['SEQN'].nunique()\n",
    "\n",
    "print(f\" Total rows: {total_rows}\")\n",
    "print(f\" Unique SEQN values: {unique_ids}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c44012",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the .pkl file\n",
    "pkl_path = \"/Users/dengshuyue/Desktop/SDOH/analysis/data/SODH_diet_mort.pkl\"\n",
    "df = pd.read_pickle(pkl_path)\n",
    "\n",
    "# Save as .csv\n",
    "csv_path = \"/Users/dengshuyue/Desktop/SDOH/analysis/data/SODH_diet_mort.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(\"✅ Conversion complete: .pkl → .csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
