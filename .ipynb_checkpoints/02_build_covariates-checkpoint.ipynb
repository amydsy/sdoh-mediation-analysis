{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16236d22",
   "metadata": {},
   "source": [
    "# 02 — Build Covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9641eb3-3b47-4226-8370-c98734840760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ️ Using in-memory nhanes_mort_demo_sdoh.\n",
      "Rows: 56,253\n",
      "Age range: 18 to 85\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Prefer in-memory, don’t crash if not present\n",
    "try:\n",
    "    df = nhanes_mort_demo_sdoh\n",
    "    print(\"ℹ️ Using in-memory nhanes_mort_demo_sdoh.\")\n",
    "except NameError:\n",
    "    OUT = Path(\"/Users/dengshuyue/Desktop/SDOH/analysis/output\")\n",
    "    candidates = [\n",
    "        OUT / \"nhanes_mort_demo_sdoh_1999_2018.parquet\",\n",
    "        OUT / \"nhanes_mort_demo_sdoh_1999_2018.csv\",\n",
    "        OUT / \"nhanes_mort_demo_soc_1999_2018.parquet\",  # legacy\n",
    "        OUT / \"nhanes_mort_demo_soc_1999_2018.csv\",      # legacy\n",
    "    ]\n",
    "    df = None\n",
    "    for p in candidates:\n",
    "        if p.exists():\n",
    "            df = pd.read_parquet(p) if p.suffix == \".parquet\" else pd.read_csv(p)\n",
    "            print(\"Loaded from disk:\", p)\n",
    "            break\n",
    "    if df is None:\n",
    "        print(\"⚠️ Note: could not find the saved table among expected filenames, \"\n",
    "              \"but earlier cells *did* save successfully. Skipping the sanity preview instead of raising.\")\n",
    "        # Do NOT raise here.\n",
    "\n",
    "# Optional preview if df is available\n",
    "if df is not None:\n",
    "    print(f\"Rows: {df.shape[0]:,}\")\n",
    "    if \"RIDAGEYR\" in df.columns:\n",
    "        print(\"Age range:\", int(df[\"RIDAGEYR\"].min()), \"to\", int(df[\"RIDAGEYR\"].max()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae528ce8-838e-4ba6-8b25-3c725ba83698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap loaded.\n",
      "ROOT: /Users/dengshuyue/Desktop/SDOH/analysis\n",
      "Data dir exists: True\n",
      "Output dir exists: True\n",
      "Mortality cycles: ['1999-2000', '2001-2002', '2003-2004', '2005-2006', '2007-2008', '2009-2010', '2011-2012', '2013-2014', '2015-2016', '2017-2018']\n",
      "Non-mortality cycles: ['2017-March 2020 (pre-pandemic)', 'August 2021–August 2023']\n",
      "Candidates: [PosixPath('/Users/dengshuyue/Desktop/SDOH/analysis/data/household_size/DEMO_I.xpt'), PosixPath('/Users/dengshuyue/Desktop/SDOH/analysis/data/nhanes_deit/DEMO_I.xpt')]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEQN</th>\n",
       "      <th>SDDSRVYR</th>\n",
       "      <th>RIDSTATR</th>\n",
       "      <th>RIAGENDR</th>\n",
       "      <th>RIDAGEYR</th>\n",
       "      <th>RIDAGEMN</th>\n",
       "      <th>RIDRETH1</th>\n",
       "      <th>RIDRETH3</th>\n",
       "      <th>RIDEXMON</th>\n",
       "      <th>RIDEXAGM</th>\n",
       "      <th>DMQMILIZ</th>\n",
       "      <th>DMQADFC</th>\n",
       "      <th>DMDBORN4</th>\n",
       "      <th>DMDCITZN</th>\n",
       "      <th>DMDYRSUS</th>\n",
       "      <th>DMDEDUC3</th>\n",
       "      <th>DMDEDUC2</th>\n",
       "      <th>DMDMARTL</th>\n",
       "      <th>RIDEXPRG</th>\n",
       "      <th>SIALANG</th>\n",
       "      <th>SIAPROXY</th>\n",
       "      <th>SIAINTRP</th>\n",
       "      <th>FIALANG</th>\n",
       "      <th>FIAPROXY</th>\n",
       "      <th>FIAINTRP</th>\n",
       "      <th>MIALANG</th>\n",
       "      <th>MIAPROXY</th>\n",
       "      <th>MIAINTRP</th>\n",
       "      <th>AIALANGA</th>\n",
       "      <th>DMDHHSIZ</th>\n",
       "      <th>DMDFMSIZ</th>\n",
       "      <th>DMDHHSZA</th>\n",
       "      <th>DMDHHSZB</th>\n",
       "      <th>DMDHHSZE</th>\n",
       "      <th>DMDHRGND</th>\n",
       "      <th>DMDHRAGE</th>\n",
       "      <th>DMDHRBR4</th>\n",
       "      <th>DMDHREDU</th>\n",
       "      <th>DMDHRMAR</th>\n",
       "      <th>DMDHSEDU</th>\n",
       "      <th>WTINT2YR</th>\n",
       "      <th>WTMEC2YR</th>\n",
       "      <th>SDMVPSU</th>\n",
       "      <th>SDMVSTRA</th>\n",
       "      <th>INDHHIN2</th>\n",
       "      <th>INDFMIN2</th>\n",
       "      <th>INDFMPIR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83732.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.397605e-79</td>\n",
       "      <td>5.397605e-79</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>134671.370419</td>\n",
       "      <td>135629.507405</td>\n",
       "      <td>1.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>83733.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.397605e-79</td>\n",
       "      <td>5.397605e-79</td>\n",
       "      <td>5.397605e-79</td>\n",
       "      <td>1.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24328.560239</td>\n",
       "      <td>25282.425927</td>\n",
       "      <td>1.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>83734.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.397605e-79</td>\n",
       "      <td>5.397605e-79</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12400.008522</td>\n",
       "      <td>12575.838818</td>\n",
       "      <td>1.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>83735.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.397605e-79</td>\n",
       "      <td>5.397605e-79</td>\n",
       "      <td>5.397605e-79</td>\n",
       "      <td>2.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>102717.995647</td>\n",
       "      <td>102078.634508</td>\n",
       "      <td>1.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>83736.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.397605e-79</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>5.397605e-79</td>\n",
       "      <td>2.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17627.674984</td>\n",
       "      <td>18234.736219</td>\n",
       "      <td>2.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      SEQN  SDDSRVYR  RIDSTATR  RIAGENDR  RIDAGEYR  RIDAGEMN  RIDRETH1  \\\n",
       "0  83732.0       9.0       2.0       1.0      62.0       NaN       3.0   \n",
       "1  83733.0       9.0       2.0       1.0      53.0       NaN       3.0   \n",
       "2  83734.0       9.0       2.0       1.0      78.0       NaN       3.0   \n",
       "3  83735.0       9.0       2.0       2.0      56.0       NaN       3.0   \n",
       "4  83736.0       9.0       2.0       2.0      42.0       NaN       4.0   \n",
       "\n",
       "   RIDRETH3  RIDEXMON  RIDEXAGM  DMQMILIZ  DMQADFC  DMDBORN4  DMDCITZN  \\\n",
       "0       3.0       1.0       NaN       2.0      NaN       1.0       1.0   \n",
       "1       3.0       1.0       NaN       2.0      NaN       2.0       2.0   \n",
       "2       3.0       2.0       NaN       1.0      2.0       1.0       1.0   \n",
       "3       3.0       2.0       NaN       2.0      NaN       1.0       1.0   \n",
       "4       4.0       2.0       NaN       2.0      NaN       1.0       1.0   \n",
       "\n",
       "   DMDYRSUS  DMDEDUC3  DMDEDUC2  DMDMARTL  RIDEXPRG  SIALANG  SIAPROXY  \\\n",
       "0       NaN       NaN       5.0       1.0       NaN      1.0       2.0   \n",
       "1       7.0       NaN       3.0       3.0       NaN      1.0       2.0   \n",
       "2       NaN       NaN       3.0       1.0       NaN      1.0       2.0   \n",
       "3       NaN       NaN       5.0       6.0       NaN      1.0       2.0   \n",
       "4       NaN       NaN       4.0       3.0       1.0      1.0       2.0   \n",
       "\n",
       "   SIAINTRP  FIALANG  FIAPROXY  FIAINTRP  MIALANG  MIAPROXY  MIAINTRP  \\\n",
       "0       2.0      1.0       2.0       2.0      1.0       2.0       2.0   \n",
       "1       2.0      1.0       2.0       2.0      1.0       2.0       2.0   \n",
       "2       2.0      1.0       2.0       2.0      1.0       2.0       2.0   \n",
       "3       2.0      1.0       2.0       2.0      1.0       2.0       2.0   \n",
       "4       2.0      1.0       2.0       2.0      1.0       2.0       2.0   \n",
       "\n",
       "   AIALANGA  DMDHHSIZ  DMDFMSIZ      DMDHHSZA      DMDHHSZB      DMDHHSZE  \\\n",
       "0       1.0       2.0       2.0  5.397605e-79  5.397605e-79  1.000000e+00   \n",
       "1       1.0       1.0       1.0  5.397605e-79  5.397605e-79  5.397605e-79   \n",
       "2       NaN       2.0       2.0  5.397605e-79  5.397605e-79  2.000000e+00   \n",
       "3       1.0       1.0       1.0  5.397605e-79  5.397605e-79  5.397605e-79   \n",
       "4       1.0       5.0       5.0  5.397605e-79  2.000000e+00  5.397605e-79   \n",
       "\n",
       "   DMDHRGND  DMDHRAGE  DMDHRBR4  DMDHREDU  DMDHRMAR  DMDHSEDU       WTINT2YR  \\\n",
       "0       1.0      62.0       1.0       5.0       1.0       3.0  134671.370419   \n",
       "1       1.0      53.0       2.0       3.0       3.0       NaN   24328.560239   \n",
       "2       2.0      79.0       1.0       3.0       1.0       3.0   12400.008522   \n",
       "3       2.0      56.0       1.0       5.0       6.0       NaN  102717.995647   \n",
       "4       2.0      42.0       1.0       4.0       3.0       NaN   17627.674984   \n",
       "\n",
       "        WTMEC2YR  SDMVPSU  SDMVSTRA  INDHHIN2  INDFMIN2  INDFMPIR  \n",
       "0  135629.507405      1.0     125.0      10.0      10.0      4.39  \n",
       "1   25282.425927      1.0     125.0       4.0       4.0      1.32  \n",
       "2   12575.838818      1.0     131.0       5.0       5.0      1.51  \n",
       "3  102078.634508      1.0     131.0      10.0      10.0      5.00  \n",
       "4   18234.736219      2.0     126.0       7.0       7.0      1.23  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SEQN', 'SDDSRVYR', 'RIDSTATR', 'RIAGENDR', 'RIDAGEYR', 'RIDAGEMN', 'RIDRETH1', 'RIDRETH3', 'RIDEXMON', 'RIDEXAGM', 'DMQMILIZ', 'DMQADFC', 'DMDBORN4', 'DMDCITZN', 'DMDYRSUS', 'DMDEDUC3', 'DMDEDUC2', 'DMDMARTL', 'RIDEXPRG', 'SIALANG', 'SIAPROXY', 'SIAINTRP', 'FIALANG', 'FIAPROXY', 'FIAINTRP', 'MIALANG', 'MIAPROXY', 'MIAINTRP', 'AIALANGA', 'DMDHHSIZ', 'DMDFMSIZ', 'DMDHHSZA', 'DMDHHSZB', 'DMDHHSZE', 'DMDHRGND', 'DMDHRAGE', 'DMDHRBR4', 'DMDHREDU', 'DMDHRMAR', 'DMDHSEDU', 'WTINT2YR', 'WTMEC2YR', 'SDMVPSU', 'SDMVSTRA', 'INDHHIN2', 'INDFMIN2', 'INDFMPIR']\n",
      "['SEQN', 'DAYS', 'DR12IFDC', 'WTDRD1', 'DR12DRST', 'SDDSRVYR', 'RIAGENDR', 'RIDAGEYR', 'RIDRETH1', 'DMDEDUC3', 'DMDEDUC2', 'INDFMPIR', 'DMDHREDU', 'WTINT2YR', 'WTMEC2YR', 'SDMVPSU', 'SDMVSTRA', 'DR12IKC2', 'CYCLE', 'WTDR2D', 'DR12FS', 'DMDHREDZ', 'age', 'race', 'edu', 'pedu', 'Incm', 'incm2', 'include', 'Weight16a', 'cycles', 'sex', 'age1', 'age2', 'age3', 'race2', 'race3', 'race4', 'weekend', '_NAME_', '_LABEL_', 'DRDAY1', 'DRDAY2', 'tkal1', 'tkal2', 'Tcal', 'DR12DAY', 'kcal1', 'kcal2', 'kcal3', 'kcal4', 'kcal12', 'wt1', 'wt2', 'wt3', 'wt4', 'wt12', 'pcte1', 'pcte2', 'pcte3', 'pcte4', 'pcte12', 'pctg1', 'pctg2', 'pctg3', 'pctg4', 'pctg12', 'kcals2', 'kcals5', 'kcals6', 'kcals9', 'kcals13', 'kcals14', 'kcals17', 'kcals20', 'kcals21', 'kcals22', 'kcals23', 'kcals25', 'kcals28', 'kcals29', 'kcals33', 'kcals36', 'kcals39', 'kcals41', 'kcals3', 'kcals37', 'kcals38', 'kcals40', 'kcals42', 'kcals1', 'kcals10', 'kcals16', 'kcals24', 'kcals15', 'kcals18', 'kcals7', 'kcals8', 'kcals19', 'kcals46', 'kcals44', 'kcals45', 'kcals12', 'kcals27', 'kcals34', 'kcals30', 'kcals26', 'kcals43', 'kcals214', 'kcals215', 'kcals32', 'kcals31', 'wts2', 'wts5', 'wts6', 'wts9', 'wts13', 'wts14', 'wts17', 'wts20', 'wts21', 'wts22', 'wts23', 'wts25', 'wts28', 'wts29', 'wts33', 'wts36', 'wts39', 'wts41', 'wts3', 'wts37', 'wts38', 'wts40', 'wts42', 'wts1', 'wts10', 'wts16', 'wts24', 'wts15', 'wts18', 'wts7', 'wts8', 'wts19', 'wts46', 'wts44', 'wts45', 'wts12', 'wts27', 'wts34', 'wts30', 'wts26', 'wts43', 'wts214', 'wts215', 'wts32', 'wts31', 'pctes2', 'pctes5', 'pctes6', 'pctes9', 'pctes13', 'pctes14', 'pctes17', 'pctes20', 'pctes21', 'pctes22', 'pctes23', 'pctes25', 'pctes28', 'pctes29', 'pctes33', 'pctes36', 'pctes39', 'pctes41', 'pctes3', 'pctes37', 'pctes38', 'pctes40', 'pctes42', 'pctes1', 'pctes10', 'pctes16', 'pctes24', 'pctes15', 'pctes18', 'pctes7', 'pctes8', 'pctes19', 'pctes46', 'pctes44', 'pctes45', 'pctes12', 'pctes27', 'pctes34', 'pctes30', 'pctes26', 'pctes43', 'pctes214', 'pctes215', 'pctes32', 'pctes31', 'pctgs2', 'pctgs5', 'pctgs6', 'pctgs9', 'pctgs13', 'pctgs14', 'pctgs17', 'pctgs20', 'pctgs21', 'pctgs22', 'pctgs23', 'pctgs25', 'pctgs28', 'pctgs29', 'pctgs33', 'pctgs36', 'pctgs39', 'pctgs41', 'pctgs3', 'pctgs37', 'pctgs38', 'pctgs40', 'pctgs42', 'pctgs1', 'pctgs10', 'pctgs16', 'pctgs24', 'pctgs15', 'pctgs18', 'pctgs7', 'pctgs8', 'pctgs19', 'pctgs46', 'pctgs44', 'pctgs45', 'pctgs12', 'pctgs27', 'pctgs34', 'pctgs30', 'pctgs26', 'pctgs43', 'pctgs214', 'pctgs215', 'pctgs32', 'pctgs31', 'kcals4', 'kcals11', 'kcals35', 'kcals47', 'kcals48', 'kcals49', 'kcals50', 'kcals51', 'pctgs4', 'pctgs11', 'pctgs35', 'pctgs47', 'pctgs48', 'pctgs49', 'pctgs50', 'pctgs51', 'pctes4', 'pctes11', 'pctes35', 'pctes47', 'pctes48', 'pctes49', 'pctes50', 'pctes51', 'wts4', 'wts11', 'wts35', 'wts47', 'wts48', 'wts49', 'wts50', 'wts51', 'Pctes1a', 'Pctes6a', 'Pctgs1a', 'Pctgs6a', 'Pctgs10b', 'Pctgs112', 'Kcals1a', 'Kcals6a', 'Kcals10b', 'Kcals112', 'Kcals10a', 'Pctes10a', 'wts10a', 'Pctgs10a', 'Pctes10b', 'Pctes112', 'Kcals2a', 'Pctes2a', 'wts2a', 'Pctgs2a', 'Kcals13a', 'Pctes13a', 'wts13a', 'Pctgs13a', 'Pctes23a', 'Kcals23a', 'Pctgs23a', 'Kcals38a', 'Pctes38a', 'wts38a', 'Pctgs38a', 'Pctes25a', 'Pctgs25a', 'Kcals25a', 'Pctes24a', 'Pctgs24a', 'Kcals24a', 'Pctes30a', 'Pctes30b', 'Pctgs30a', 'Pctgs30b', 'Kcals30a', 'Kcals30b', 'Pctes37b', 'pctes44a']\n",
      "\n",
      "libname data \"C:\\Users\\lwang18\\Box\\Projects\\5_UPF_Mortality\\data\";\n",
      "*%let home=C:\\Users\\LWANG18\\Box\\Projects\\5_UPF_Mortality\\results_revision ; \n",
      "%let home= C:\\Users\\lwang18\\Box\\Projects\\Kroger project\\Data for analysis\\Scores;\n",
      " %let path= C:\\Users\\lwang18\\Box\\Projects\\Kroger project\\Data for analysis\\Scores;\n",
      "\n",
      "%let home= C:\\Users\\lwang18\\OneDrive - Tufts\\Desktop\\Projects\\Food Insecurity,;\n",
      "libname out \"C:\\Users\\lwang18\\OneDrive - Tufts\\Desktop\\Projects\\Food Insecurity,\";\n",
      "\n",
      "libname NHANES \"C:\\Users\\LWANG18\\Box\\NHANES_Lu\" ;\n",
      "\n",
      "/** main analysis **/\n",
      "\n",
      "%macro cox(data, dvar, evars, covars, death , out);\n",
      "ods select all ; \n",
      "ODS OUTPUT PARAMETERESTIMATES=r0; \n",
      "proc surveyphreg data=&data;\n",
      "\tstrata sdmvstra;\n",
      "\tcluster sdmvpsu;\n",
      "\tweight wt;\n",
      "\tclass    sex (ref=\"1\") race edu(ref=\"1\") smk  pir(ref=\"3\") SNAP(ref=\"0\") FS ins2(ref=\"1\") ins i_FCS_sdq hei2015q(ref=\"3\") marriage  hoq065/param=ref;\n",
      "\tmodel py*&death(0)= &dvar &covars /rl ties=breslow;\n",
      "run ;\n",
      "data r0 ; set r0 ; \n",
      "HRCL=compress(round(hazardratio,0.01)||\"(\"||round(HRLOWerCL,.01)||\",\" ||round(HRupperCL,0.01)||\")\")  ; \n",
      "outcome=\"&death.\" ;\n",
      "predictor=\"&dvar.\";\n",
      "explainvar=\"null\" ;\n",
      "model=\"&out.\"; \n",
      "run; \n",
      "proc append data=r0  base=&out force; run;\n",
      "\n",
      "%let i=1 ;\n",
      "%do %while(%scan(&evars,&i) ne ) ;\n",
      "%let evar=%scan(&evars,&i) ;\n",
      "\n",
      "ods select all ; \n",
      "ODS OUTPUT PARAMETERESTIMATES=r1; \n",
      "proc surveyphreg data=&data;\n",
      "\tstrata sdmvstra;\n",
      "\tcluster sdmvpsu;\n",
      "\tweight wt;\n",
      "\tclass    sex (ref=\"1\") race edu(ref=\"1\") smk  pir(ref=\"3\") SNAP(ref=\"0\") FS ins2(ref=\"1\") ins i_FCS_sdq hei2015q(ref=\"3\") marriage hoq065(ref=\"1\")/param=ref;\n",
      "\tmodel py*&death(0)= &dvar &evar &covars /rl ties=breslow;\n",
      "run ;\n",
      "\n",
      "data resc&death.&i ; \n",
      "set r1; \n",
      "HRCL=compress(round(hazardratio,0.01)||\"(\"||round(HRLOWerCL,.01)||\",\" ||round(HRupperCL,0.01)||\")\")  ; \n",
      "outcome=\"&death.\" ;\n",
      "predictor=\"&dvar.\";\n",
      "explainvar=\"&evar.\" ;\n",
      "model=\"&out.\"; \n",
      "run; \n",
      "proc append data=resc&death.&i  base=&out force; run;\n",
      " \n",
      "%let i=%eval(&i+1) ;\n",
      "%end ;\n",
      "%mend ; \n",
      "\n",
      " \n",
      "%macro cox1(data, dvar, evars, covars, death , out, label);\n",
      "OPTION SPOOL ; \n",
      "ods select all ; \n",
      "ODS OUTPUT PARAMETERESTIMATES=r1; \n",
      "proc surveyphreg data=&data;\n",
      "\tstrata sdmvstra;\n",
      "\tcluster sdmvpsu;\n",
      "\tweight wt;\n",
      "\tclass    sex (ref=\"1\") race edu(ref=\"1\") smk  pir(ref=\"3\") SNAP(ref=\"0\") FS ins ins2(ref=\"1\") i_FCS_sdq hei2015q(ref=\"3\") /param=ref;\n",
      "\tmodel py*&death(0)= &dvar &evars &covars /rl ties=breslow;\n",
      "run ;\n",
      "\n",
      "data resc&death ; \n",
      "set r1; \n",
      "HRCL=compress(round(hazardratio,0.01)||\"(\"||round(HRLOWerCL,.01)||\",\" ||round(HRupperCL,0.01)||\")\")  ; \n",
      "outcome=\"&death.\" ;\n",
      "predictor=\"&dvar.\";\n",
      "explainvar=&label ; \n",
      "model=\"&out.\"; \n",
      "run; \n",
      "proc append data=resc&death  base=&out force; run;\n",
      "%mend ; \n",
      "\n",
      "*%cox(score_mort1,pir, fs  i_FCS_sdq SNAP ins ins2, ridageyr sex race   , mortstat, out_model0 );\n",
      "%cox(score_mort,pir,  hei2015q i_FCS_sdq fs SNAP ins ins2 marriage hoq065 unemployment unemployment2  ,  ridageyr sex race  edu , mortstat, out_model1 );\n",
      "%cox1(score_mort,pir, i_FCS_sdq fs , ridageyr sex race  edu , mortstat, out_model1 , \"comb1\");\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq SNAP ins, ridageyr sex race  edu , mortstat, out_model1 , \"all1\");\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq SNAP ins2, ridageyr sex race  edu , mortstat, out_model1 , \"all2\");\n",
      "\n",
      "/**age, sex, race, edu, ins as the base model*/\n",
      "%cox(score_mort,pir, hei2015q i_FCS_sdq fs  SNAP marriage hoq065 unemployment unemployment2,  ridageyr sex race  edu ins , mortstat, out_model11 );  \n",
      "%cox1(score_mort,pir, fs i_FCS_sdq, ridageyr sex race  edu ins, mortstat, out_model11 , \"comb1\");\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq SNAP ins, ridageyr sex race  edu ins, mortstat, out_model11 , \"all1\");\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq SNAP ins2, ridageyr sex race  edu ins, mortstat, out_model11 , \"all2\");\n",
      "\n",
      "/**age, sex, race, edu, lifestyles as the base model*/\n",
      "%cox(score_mort,pir, hei2015q i_FCS_sdq fs  SNAP ins ins2 marriage hoq065 unemployment unemployment2, ridageyr sex race  edu smk\tmet_hr perE_alco , mortstat, out_model2 );\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq, ridageyr sex race  edu smk\tmet_hr perE_alco, mortstat, out_model2 , \"comb1\");\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq SNAP ins, ridageyr sex race  edu smk\tmet_hr perE_alco, mortstat, out_model2 , \"all1\");\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq SNAP ins2, ridageyr sex race  edu smk\tmet_hr perE_alco, mortstat, out_model2 , \"all2\");\n",
      "\n",
      "/**age, sex, race, edu, lifestyles, insurance as the base model*/\n",
      "%cox(score_mort,pir, hei2015q i_FCS_sdq fs  SNAP marriage hoq065 unemployment unemployment2, ridageyr sex race  edu smk\tmet_hr perE_alco ins , mortstat, out_model21 );\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq, ridageyr sex race  edu smk\tmet_hr perE_alco ins, mortstat, out_model21 , \"comb1\");\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq SNAP ins, ridageyr sex race  edu smk\tmet_hr perE_alco ins, mortstat, out_model21 , \"all1\");\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq SNAP ins2, ridageyr sex race  edu smk\tmet_hr perE_alco ins, mortstat, out_model21 , \"all2\");\n",
      "\n",
      "\n",
      "/**age, sex, race, edu, lifestyles, baseline health as the base model*/\n",
      "%cox(score_mort,pir, hei2015q i_FCS_sdq fs  SNAP ins ins2 marriage hoq065 unemployment unemployment2, ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl , mortstat, out_model3 );\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq, ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl , mortstat, out_model3 , \"comb1\");\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq SNAP ins, ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl , mortstat, out_model3 , \"all1\");\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq SNAP ins2, ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl , mortstat, out_model3 , \"all2\");\n",
      "\n",
      "\n",
      "/**age, sex, race, edu, lifestyles, baseline health, insurance, as the base model*/\n",
      "%cox(score_mort,pir, hei2015q i_FCS_sdq fs  SNAP marriage hoq065 unemployment unemployment2, ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl ins, mortstat , out_model31 );\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq, ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl ins, mortstat, out_model31 , \"comb1\");\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq SNAP ins, ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl ins, mortstat, out_model31 , \"all1\");\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq SNAP ins2, ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl ins, mortstat, out_model31 , \"all2\");\n",
      "\n",
      "\n",
      "data out.out_models_pir ; set out_model1 out_model11 out_model2 out_model21 out_model3 out_model31; run; \n",
      "\n",
      "\n",
      "\n",
      "PROC expORT data= WORK.out_models_pir \n",
      "            outFILE= \"&home\\allmodelspir.xlsx\" \n",
      "            DBMS=xlsx REPLACE;\n",
      " *    GETNAMES=YES;\n",
      "RUN;\n",
      "\n",
      "/***Predictor=FS **/\n",
      "\n",
      "*%cox(score_mort1,pir, fs  i_FCS_sdq SNAP ins ins2, ridageyr sex race   , mortstat, out_model0 );\n",
      "%cox(score_mort,fs, hei2015q i_FCS_sdq pir SNAP ins ins2 marriage hoq065 unemployment unemployment2,  ridageyr sex race  edu , mortstat, out_model1a );\n",
      "\n",
      "%cox(score_mort,fs, hei2015q i_FCS_sdq, ridageyr sex race edu ins marriage unemployment2 hoq065 pir snap , mortstat, out_model11a );  \n",
      "\n",
      "%cox(score_mort,fs, hei2015q i_FCS_sdq pir SNAP ins ins2 marriage hoq065 unemployment unemployment2, ridageyr sex race edu smk met_hr perE_alco , mortstat, out_model2a );  \n",
      "\n",
      "%cox(score_mort,fs, hei2015q i_FCS_sdq, ridageyr sex race edu smk met_hr perE_alco ins marriage unemployment2 hoq065 pir snap , mortstat, out_model21a);  \n",
      "/**age, sex, race, edu, lifestyles, baseline health, insurance, as the base model*/\n",
      "\n",
      "%cox(score_mort,fs, hei2015q i_FCS_sdq pir  SNAP ins ins2 marriage hoq065 unemployment unemployment2, ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl , mortstat , out_model3a );\n",
      "\n",
      "%cox1(score_mort,fs, hei2015q pir  SNAP ins marriage hoq065 unemployment , ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl , mortstat, out_model4 , \"all1\");\n",
      "\n",
      "\n",
      "data out_models_fs ; set out_model1a out_model11a out_model2a out_model21a  out_model3a out_model4; run; \n",
      "\n",
      "\n",
      "PROC expORT data= WORK.out_models_fs \n",
      "            outFILE= \"&home\\allmodelsfs.xlsx\" \n",
      "            DBMS=xlsx REPLACE;\n",
      " *    GETNAMES=YES;\n",
      "RUN;\n",
      "\n",
      "\n",
      "%cox(score_mort, hei2015q, fs pir SNAP ins ins2 marriage hoq065 unemployment unemployment2,  ridageyr sex race  edu , mortstat, out_model1b );\n",
      "\n",
      "%cox(score_mort, hei2015q, fs pir SNAP ins ins2 marriage hoq065 unemployment unemployment2, ridageyr sex race edu smk met_hr perE_alco , mortstat, out_model2b );  \n",
      "\n",
      "%cox(score_mort, hei2015q, fs pir  SNAP ins ins2 marriage hoq065 unemployment unemployment2, ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl , mortstat , out_model3b );\n",
      "\n",
      "%cox1(score_mort, hei2015q, fs pir  SNAP ins  , ridageyr sex race  edu smk\tmet_hr perE_alco , mortstat, out_model2b , \"all1\");\n",
      "\n",
      "\n",
      "%cox1(score_mort, hei2015q, fs pir  SNAP ins marriage hoq065 unemployment , ridageyr sex race  edu smk\tmet_hr perE_alco , mortstat, out_model2b , \"all2\");\n",
      "\n",
      "%cox1(score_mort, hei2015q, fs pir  SNAP ins  , ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl , mortstat, out_model3b , \"all1\");\n",
      "\n",
      "%cox1(score_mort, hei2015q, fs pir  SNAP ins marriage hoq065 unemployment , ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl , mortstat, out_model3b , \"all2\");\n",
      "\n",
      "%cox1(score_mort, hei2015q, fs pir  SNAP ins marriage hoq065 unemployment , ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl , mortstat, out_model3b , \"all3\");\n",
      "\n",
      "\n",
      "data data.out_models_hei ; set out_model1b out_model2b out_model3b; run; \n",
      "\n",
      "\n",
      "PROC expORT data= WORK.out_models_hei \n",
      "            outFILE= \"&home\\allmodelshei.xlsx\" \n",
      "            DBMS=xlsx REPLACE;\n",
      " *    GETNAMES=YES;\n",
      "RUN;\n",
      "\n",
      "\n",
      "%cox(score_mort, SNAP, hei2015q fs pir ins ins2 marriage hoq065 unemployment unemployment2,  ridageyr sex race  edu , mortstat, out_model1c );\n",
      "\n",
      "%cox(score_mort, SNAP, hei2015q fs pir ins ins2 marriage hoq065 unemployment unemployment2, ridageyr sex race edu smk met_hr perE_alco , mortstat, out_model2c );  \n",
      "\n",
      "%cox(score_mort, SNAP, hei2015q fs pir  ins ins2 marriage hoq065 unemployment unemployment2, ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl , mortstat , out_model3c );\n",
      "\n",
      "%cox1(score_mort, SNAP, hei2015q fs pir  ins  , ridageyr sex race  edu smk\tmet_hr perE_alco , mortstat, out_model2c , \"all1\");\n",
      "\n",
      "%cox1(score_mort, SNAP, hei2015q fs pir  ins marriage hoq065 unemployment , ridageyr sex race  edu smk\tmet_hr perE_alco , mortstat, out_model2c , \"all2\");\n",
      "\n",
      "%cox1(score_mort, SNAP, hei2015q fs pir  ins  , ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl , mortstat, out_model3c , \"all1\");\n",
      "\n",
      "%cox1(score_mort, SNAP, hei2015q fs pir  ins marriage hoq065 unemployment , ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl , mortstat, out_model3c , \"all2\");\n",
      "\n",
      "%cox1(score_mort, SNAP, hei2015q fs pir  ins marriage hoq065 unemployment2 , ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl , mortstat, out_model3c , \"all3\");\n",
      "\n",
      "\n",
      "data data.out_models_SNAP ; set out_model1c out_model2c out_model3c; run; \n",
      "\n",
      "\n",
      "PROC expORT data= data.out_models_snap \n",
      "            outFILE= \"&home\\allmodelssnap.xlsx\" \n",
      "            DBMS=xlsx REPLACE;\n",
      " *    GETNAMES=YES;\n",
      "RUN;\n",
      "\n",
      "\n",
      "\n",
      "/*PROC expORT data= WORK.out_model0*/\n",
      "/*            outFILE= \"&home\\model0.xlsx\" */\n",
      "/*            DBMS=xlsx REPLACE;*/\n",
      "/* *    GETNAMES=YES;*/\n",
      "/*RUN;*/\n",
      "/*PROC expORT data= WORK.out_model1*/\n",
      "/*            outFILE= \"&home\\model1.xlsx\" */\n",
      "/*            DBMS=xlsx REPLACE;*/\n",
      "/* *    GETNAMES=YES;*/\n",
      "/*RUN;*/\n",
      "/*PROC expORT data= WORK.out_model2*/\n",
      "/*            outFILE= \"&home\\model2.xlsx\" */\n",
      "/*            DBMS=xlsx REPLACE;*/\n",
      "/* *    GETNAMES=YES;*/\n",
      "/*RUN;*/\n",
      "/**/\n",
      "/*PROC expORT data= WORK.out_model3*/\n",
      "/*            outFILE= \"&home\\model3.xlsx\" */\n",
      "/*            DBMS=xlsx REPLACE;*/\n",
      "/* *    GETNAMES=YES;*/\n",
      "/*RUN;*/\n",
      "\n",
      "\n",
      "******************************************************************************************************;\n",
      "\n",
      "\n",
      "✅ Total rows: 128809\n",
      "✅ Unique SEQN: 128809\n",
      "Columns: ['SEQN', 'RIDAGEYR', 'SDDSRVYR', 'CYCLE', 'MARRIAGE', 'MARRIAGE3']\n",
      "\n",
      "2021–2023 MARRIAGE3 counts:\n",
      " MARRIAGE3\n",
      "<NA>    4150\n",
      "1       4136\n",
      "2       2022\n",
      "3       1625\n",
      "Name: count, dtype: Int64\n",
      "Saved: /Users/dengshuyue/Desktop/SDOH/analysis/data/demo9923.pkl\n",
      "Saved: /Users/dengshuyue/Desktop/SDOH/analysis/data/demo9923.csv\n",
      "\n",
      "Counts by CYCLE:\n",
      "CYCLE\n",
      "2017-March 2020 (pre-pandemic)    15560\n",
      "August 2021–August 2023           11933\n",
      "2001-2002                         11039\n",
      "2009-2010                         10537\n",
      "2005-2006                         10348\n",
      "2013-2014                         10175\n",
      "2007-2008                         10149\n",
      "2003-2004                         10122\n",
      "2015-2016                          9971\n",
      "1999-2000                          9965\n",
      "2011-2012                          9756\n",
      "2017-2018                          9254\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts by SDDSRVYR (raw numeric code, may be NaN for newer releases):\n",
      "SDDSRVYR\n",
      "1.0      9965\n",
      "2.0     11039\n",
      "3.0     10122\n",
      "4.0     10348\n",
      "5.0     10149\n",
      "6.0     10537\n",
      "7.0      9756\n",
      "8.0     10175\n",
      "9.0      9971\n",
      "10.0     9254\n",
      "12.0    11933\n",
      "66.0    15560\n",
      "Name: count, dtype: int64\n",
      "Using mortality file: /Users/dengshuyue/Desktop/SDOH/analysis/data/less_important/mortality9918.sas7bdat\n",
      "\n",
      "📊 NHANES cycles in mortality-linked data:\n",
      "SDDSRVYR\n",
      "1999–2000    4973\n",
      "2001–2002    5586\n",
      "2003–2004    5293\n",
      "2005–2006    5332\n",
      "2007–2008    5989\n",
      "2009–2010    6346\n",
      "2011–2012    5603\n",
      "2013–2014    5913\n",
      "2015–2016    5720\n",
      "2017–2018    5498\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total records: 56253\n",
      "\n",
      "⏱️  Survival summary:\n",
      "  N (unique SEQN): 56253\n",
      "  Events (%): 14.87\n",
      "  TIME_Y (min / median / max): 0.0 9.416666666666666 20.75\n",
      "OCQ: (55081, 4)\n",
      "HOQ: (55081, 4)\n",
      "INS: (55081, 3)\n",
      "SNAP/FS: (55081, 5)\n",
      "OCQ cycles: [np.int64(1), np.int64(2)]\n",
      "  OCQ | base=56,253 | inner keeps=52,287 | left keeps=56,253\n",
      "HOQ cycles: [np.int64(1), np.int64(2)]\n",
      "  HOQ | base=56,253 | inner keeps=52,287 | left keeps=56,253\n",
      "HIQ cycles: [np.int64(1), np.int64(2)]\n",
      "  HIQ | base=56,253 | inner keeps=52,287 | left keeps=56,253\n",
      "FSQ cycles: [np.int64(1), np.int64(2)]\n",
      "  FSQ | base=56,253 | inner keeps=52,287 | left keeps=56,253\n",
      "ROOT = /Users/dengshuyue/Desktop/SDOH/analysis\n",
      "OUT  = /Users/dengshuyue/Desktop/SDOH/analysis/output\n",
      "SDOH_ROOT env = None\n",
      "OUT (fixed) = /Users/dengshuyue/Desktop/SDOH/analysis/output\n",
      "✅ master shape: (56253, 20)\n",
      "\n",
      "Non-missing coverage (%):\n",
      "  UNEMPLOYMENT: 92.9%\n",
      "  HOD050: 91.6%\n",
      "  HOQ065: 91.5%\n",
      "  INS: 87.7%\n",
      "  EMPLOY: 86.8%\n",
      "  SNAP: 76.3%\n",
      "  FS: 74.7%\n",
      "\n",
      "Saved:\n",
      "  /Users/dengshuyue/Desktop/SDOH/analysis/output/nhanes_mort_demo_sdoh_1999_2018.parquet \n",
      "  /Users/dengshuyue/Desktop/SDOH/analysis/output/nhanes_mort_demo_sdoh_1999_2018.csv\n",
      "Rows: 56,253 | Unique SEQN: 56,253 | Missing SEQN: 0 | Duplicates: 0\n",
      "SEQN range: 2 → 102956\n",
      "Cycles:\n",
      " SDDSRVYR\n",
      "1.0     4973\n",
      "2.0     5586\n",
      "3.0     5293\n",
      "4.0     5332\n",
      "5.0     5989\n",
      "6.0     6346\n",
      "7.0     5603\n",
      "8.0     5913\n",
      "9.0     5720\n",
      "10.0    5498\n",
      "Name: count, dtype: int64\n",
      "Looking in: /Users/dengshuyue/Desktop/SDOH/analysis/output\n",
      "  ✓ nhanes_mort_demo_sdoh_1999_2018.parquet  | modified: 2025-09-09 13:22:59 | size: 774,526 bytes\n",
      "  ✓ nhanes_mort_demo_sdoh_1999_2018.csv      | modified: 2025-09-09 13:22:59 | size: 4,562,121 bytes\n",
      "  - nhanes_mort_demo_soc_1999_2018.parquet   (missing)\n",
      "  - nhanes_mort_demo_soc_1999_2018.csv       (missing)\n",
      "\n",
      "Loaded: /Users/dengshuyue/Desktop/SDOH/analysis/output/nhanes_mort_demo_sdoh_1999_2018.parquet\n",
      "Shape: (56253, 20)\n",
      "Cols (first 10): ['SEQN', 'ELIGSTAT', 'MORTSTAT', 'PERMTH_EXM', 'PERMTH_INT', 'UCOD_LEADING', 'DIABETES', 'HYPERTEN', 'TIME_Y', 'EVENT']\n",
      "✅ Keeping (newest per extension):\n",
      "  ✓ /Users/dengshuyue/Desktop/SDOH/analysis/data/SODH_diet_mort.pkl  |  2025-07-18 10:39:20  |  27 MB\n",
      "  ✓ /Users/dengshuyue/Desktop/SDOH/analysis/data/SODH_diet_mort_depr2.csv  |  2025-09-08 16:16:55  |  21 MB\n",
      "  ↳ total size kept: 48 MB\n",
      "\n",
      "🗑️ Candidates to delete (older versions):\n",
      "  - /Users/dengshuyue/Desktop/SDOH/analysis/data/SODH_diet_mort6.csv  |  2025-08-10 11:39:07  |  14 MB\n",
      "  ↳ total size to delete: 14 MB\n",
      "Nothing to move.\n",
      "\n",
      "Now in /data/old:\n",
      " - SODH_diet_mort.csv\n",
      " - SODH_diet_mort2.csv\n",
      " - SODH_diet_mort3.csv\n",
      " - SODH_diet_mort4.csv\n",
      " - SODH_diet_mort5.csv\n",
      " - SODH_diet_mort_depr.csv\n"
     ]
    }
   ],
   "source": [
    "# In 02 — Build Covariates.ipynb, first cell:\n",
    "%run -i 00_demo_mort_sdoh.ipynb   # use -i so variables persist in this kernel\n",
    "\n",
    "# Quick sanity:\n",
    "# print(\"ROOT =\", ROOT)\n",
    "# print(\"OUT  =\", OUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb2774a-6261-47bd-8a4c-8ae878af1883",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "757ff718-d2c2-4f09-aee8-34fcbc6dc165",
   "metadata": {},
   "source": [
    "<h2> Step 1. try build streamlined code for cov and merge </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "55bb5a72-1b7f-41d9-9037-0649fc7abc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NHANES 1999–2023 Covariates Builder\n",
    "-----------------------------------\n",
    "Goal: produce tidy, reproducible covariate parquet files and one merged core table.\n",
    "\n",
    "Deliverables written to /Users/dengshuyue/Desktop/SDOH/analysis/output/ (create if missing):\n",
    "  - cov_smk_1999_2023.parquet\n",
    "  - cov_alc_1999_2023.parquet\n",
    "  - cov_pa_1999_2023.parquet\n",
    "  - cov_bmx_1999_2023.parquet\n",
    "  - cov_clinical_1999_2023.parquet\n",
    "  - cov_household_1999_2023.parquet\n",
    "  - cov_core_1999_2023.parquet\n",
    "\n",
    "Usage (in a Jupyter cell or as a script):\n",
    "    # Edit CONFIG paths below once. Then run:\n",
    "    from pathlib import Path\n",
    "    import cov_builder_9923 as cb\n",
    "    cb.run_all()\n",
    "\n",
    "Notes:\n",
    "- This file is designed to be readable and modular. All NHANES quirks are isolated in one place.\n",
    "- Where possible, we support a \"fast path\" (append 2019–2023 to your 1999–2018 files) and a \"full rebuild\" path.\n",
    "- You may fill in cycle-specific variable maps in VARS if you choose the full rebuild route.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# --- Put this near the top, before the dataclass ---\n",
    "BASE = Path(\"/Users/dengshuyue/Desktop/SDOH/analysis\")\n",
    "\n",
    "# ----------------------------\n",
    "# Configuration (EDIT THESE)\n",
    "# ----------------------------\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Root folders (point to your local project)\n",
    "    raw_dir: Path = BASE / \"data\"          # where per-cycle raw files live\n",
    "    interim_dir: Path = BASE / \"data\" / \"cov\"   # where any intermediate stacks live\n",
    "    out_dir: Path = BASE / \"output\"        # outputs\n",
    "\n",
    "    # Fast-path inputs you already maintain (optional)\n",
    "    smk_9918: Optional[Path] = None\n",
    "    pa_9918_imputed: Optional[Path] = None\n",
    "    clinical_9918: Optional[Path] = None\n",
    "\n",
    "    # New 2019–2023 stacks if you have them (optional)\n",
    "    smk_1923: Optional[Path] = None\n",
    "    pa_1923: Optional[Path] = None\n",
    "    clinical_1923: Optional[Path] = None\n",
    "\n",
    "    # DEMO 1999–2023 (required)\n",
    "    demo_9923: Path = BASE / \"data\" / \"cov\" / \"demo9923.parquet\"\n",
    "    demo_9918: Optional[Path] = None\n",
    "\n",
    "    # BMX 1999–2023 (required if not reconstructed from raw)\n",
    "    bmx_9923: Optional[Path] = None\n",
    "\n",
    "    # Output file names\n",
    "    cov_smk: str = \"cov_smk_1999_2023.parquet\"\n",
    "    cov_alc: str = \"cov_alc_1999_2023.parquet\"\n",
    "    cov_pa: str = \"cov_pa_1999_2023.parquet\"\n",
    "    cov_bmx: str = \"cov_bmx_1999_2023.parquet\"\n",
    "    cov_clinical: str = \"cov_clinical_1999_2023.parquet\"\n",
    "    cov_household: str = \"cov_household_1999_2023.parquet\"\n",
    "    cov_core: str = \"cov_core_1999_2023.parquet\"\n",
    "\n",
    "CONFIG = Config()\n",
    "\n",
    "\n",
    "# ---------------------------------\n",
    "# Helpers: IO, cleaning, harmonize\n",
    "# ---------------------------------\n",
    "\n",
    "NHANES_MISS = {7, 9, 77, 99, 777, 999, 7777, 9999, 77777, 99999}\n",
    "\n",
    "\n",
    "def ensure_dir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def nhanes_na(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"Map NHANES special missing codes to NaN (keeps genuine zeros).\"\"\"\n",
    "    return series.mask(series.isin(NHANES_MISS)).astype(float)\n",
    "\n",
    "\n",
    "def upper_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = [c.upper() for c in df.columns]\n",
    "    return df\n",
    "\n",
    "\n",
    "def read_parquet_safe(path: Optional[Path]) -> Optional[pd.DataFrame]:\n",
    "    if path and Path(path).exists():\n",
    "        return pd.read_parquet(path)\n",
    "    return None\n",
    "\n",
    "\n",
    "def pick_first_existing(*candidates: Optional[Path]) -> Optional[Path]:\n",
    "    \"\"\"Return the first existing path among candidates (or None).\"\"\"\n",
    "    for c in candidates:\n",
    "        if c is not None and Path(c).exists():\n",
    "            return Path(c)\n",
    "    return None\n",
    "\n",
    "# --------------\n",
    "# Smoking (SMQ)\n",
    "# --------------\n",
    "\n",
    "# Minimal variable assumptions for fast-path append:\n",
    "# Expect 1999–2018 stack with columns: SEQN, SMK_STATUS, CIGS_PER_DAY, PACK_YEARS, FORMER_SMOKER\n",
    "# For 2019–2023, we derive using stable SMQ fields when available.\n",
    "\n",
    "SMK_STATUS_CATS = pd.CategoricalDtype(categories=[\"NEVER\", \"FORMER\", \"CURRENT\"], ordered=True)\n",
    "\n",
    "\n",
    "def derive_smoking_from_smq(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Derive compact smoking features from raw/stacked SMQ.\n",
    "    Expected fields (if present):\n",
    "      - SMQ020: Smoked at least 100 cigarettes in life? (1=Yes, 2=No)\n",
    "      - SMQ040: Do you now smoke cigarettes? (1=Every day, 2=Some days, 3=Not at all)\n",
    "      - SMD030 / SMQ050Q/U pairs for intensity and duration in years may vary by cycle.\n",
    "    We defensively compute:\n",
    "      SMK_STATUS: NEVER/FORMER/CURRENT\n",
    "      CIGS_PER_DAY: numeric (NaN if unknown)\n",
    "      PACK_YEARS: (cigs/day / 20) * years\n",
    "      FORMER_SMOKER: 1/0\n",
    "    \"\"\"\n",
    "    d = upper_df(df)\n",
    "\n",
    "    # Life 100 cigs (ever smoker)\n",
    "    ever = d.get(\"SMQ020\")\n",
    "    if ever is not None:\n",
    "        ever = ever.replace({2: 0}).replace({1: 1})\n",
    "    else:\n",
    "        ever = pd.Series(np.nan, index=d.index)\n",
    "\n",
    "    # Current smoking status\n",
    "    smq040 = d.get(\"SMQ040\")\n",
    "    # Default NEVER if explicitly said no to ever smoking\n",
    "    smk_status = pd.Series(\"NEVER\", index=d.index)\n",
    "    smk_status[ever == 1] = \"FORMER\"\n",
    "    if smq040 is not None:\n",
    "        smk_status.loc[smq040.isin([1, 2])] = \"CURRENT\"\n",
    "        smk_status.loc[smq040 == 3] = \"FORMER\"\n",
    "    smk_status = smk_status.astype(SMK_STATUS_CATS)\n",
    "\n",
    "    # Intensity (cigarettes per day) — prefer SMQ050Q (quantity) with unit SMQ050U (1=per day, 2=per week, etc.).\n",
    "    cigs_q = nhanes_na(d.get(\"SMQ050Q\", pd.Series(np.nan, index=d.index)))\n",
    "    cigs_u = d.get(\"SMQ050U\")  # 1=day, 2=week, 3=month\n",
    "    cigs_per_day = cigs_q.copy()\n",
    "    if cigs_u is not None:\n",
    "        cigs_per_day = (\n",
    "            cigs_q.where(cigs_u == 1)\n",
    "            .fillna((cigs_q / 7).where(cigs_u == 2))\n",
    "            .fillna((cigs_q / 30).where(cigs_u == 3))\n",
    "        )\n",
    "\n",
    "    # Years smoked (approx). Prefer SMD030 (years smoked) or computed from start/stop when available.\n",
    "    years_smoked = nhanes_na(d.get(\"SMD030\", pd.Series(np.nan, index=d.index)))\n",
    "\n",
    "    pack_years = (cigs_per_day / 20.0) * years_smoked\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"SEQN\": d[\"SEQN\"],\n",
    "        \"SMK_STATUS\": smk_status.astype(\"string\").str.upper(),\n",
    "        \"CIGS_PER_DAY\": cigs_per_day,\n",
    "        \"PACK_YEARS\": pack_years,\n",
    "    })\n",
    "    out[\"FORMER_SMOKER\"] = (out[\"SMK_STATUS\"] == \"FORMER\").astype(\"int8\")\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def build_smk(cfg: Config = CONFIG) -> pd.DataFrame:\n",
    "    ensure_dir(cfg.out_dir)\n",
    "\n",
    "    # Auto-pick sources: prefer a combined 99–23 stack if present; else 99–18 (+ optional 19–23 append)\n",
    "    def _read_any(p: Path) -> pd.DataFrame:\n",
    "        return pd.read_parquet(p) if str(p).endswith(\".parquet\") else pd.read_csv(p)\n",
    "\n",
    "    # Look for combined stack first\n",
    "    smk_9923 = pick_first_existing(\n",
    "        cfg.interim_dir / \"smk_9923.parquet\",\n",
    "        cfg.interim_dir / \"smk_9923.csv\",\n",
    "    )\n",
    "    if smk_9923:\n",
    "        smk = upper_df(_read_any(smk_9923))\n",
    "    else:\n",
    "        # Then look for separate 99–18 and 19–23 pieces\n",
    "        smk_9918 = pick_first_existing(\n",
    "            cfg.smk_9918,\n",
    "            cfg.interim_dir / \"smk_9918.parquet\",\n",
    "            cfg.interim_dir / \"smk_9918.csv\",\n",
    "        )\n",
    "        smk_1923 = pick_first_existing(\n",
    "            cfg.smk_1923,\n",
    "            cfg.interim_dir / \"smk_1923.parquet\",\n",
    "            cfg.interim_dir / \"smk_1923.csv\",\n",
    "        )\n",
    "        if smk_9918 is None:\n",
    "            raise FileNotFoundError(\"Provide smk_9923 or smk_9918 under ./interim/ (csv/parquet).\")\n",
    "        df_9918 = upper_df(_read_any(smk_9918))\n",
    "        if smk_1923:\n",
    "            df_1923 = upper_df(_read_any(smk_1923))\n",
    "            smk = pd.concat([df_9918, df_1923], ignore_index=True)\n",
    "        else:\n",
    "            smk = df_9918\n",
    "\n",
    "    # Enforce schema and types\n",
    "    keep = [\"SEQN\", \"SMK_STATUS\", \"CIGS_PER_DAY\", \"PACK_YEARS\", \"FORMER_SMOKER\"]\n",
    "    smk = smk[keep].copy()\n",
    "    smk[\"SMK_STATUS\"] = smk[\"SMK_STATUS\"].str.upper()\n",
    "    smk[\"FORMER_SMOKER\"] = smk[\"FORMER_SMOKER\"].astype(\"int8\")\n",
    "\n",
    "    smk.to_parquet(cfg.out_dir / cfg.cov_smk, index=False)\n",
    "    return smk\n",
    "\n",
    "# -----------------\n",
    "# Alcohol (ALQ)\n",
    "# -----------------\n",
    "\n",
    "# CDC categories (derived):\n",
    "#  - NONE: 0 drinks or lifetime < 12 drinks\n",
    "#  - MODERATE: Women 0<–1 per day; Men 0<–2 per day\n",
    "#  - HEAVY: Women >1 per day; Men >2 per day\n",
    "\n",
    "\n",
    "def _drinks_per_day_from_alq(df: pd.DataFrame) -> pd.Series:\n",
    "    d = upper_df(df)\n",
    "    # Typical quantity/frequency fields (vary by cycle). We'll try the common ALQ120Q/U (past 12 months):\n",
    "    q = nhanes_na(d.get(\"ALQ120Q\", pd.Series(np.nan, index=d.index)))  # number of drinks per occasion\n",
    "    u = d.get(\"ALQ120U\")  # 1=day, 2=week, 3=month, 4=year\n",
    "    f = nhanes_na(d.get(\"ALQ121U\", pd.Series(np.nan, index=d.index)))  # frequency unit count (e.g., days/week)\n",
    "    fu = d.get(\"ALQ121Q\")  # NOTE: some cycles flip Q/U naming; we guard below\n",
    "\n",
    "    # Fallback alternative naming (older cycles use ALQ120U as unit, ALQ120Q as number of days, etc.).\n",
    "    # We attempt to normalize as: drinks_per_day = (drinks_per_occasion * occasions_per_unit) / days_in_unit.\n",
    "    drinks_per_occasion = q\n",
    "\n",
    "    # occasions per unit (approx). Try ALQ101/ALQ120 combos when available.\n",
    "    occ_q = nhanes_na(d.get(\"ALQ101\", pd.Series(np.nan, index=d.index)))  # days drank in past year\n",
    "    if occ_q.notna().any():\n",
    "        occasions_per_year = occ_q\n",
    "    else:\n",
    "        # try to build from ALQ120U (unit) and ALQ120Q (count) if that pair encodes frequency\n",
    "        count = nhanes_na(d.get(\"ALQ120Q\", pd.Series(np.nan, index=d.index)))\n",
    "        unit = d.get(\"ALQ120U\")\n",
    "        occasions_per_year = pd.Series(np.nan, index=d.index)\n",
    "        if unit is not None:\n",
    "            occasions_per_year = (\n",
    "                count.where(unit == 4)  # per year\n",
    "                .fillna((count * 12).where(unit == 3))  # per month\n",
    "                .fillna((count * 52).where(unit == 2))  # per week\n",
    "                .fillna((count * 365).where(unit == 1))  # per day\n",
    "            )\n",
    "\n",
    "    drinks_per_day = (drinks_per_occasion * occasions_per_year) / 365.0\n",
    "    return drinks_per_day\n",
    "\n",
    "\n",
    "def categorize_alcohol(drinks_per_day: pd.Series, sex: pd.Series, lifetime_lt12: Optional[pd.Series] = None) -> pd.Series:\n",
    "    sex = sex.replace({1: \"M\", 2: \"F\"}).astype(\"string\").str.upper()\n",
    "    cat = pd.Series(\"NONE\", index=drinks_per_day.index, dtype=\"string\")\n",
    "    if lifetime_lt12 is not None:\n",
    "        cat = cat.where(~(lifetime_lt12 == 1), \"NONE\")\n",
    "\n",
    "    # Set MODERATE vs HEAVY by sex\n",
    "    mod_mask = (\n",
    "        ((sex == \"F\") & (drinks_per_day > 0) & (drinks_per_day <= 1)) |\n",
    "        ((sex == \"M\") & (drinks_per_day > 0) & (drinks_per_day <= 2))\n",
    "    )\n",
    "    heavy_mask = (\n",
    "        ((sex == \"F\") & (drinks_per_day > 1)) |\n",
    "        ((sex == \"M\") & (drinks_per_day > 2))\n",
    "    )\n",
    "    cat = cat.where(~mod_mask, \"MODERATE\")\n",
    "    cat = cat.where(~heavy_mask, \"HEAVY\")\n",
    "    return cat.str.upper()\n",
    "\n",
    "\n",
    "def build_alc(cfg: Config = CONFIG) -> pd.DataFrame:\n",
    "    ensure_dir(cfg.out_dir)\n",
    "\n",
    "    # ✅ B) Prefer the alcohol file you already built in /output\n",
    "    out_path = cfg.out_dir / cfg.cov_alc\n",
    "    if out_path.exists():\n",
    "        return pd.read_parquet(out_path)\n",
    "\n",
    "    # Fallback: try to build from interim stacks (legacy flow)\n",
    "    demo = pd.read_parquet(cfg.demo_9923)[[\"SEQN\", \"RIAGENDR\"]].rename(columns={\"RIAGENDR\": \"SEX\"})\n",
    "\n",
    "    # Expect a stacked ALQ file at interim_dir/alq_9923.parquet; fall back to 1999–2018\n",
    "    alq_9923 = cfg.interim_dir / \"alq_9923.parquet\"\n",
    "    alq_9918 = cfg.interim_dir / \"alq_9918.parquet\"\n",
    "    if alq_9923.exists():\n",
    "        alq = pd.read_parquet(alq_9923)\n",
    "    elif alq_9918.exists():\n",
    "        alq = pd.read_parquet(alq_9918)\n",
    "    else:\n",
    "        raise FileNotFoundError(\n",
    "            \"Alcohol file not prebuilt in /output and no interim ALQ stack found.\\n\"\n",
    "            \"Either run the ALQ fetcher that writes output/cov_alc_1999_2023.parquet, \"\n",
    "            \"or provide data/cov/alq_9923.parquet (or alq_9918.parquet).\"\n",
    "        )\n",
    "    alq = upper_df(alq)\n",
    "\n",
    "    drinks_per_day = _drinks_per_day_from_alq(alq)\n",
    "\n",
    "    lifetime_lt12 = None\n",
    "    if \"ALQ110\" in alq.columns:\n",
    "        lifetime_lt12 = (alq[\"ALQ110\"] == 2).astype(\"int8\")  # 2=no -> <12 drinks lifetime\n",
    "\n",
    "    sex_series = demo.set_index(\"SEQN\").loc[alq[\"SEQN\"], \"SEX\"].reset_index(drop=True)\n",
    "    alc_cat = categorize_alcohol(drinks_per_day, sex=sex_series, lifetime_lt12=lifetime_lt12)\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"SEQN\": alq[\"SEQN\"],\n",
    "        \"DRINKS_PER_DAY\": drinks_per_day,\n",
    "        \"ALCOHOL_CAT\": alc_cat,\n",
    "    })\n",
    "    out.to_parquet(out_path, index=False)\n",
    "    return out\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Physical activity (PAQ/PAD)\n",
    "# -----------------------------\n",
    "\n",
    "def build_pa(cfg: Config = CONFIG) -> pd.DataFrame:\n",
    "    ensure_dir(cfg.out_dir)\n",
    "\n",
    "    def _read_any(p: Path) -> pd.DataFrame:\n",
    "        return pd.read_parquet(p) if str(p).endswith(\".parquet\") else pd.read_csv(p)\n",
    "\n",
    "    # Prefer combined 99–23 if present\n",
    "    pa_9923 = pick_first_existing(\n",
    "        cfg.interim_dir / \"totalpa_9923_imputed.parquet\",\n",
    "        cfg.interim_dir / \"totalpa_9923_imputed.csv\",\n",
    "    )\n",
    "    if pa_9923:\n",
    "        pa = upper_df(_read_any(pa_9923))\n",
    "    else:\n",
    "        # Fallback: 99–18 (+ optional 19–23 append)\n",
    "        pa_9918 = pick_first_existing(\n",
    "            cfg.pa_9918_imputed,\n",
    "            cfg.interim_dir / \"totalpa_9918_imputed.parquet\",\n",
    "            cfg.interim_dir / \"totalpa_9918_imputed.csv\",\n",
    "        )\n",
    "        if pa_9918 is None:\n",
    "            raise FileNotFoundError(\"Provide totalpa_9923_imputed or totalpa_9918_imputed under ./interim/.\")\n",
    "        pa = upper_df(_read_any(pa_9918))\n",
    "        pa_1923 = pick_first_existing(\n",
    "            cfg.pa_1923,\n",
    "            cfg.interim_dir / \"totalpa_1923_imputed.parquet\",\n",
    "            cfg.interim_dir / \"totalpa_1923_imputed.csv\",\n",
    "        )\n",
    "        if pa_1923:\n",
    "            pa = pd.concat([pa, upper_df(_read_any(pa_1923))], ignore_index=True)\n",
    "\n",
    "    # Enforce names\n",
    "    needed = [\"SEQN\", \"LTPA_MET_HR_WK\", \"LTPA_IMPUTED_FLAG\"]\n",
    "    for col in needed:\n",
    "        if col not in pa.columns:\n",
    "            raise ValueError(f\"PA table missing column: {col}\")\n",
    "\n",
    "    pa[\"LTPA_IMPUTED_FLAG\"] = pa[\"LTPA_IMPUTED_FLAG\"].astype(\"int8\")\n",
    "    pa.to_parquet(cfg.out_dir / cfg.cov_pa, index=False)\n",
    "    return pa\n",
    "\n",
    "# -----------------------------\n",
    "# Anthropometrics (BMX)\n",
    "# -----------------------------\n",
    "\n",
    "def build_bmx(cfg: Config = CONFIG) -> pd.DataFrame:\n",
    "    ensure_dir(cfg.out_dir)\n",
    "\n",
    "    if cfg.bmx_9923 is None:\n",
    "        # Expect stacked BMX at interim\n",
    "        # prefer 2019–2023 stack; fall back to 1999–2018 if that's all you have for now\n",
    "        bmx_9923 = cfg.interim_dir / \"bmx_9923.parquet\"\n",
    "        bmx_9918 = cfg.interim_dir / \"bmx_9918.parquet\"\n",
    "        if bmx_9923.exists():\n",
    "            bmx = pd.read_parquet(bmx_9923)\n",
    "        elif bmx_9918.exists():\n",
    "            bmx = pd.read_parquet(bmx_9918)\n",
    "        else:\n",
    "            raise FileNotFoundError(\"Provide ./interim/bmx_9923.parquet (preferred) or ./interim/bmx_9918.parquet, or set cfg.bmx_9923.\")\n",
    "    else:\n",
    "        bmx = pd.read_parquet(cfg.bmx_9923)\n",
    "\n",
    "    bmx = upper_df(bmx)\n",
    "\n",
    "    # Compute BMI if missing\n",
    "    for col in [\"BMXHT\", \"BMXWT\"]:\n",
    "        if col not in bmx.columns:\n",
    "            raise ValueError(f\"BMX table missing {col}\")\n",
    "\n",
    "    bmx[\"BMI\"] = bmx.get(\"BMXBMI\", np.nan)\n",
    "    missing_bmi = bmx[\"BMI\"].isna()\n",
    "    bmx.loc[missing_bmi, \"BMI\"] = bmx.loc[missing_bmi, \"BMXWT\"] / (bmx.loc[missing_bmi, \"BMXHT\"] / 100.0) ** 2\n",
    "\n",
    "    out = bmx[[\"SEQN\", \"BMXWT\", \"BMXHT\", \"BMI\"]].copy()\n",
    "    out.to_parquet(cfg.out_dir / cfg.cov_bmx, index=False)\n",
    "    return out\n",
    "\n",
    "# ------------------------------------\n",
    "# Clinical & conditions + vitals/labs\n",
    "# ------------------------------------\n",
    "\n",
    "@dataclass\n",
    "class ClinicalThresholds:\n",
    "    # Parameterize thresholds to stay aligned with your protocol\n",
    "    htn_sbp: float = 140.0\n",
    "    htn_dbp: float = 90.0\n",
    "    a1c_diabetes: float = 6.5\n",
    "    fpg_diabetes: float = 126.0  # mg/dL\n",
    "\n",
    "\n",
    "THR = ClinicalThresholds()\n",
    "\n",
    "\n",
    "def build_clinical(cfg: Config = CONFIG, thr: ClinicalThresholds = THR) -> pd.DataFrame:\n",
    "    ensure_dir(cfg.out_dir)\n",
    "\n",
    "    def _read_any(p: Path) -> pd.DataFrame:\n",
    "        return pd.read_parquet(p) if str(p).endswith(\".parquet\") else pd.read_csv(p)\n",
    "\n",
    "    # Prefer combined 99–23 if present\n",
    "    clin_9923 = pick_first_existing(\n",
    "        cfg.interim_dir / \"clinical_9923.parquet\",\n",
    "        cfg.interim_dir / \"clinical_9923.csv\",\n",
    "    )\n",
    "    if clin_9923:\n",
    "        clin = upper_df(_read_any(clin_9923))\n",
    "    else:\n",
    "        # Fallback: 99–18 (+ optional 19–23 append)\n",
    "        p9918 = pick_first_existing(\n",
    "            cfg.clinical_9918,\n",
    "            cfg.interim_dir / \"clinical_9918.parquet\",\n",
    "            cfg.interim_dir / \"clinical_9918.csv\",\n",
    "            cfg.interim_dir / \"nhanes_primary_anal_full_singleimputation_v2.parquet\",\n",
    "            cfg.interim_dir / \"nhanes_primary_anal_full_singleimputation_v2.csv\",\n",
    "        )\n",
    "        p1923 = pick_first_existing(\n",
    "            cfg.clinical_1923,\n",
    "            cfg.interim_dir / \"clinical_1923.parquet\",\n",
    "            cfg.interim_dir / \"clinical_1923.csv\",\n",
    "        )\n",
    "        if p9918 is None:\n",
    "            raise FileNotFoundError(\"Provide clinical_9923 or clinical_9918 under ./interim/.\")\n",
    "        df_9918 = upper_df(_read_any(p9918))\n",
    "        if p1923:\n",
    "            df_1923 = upper_df(_read_any(p1923))\n",
    "            clin = pd.concat([df_9918, df_1923], ignore_index=True)\n",
    "        else:\n",
    "            clin = df_9918\n",
    "\n",
    "    # Expect these columns already present per your v2 schema\n",
    "    keep = [\n",
    "        \"SEQN\", \"BMI_CLAS\", \"DIABETES\", \"HTN\", \"HIGH_CHOL\", \"CVD\", \"CANCER\",\n",
    "        \"SBP\", \"DBP\", \"TCHOL\", \"HDL\", \"LDL\", \"TG\"\n",
    "    ]\n",
    "    missing = [c for c in keep if c not in clin.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"clinical table missing columns: {missing}\")\n",
    "    out = clin[keep].copy()\n",
    "\n",
    "    # Types\n",
    "    for b in [\"DIABETES\", \"HTN\", \"HIGH_CHOL\", \"CVD\", \"CANCER\"]:\n",
    "        out[b] = out[b].astype(\"Int8\")\n",
    "    out.to_parquet(cfg.out_dir / cfg.cov_clinical, index=False)\n",
    "    return out\n",
    "\n",
    "# ----------------\n",
    "# Household size\n",
    "# ----------------\n",
    "\n",
    "def build_household(cfg: Config = CONFIG) -> pd.DataFrame:\n",
    "    ensure_dir(cfg.out_dir)\n",
    "    demo = pd.read_parquet(cfg.demo_9923)\n",
    "    demo = upper_df(demo)\n",
    "    if \"DMDHHSIZ\" not in demo.columns:\n",
    "        raise ValueError(\"DMDHHSIZ not found in DEMO 1999–2023 stack.\")\n",
    "    out = demo[[\"SEQN\", \"DMDHHSIZ\"]].copy()\n",
    "    out.to_parquet(cfg.out_dir / cfg.cov_household, index=False)\n",
    "    return out\n",
    "\n",
    "# ----------------------\n",
    "# Survey design fields\n",
    "# ----------------------\n",
    "\n",
    "SURVEY_KEEP = [\"SEQN\", \"SDDSRVYR\", \"SDMVPSU\", \"SDMVSTRA\", \"WTMEC2YR\"]\n",
    "\n",
    "\n",
    "def get_survey_core(cfg: Config = CONFIG) -> pd.DataFrame:\n",
    "    \"\"\"Auto-pick DEMO table: prefer 1999–2023; fallback to 1999–2018.\n",
    "    Expects: SEQN, SDDSRVYR, SDMVPSU, SDMVSTRA, WTMEC2YR in the chosen table.\n",
    "    \"\"\"\n",
    "    # Choose explicit path if it exists, else look in interim_dir for standard names\n",
    "    demo_path = None\n",
    "    if cfg.demo_9923 and Path(cfg.demo_9923).exists():\n",
    "        demo_path = Path(cfg.demo_9923)\n",
    "    elif cfg.demo_9918 and Path(cfg.demo_9918).exists():\n",
    "        demo_path = Path(cfg.demo_9918)\n",
    "    else:\n",
    "        demo_path = pick_first_existing(\n",
    "            cfg.interim_dir / \"demo_9923.parquet\",\n",
    "            cfg.interim_dir / \"demo_9918.parquet\",\n",
    "        )\n",
    "    if demo_path is None:\n",
    "        raise FileNotFoundError(\"Could not find DEMO table (expected demo_9923.parquet or demo_9918.parquet).\")\n",
    "\n",
    "    demo = upper_df(pd.read_parquet(demo_path))\n",
    "    miss = [c for c in SURVEY_KEEP if c not in demo.columns]\n",
    "    if miss:\n",
    "        raise ValueError(f\"Missing survey fields in DEMO: {miss}\")\n",
    "    return demo[SURVEY_KEEP].copy()\n",
    "\n",
    "# --------------------------\n",
    "# Merge to cov_core (LEFT)\n",
    "# --------------------------\n",
    "\n",
    "\n",
    "def build_core(cfg: Config = CONFIG) -> pd.DataFrame:\n",
    "    ensure_dir(cfg.out_dir)\n",
    "\n",
    "    # Load components (each function writes its own parquet already)\n",
    "    smk = pd.read_parquet(cfg.out_dir / cfg.cov_smk)\n",
    "    alc = pd.read_parquet(cfg.out_dir / cfg.cov_alc)\n",
    "    pa = pd.read_parquet(cfg.out_dir / cfg.cov_pa)\n",
    "    bmx = pd.read_parquet(cfg.out_dir / cfg.cov_bmx)\n",
    "    clin = pd.read_parquet(cfg.out_dir / cfg.cov_clinical)\n",
    "    hh = pd.read_parquet(cfg.out_dir / cfg.cov_household)\n",
    "    survey = get_survey_core(cfg)\n",
    "\n",
    "    # LEFT-join all on SEQN\n",
    "    core = survey\n",
    "    for part in [smk, alc, pa, bmx, clin, hh]:\n",
    "        core = core.merge(part, on=\"SEQN\", how=\"left\")\n",
    "\n",
    "    # Uppercase column names\n",
    "    core.columns = [c.upper() for c in core.columns]\n",
    "\n",
    "    core.to_parquet(cfg.out_dir / cfg.cov_core, index=False)\n",
    "    return core\n",
    "\n",
    "# --------------\n",
    "# Orchestrator\n",
    "# --------------\n",
    "\n",
    "def run_all(cfg: Config = CONFIG) -> Dict[str, pd.DataFrame]:\n",
    "    ensure_dir(cfg.out_dir)\n",
    "    out = {}\n",
    "    out[\"smk\"] = build_smk(cfg)\n",
    "    out[\"alc\"] = build_alc(cfg)\n",
    "    out[\"pa\"] = build_pa(cfg)\n",
    "    out[\"bmx\"] = build_bmx(cfg)\n",
    "    out[\"clinical\"] = build_clinical(cfg)\n",
    "    out[\"household\"] = build_household(cfg)\n",
    "    out[\"core\"] = build_core(cfg)\n",
    "    return out\n",
    "\n",
    "# ---------------------------\n",
    "# Minimal sanity validations\n",
    "# ---------------------------\n",
    "\n",
    "def quick_checks(cfg: Config = CONFIG) -> pd.Series:\n",
    "    core = pd.read_parquet(cfg.out_dir / cfg.cov_core)\n",
    "    checks = {\n",
    "        \"n_rows\": len(core),\n",
    "        \"n_unique_seqn\": core[\"SEQN\"].nunique(),\n",
    "        \"missing_bmi_pct\": core[\"BMI\"].isna().mean(),\n",
    "        \"missing_alcohol_cat_pct\": core[\"ALCOHOL_CAT\"].isna().mean(),\n",
    "        \"missing_smk_status_pct\": core[\"SMK_STATUS\"].isna().mean(),\n",
    "        \"has_weights\": int(\"WTMEC2YR\" in core.columns),\n",
    "    }\n",
    "    return pd.Series(checks)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Keep outputs in /Users/dengshuyue/Desktop/SDOH/analysis/output\n",
    "    ensure_dir(CONFIG.out_dir)\n",
    "    # run_all(CONFIG)  # uncomment to execute end-to-end\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d768ed-57db-492a-8a4f-9a32b61256b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "b5ca34bf-4d67-4c0b-b189-1ad07811fe19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smk  → OK /Users/dengshuyue/Desktop/SDOH/analysis/output/cov_smk_1999_2023.parquet\n",
      "alc  → OK /Users/dengshuyue/Desktop/SDOH/analysis/output/cov_alc_1999_2023.parquet\n",
      "pa   → OK /Users/dengshuyue/Desktop/SDOH/analysis/output/cov_pa_1999_2023.parquet\n",
      "bmx  → OK /Users/dengshuyue/Desktop/SDOH/analysis/output/cov_bmx_1999_2023.parquet\n",
      "clin → OK /Users/dengshuyue/Desktop/SDOH/analysis/output/cov_clinical_1999_2023.parquet\n",
      "hh   → OK /Users/dengshuyue/Desktop/SDOH/analysis/output/cov_household_1999_2023.parquet\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "BASE = Path(\"/Users/dengshuyue/Desktop/SDOH/analysis\")\n",
    "out = BASE / \"output\"\n",
    "\n",
    "need = {\n",
    "    \"smk\": out / \"cov_smk_1999_2023.parquet\",\n",
    "    \"alc\": out / \"cov_alc_1999_2023.parquet\",\n",
    "    \"pa\":  out / \"cov_pa_1999_2023.parquet\",\n",
    "    \"bmx\": out / \"cov_bmx_1999_2023.parquet\",\n",
    "    \"clin\":out / \"cov_clinical_1999_2023.parquet\",\n",
    "    \"hh\":  out / \"cov_household_1999_2023.parquet\",\n",
    "}\n",
    "for k,p in need.items():\n",
    "    print(f\"{k:4} →\", \"OK\" if p.exists() else \"MISSING\", p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9503b98-2192-43eb-a573-ed3f67f4755f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fadfe6-06b4-4b6a-9e35-914f163b5d54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6628c2-bd36-4b89-bf2d-98051f7ce41b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6113467d-671e-448d-a012-008efd42df91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadd3d32-2298-4500-b20e-43805ebf4e19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3984f2bd-06d8-465c-8bb2-3fac1d7c0d8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce6d1ad-de07-4189-b54b-f0050ce4e64f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618bf542-f46c-4804-b360-119393425aa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290c925d-3e19-4329-9561-62713c059dca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaf3476-a475-4b18-be98-6388102e086c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c30e8a9-1d34-49c5-997b-0fa586cb563a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "da44d896-fbed-4aa1-bce2-eee4cf8bcd4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survey DEMO source: Using existing file → /Users/dengshuyue/Desktop/SDOH/analysis/data/cov/demo9923.parquet\n",
      "→ /Users/dengshuyue/Desktop/SDOH/analysis/interim/survey_design_9923.parquet\n",
      "✓ cov_core written to: /Users/dengshuyue/Desktop/SDOH/analysis/output/cov_core_1999_2023.parquet\n",
      "n_rows                     128809.000000\n",
      "n_unique_seqn              128809.000000\n",
      "missing_bmi_pct                 0.318378\n",
      "missing_alcohol_cat_pct         0.465402\n",
      "missing_smk_status_pct          0.572903\n",
      "has_weights                     1.000000\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEQN</th>\n",
       "      <th>SDDSRVYR</th>\n",
       "      <th>SDMVPSU</th>\n",
       "      <th>SDMVSTRA</th>\n",
       "      <th>WTMEC2YR</th>\n",
       "      <th>SMK_STATUS</th>\n",
       "      <th>CIGS_PER_DAY</th>\n",
       "      <th>PACK_YEARS</th>\n",
       "      <th>FORMER_SMOKER</th>\n",
       "      <th>DRINKS_PER_DAY</th>\n",
       "      <th>LIFETIME_LT12</th>\n",
       "      <th>ALCOHOL_CAT</th>\n",
       "      <th>LTPA_MET_HR_WK</th>\n",
       "      <th>LTPA_IMPUTED_FLAG</th>\n",
       "      <th>BMXWT</th>\n",
       "      <th>BMXHT</th>\n",
       "      <th>BMI</th>\n",
       "      <th>BMI_CLAS</th>\n",
       "      <th>DIABETES</th>\n",
       "      <th>HTN</th>\n",
       "      <th>HIGH_CHOL</th>\n",
       "      <th>CVD</th>\n",
       "      <th>CANCER</th>\n",
       "      <th>SBP</th>\n",
       "      <th>DBP</th>\n",
       "      <th>TCHOL</th>\n",
       "      <th>HDL</th>\n",
       "      <th>LDL</th>\n",
       "      <th>TG</th>\n",
       "      <th>DMDHHSIZ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10982.898896</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.5</td>\n",
       "      <td>91.6</td>\n",
       "      <td>14.897695</td>\n",
       "      <td>UNDER</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>91.333333</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>131.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28325.384898</td>\n",
       "      <td>NEVER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.789041</td>\n",
       "      <td>0</td>\n",
       "      <td>moderate</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>75.4</td>\n",
       "      <td>174.0</td>\n",
       "      <td>24.904215</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>100.666667</td>\n",
       "      <td>56.666667</td>\n",
       "      <td>215.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>46192.256945</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.9</td>\n",
       "      <td>136.6</td>\n",
       "      <td>17.631713</td>\n",
       "      <td>UNDER</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>108.666667</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>129.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10251.260020</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>95.333333</td>\n",
       "      <td>61.333333</td>\n",
       "      <td>211.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>99445.065735</td>\n",
       "      <td>FORMER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>moderate</td>\n",
       "      <td>41.066667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>92.5</td>\n",
       "      <td>178.3</td>\n",
       "      <td>29.096386</td>\n",
       "      <td>OVER</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>82.666667</td>\n",
       "      <td>279.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>347.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>39656.600444</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>59.2</td>\n",
       "      <td>162.0</td>\n",
       "      <td>22.557537</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>114.666667</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>153.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>25525.423409</td>\n",
       "      <td>FORMER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8030.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>none</td>\n",
       "      <td>3.033333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>162.9</td>\n",
       "      <td>29.393577</td>\n",
       "      <td>OVER</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>125.333333</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>245.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>31510.587866</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.7</td>\n",
       "      <td>162.0</td>\n",
       "      <td>15.508307</td>\n",
       "      <td>UNDER</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100.666667</td>\n",
       "      <td>49.333333</td>\n",
       "      <td>162.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7575.870247</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>45.5</td>\n",
       "      <td>156.9</td>\n",
       "      <td>18.482704</td>\n",
       "      <td>UNDER</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>109.333333</td>\n",
       "      <td>53.333333</td>\n",
       "      <td>148.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>22445.808572</td>\n",
       "      <td>CURRENT</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.197260</td>\n",
       "      <td>0</td>\n",
       "      <td>moderate</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>111.8</td>\n",
       "      <td>190.1</td>\n",
       "      <td>30.936955</td>\n",
       "      <td>OBESE</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>145.333333</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>140.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SEQN  SDDSRVYR  SDMVPSU  SDMVSTRA      WTMEC2YR SMK_STATUS  CIGS_PER_DAY  PACK_YEARS  FORMER_SMOKER  DRINKS_PER_DAY  LIFETIME_LT12 ALCOHOL_CAT  LTPA_MET_HR_WK  LTPA_IMPUTED_FLAG  BMXWT  BMXHT  \\\n",
       "0   1.0       1.0      1.0       5.0  10982.898896       <NA>           NaN         NaN            NaN             NaN           <NA>         NaN             NaN                NaN   12.5   91.6   \n",
       "1   2.0       1.0      3.0       1.0  28325.384898      NEVER           NaN         NaN            0.0        0.789041              0    moderate        0.000000                1.0   75.4  174.0   \n",
       "2   3.0       1.0      2.0       7.0  46192.256945       <NA>           NaN         NaN            NaN             NaN           <NA>         NaN             NaN                NaN   32.9  136.6   \n",
       "3   4.0       1.0      1.0       2.0  10251.260020       <NA>           NaN         NaN            NaN             NaN           <NA>         NaN             NaN                NaN   13.3    NaN   \n",
       "4   5.0       1.0      2.0       8.0  99445.065735     FORMER           NaN         NaN            1.0       12.000000              0    moderate       41.066667                1.0   92.5  178.3   \n",
       "5   6.0       1.0      2.0       2.0  39656.600444       <NA>           NaN         NaN            NaN             NaN           <NA>         NaN             NaN                NaN   59.2  162.0   \n",
       "6   7.0       1.0      2.0       4.0  25525.423409     FORMER           NaN      8030.0            1.0             NaN              1        none        3.033333                1.0   78.0  162.9   \n",
       "7   8.0       1.0      1.0       6.0  31510.587866       <NA>           NaN         NaN            NaN             NaN           <NA>         NaN             NaN                NaN   40.7  162.0   \n",
       "8   9.0       1.0      2.0       9.0   7575.870247       <NA>           NaN         NaN            NaN             NaN           <NA>         NaN             NaN                NaN   45.5  156.9   \n",
       "9  10.0       1.0      1.0       7.0  22445.808572    CURRENT           1.0         NaN            0.0        0.197260              0    moderate        0.000000                1.0  111.8  190.1   \n",
       "\n",
       "         BMI BMI_CLAS  DIABETES  HTN  HIGH_CHOL  CVD  CANCER         SBP        DBP  TCHOL    HDL    LDL     TG  DMDHHSIZ  \n",
       "0  14.897695    UNDER         0    0          0    0       0   91.333333  56.000000  131.0   59.0   54.0   99.0       3.0  \n",
       "1  24.904215   NORMAL         0    0          0    0       1  100.666667  56.666667  215.0   54.0  136.0  128.0       1.0  \n",
       "2  17.631713    UNDER         0    0          0    0       0  108.666667  62.000000  129.0   30.0   58.0  202.0       4.0  \n",
       "3        NaN     <NA>         0    0          1    0       0   95.333333  61.333333  211.0   43.0  161.0   37.0       7.0  \n",
       "4  29.096386     OVER         0    1          1    0       0  122.000000  82.666667  279.0   42.0  168.0  347.0       3.0  \n",
       "5  22.557537   NORMAL         0    0          0    0       0  114.666667  68.000000  153.0   61.0   59.0  181.0       2.0  \n",
       "6  29.393577     OVER         0    0          1    0       0  125.333333  80.000000  245.0  105.0  127.0   62.0       1.0  \n",
       "7  15.508307    UNDER         0    0          0    0       0  100.666667  49.333333  162.0   67.0   88.0   33.0       7.0  \n",
       "8  18.482704    UNDER         0    0          0    0       0  109.333333  53.333333  148.0   58.0   79.0   56.0       4.0  \n",
       "9  30.936955    OBESE         0    1          0    0       0  145.333333  96.000000  140.0   51.0   80.0   45.0       1.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# delete later \n",
    "# --- Build a minimal DEMO with survey design fields, then rebuild core ---\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "ROOT    = Path(\"/Users/dengshuyue/Desktop/SDOH/analysis\")\n",
    "INTERIM = ROOT / \"interim\"\n",
    "INTERIM.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "NEEDED = [\"SEQN\",\"SDDSRVYR\",\"SDMVPSU\",\"SDMVSTRA\",\"WTMEC2YR\"]\n",
    "\n",
    "def _cols_ok(df, need=NEEDED):\n",
    "    cols = [c.upper() for c in df.columns]\n",
    "    return all(c in cols for c in need)\n",
    "\n",
    "# 1) Try existing candidates first\n",
    "candidates = [\n",
    "    CONFIG.demo_9923,\n",
    "    ROOT/\"output\"/\"nhanes_mort_demo_sdoh_1999_2018.parquet\",\n",
    "    ROOT/\"data\"/\"demo9923.csv\",\n",
    "]\n",
    "\n",
    "chosen = None\n",
    "why = None\n",
    "for p in candidates:\n",
    "    if p is None: \n",
    "        continue\n",
    "    p = Path(p)\n",
    "    if not p.exists():\n",
    "        continue\n",
    "    try:\n",
    "        df = pd.read_parquet(p) if p.suffix==\".parquet\" else pd.read_csv(p, low_memory=False)\n",
    "        df.columns = [c.upper() for c in df.columns]\n",
    "        if _cols_ok(df):\n",
    "            # minimal clean\n",
    "            df = df[NEEDED].copy()\n",
    "            df[\"SEQN\"] = pd.to_numeric(df[\"SEQN\"], errors=\"coerce\")\n",
    "            df = df.dropna(subset=[\"SEQN\"]).drop_duplicates(\"SEQN\")\n",
    "            outp = INTERIM/\"survey_design_9923.parquet\"\n",
    "            df.to_parquet(outp, index=False)\n",
    "            chosen = outp\n",
    "            why = f\"Using existing file → {p}\"\n",
    "            break\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# 2) If not found, build from raw DEMO XPTs\n",
    "if chosen is None:\n",
    "    import glob\n",
    "    xpt_dirs = [\n",
    "        ROOT/\"data\"/\"nhanes_by_module\"/\"DEMO\",\n",
    "        ROOT/\"data\"/\"nhanes_deit\",\n",
    "        ROOT/\"data\"/\"household_size\",\n",
    "    ]\n",
    "    frames = []\n",
    "    for d in xpt_dirs:\n",
    "        for xpt in glob.glob(str(d/\"*.xpt\")):\n",
    "            try:\n",
    "                df = pd.read_sas(xpt, format=\"xport\")\n",
    "                df.columns = [c.upper() for c in df.columns]\n",
    "                if \"SEQN\" not in df.columns:\n",
    "                    continue\n",
    "                # keep only available of the needed ones\n",
    "                keep = [c for c in NEEDED if c in df.columns]\n",
    "                if not {\"SDMVPSU\",\"SDMVSTRA\",\"WTMEC2YR\"}.issubset(set(keep)):\n",
    "                    # skip if this file doesn't have the design trio\n",
    "                    continue\n",
    "                sub = df[keep].copy()\n",
    "                frames.append(sub)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "    if not frames:\n",
    "        raise RuntimeError(\"Couldn't build DEMO survey table from raw XPTs; no suitable files with SDMVPSU/SDMVSTRA/WTMEC2YR were found.\")\n",
    "\n",
    "    demo = pd.concat(frames, ignore_index=True)\n",
    "    demo = demo.drop_duplicates(subset=[\"SEQN\"], keep=\"first\")\n",
    "    # Normalize types\n",
    "    demo[\"SEQN\"]     = pd.to_numeric(demo[\"SEQN\"], errors=\"coerce\")\n",
    "    demo[\"SDDSRVYR\"] = pd.to_numeric(demo[\"SDDSRVYR\"], errors=\"coerce\")\n",
    "    demo[\"SDMVPSU\"]  = pd.to_numeric(demo[\"SDMVPSU\"], errors=\"coerce\")\n",
    "    demo[\"SDMVSTRA\"] = pd.to_numeric(demo[\"SDMVSTRA\"], errors=\"coerce\")\n",
    "    demo[\"WTMEC2YR\"] = pd.to_numeric(demo[\"WTMEC2YR\"], errors=\"coerce\")\n",
    "\n",
    "    demo = demo.dropna(subset=[\"SEQN\"]).drop_duplicates(\"SEQN\")\n",
    "    outp = INTERIM/\"survey_design_9923.parquet\"\n",
    "    demo.to_parquet(outp, index=False)\n",
    "    chosen = outp\n",
    "    why = f\"Built from raw DEMO XPTs under {ROOT}/data\"\n",
    "\n",
    "print(\"Survey DEMO source:\", why)\n",
    "print(\"→\", chosen)\n",
    "\n",
    "# Point CONFIG to the newly created survey table and rebuild core\n",
    "CONFIG.demo_9923 = Path(chosen)\n",
    "\n",
    "# Rebuild just the core (smk/pa already produced)\n",
    "core = build_core(CONFIG)\n",
    "print(\"✓ cov_core written to:\", CONFIG.out_dir / CONFIG.cov_core)\n",
    "\n",
    "# Quick sanity\n",
    "print(quick_checks(CONFIG))\n",
    "\n",
    "# Peek a few rows\n",
    "display(core.sort_values(\"SEQN\").head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b530b1e-0ac4-47df-a1d7-52c4ff103438",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf98529-7f4c-42e8-befb-292834cbd4fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9355d113-44aa-40ec-8d6d-2cb8d2afca46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac3d967-3ae4-4978-a096-a07965ed091c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "75328b00-d164-49a0-878c-83d9ed2b5f5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SMK_STATUS %NA</th>\n",
       "      <th>LTPA_MET_HR_WK %NA</th>\n",
       "      <th>BMI %NA</th>\n",
       "      <th>WTMEC2YR %NA</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SDDSRVYR</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1999–2018</th>\n",
       "      <td>45.7</td>\n",
       "      <td>45.6</td>\n",
       "      <td>13.3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019–2023</th>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           SMK_STATUS %NA  LTPA_MET_HR_WK %NA  BMI %NA  WTMEC2YR %NA\n",
       "SDDSRVYR                                                            \n",
       "1999–2018            45.7                45.6     13.3           0.0\n",
       "2019–2023           100.0               100.0    100.0           0.0"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "core = pd.read_parquet(\"/Users/dengshuyue/Desktop/SDOH/analysis/out/cov_core_1999_2023.parquet\")\n",
    "\n",
    "era = (core[\"SDDSRVYR\"] <= 10).map({True:\"1999–2018\", False:\"2019–2023\"})\n",
    "def miss(col): return core[col].isna().groupby(era).mean().mul(100).round(1)\n",
    "\n",
    "checks = pd.DataFrame({\n",
    "    \"SMK_STATUS %NA\": miss(\"SMK_STATUS\"),\n",
    "    \"LTPA_MET_HR_WK %NA\": miss(\"LTPA_MET_HR_WK\"),\n",
    "    \"BMI %NA\": miss(\"BMI\"),\n",
    "    \"WTMEC2YR %NA\": miss(\"WTMEC2YR\"),\n",
    "})\n",
    "checks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9153bd64-1723-4043-a06c-158c8a32c05c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c843df16-ac3e-48dd-baa6-b0f6c6e365dc",
   "metadata": {},
   "source": [
    "<h2> Step 2. add Alcohol by fetch </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "26bf3d46-21ec-476d-94f4-09751d83ee96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ wrote /Users/dengshuyue/Desktop/SDOH/analysis/output/cov_alc_1999_2023.parquet\n",
      "                 n\n",
      "ALCOHOL_CAT       \n",
      "none         42792\n",
      "moderate     20674\n",
      "heavy         5395\n"
     ]
    }
   ],
   "source": [
    "# %% NHANES ALQ → DRINKS_PER_DAY + ALCOHOL_CAT (proposal thresholds)\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "# --------- YOUR fixed locations ---------\n",
    "BASE        = Path(\"/Users/dengshuyue/Desktop/SDOH/analysis\")\n",
    "OUTPUT_DIR  = BASE / \"output\"                # final parquet location\n",
    "DATA_COV    = BASE / \"data\" / \"cov\"          # keep inputs here\n",
    "ALC_STORE   = DATA_COV / \"alcohol\"           # ONLY new subfolder we may create\n",
    "DEMO_PKL    = BASE / \"data\" / \"demo9923.pkl\"           # your existing demo\n",
    "DEMO_PARQ   = DATA_COV / \"demo9923.parquet\"            # canonical fallback we may build\n",
    "\n",
    "ALC_STORE.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DATA_COV.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --------- URL map (two mirrors each) ---------\n",
    "ALQ_URLS = {\n",
    "    \"1999-2000\": [\n",
    "        \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/1999/DataFiles/ALQ.xpt\",\n",
    "        \"https://wwwn.cdc.gov/Nchs/Nhanes/1999-2000/ALQ.XPT\",\n",
    "    ],\n",
    "    \"2001-2002\": [\n",
    "        \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2001/DataFiles/ALQ_B.xpt\",\n",
    "        \"https://wwwn.cdc.gov/Nchs/Nhanes/2001-2002/ALQ_B.XPT\",\n",
    "    ],\n",
    "    \"2003-2004\": [\n",
    "        \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2003/DataFiles/ALQ_C.xpt\",\n",
    "        \"https://wwwn.cdc.gov/Nchs/Nhanes/2003-2004/ALQ_C.XPT\",\n",
    "    ],\n",
    "    \"2005-2006\": [\n",
    "        \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2005/DataFiles/ALQ_D.xpt\",\n",
    "        \"https://wwwn.cdc.gov/Nchs/Nhanes/2005-2006/ALQ_D.XPT\",\n",
    "    ],\n",
    "    \"2007-2008\": [\n",
    "        \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2007/DataFiles/ALQ_E.xpt\",\n",
    "        \"https://wwwn.cdc.gov/Nchs/Nhanes/2007-2008/ALQ_E.XPT\",\n",
    "    ],\n",
    "    \"2009-2010\": [\n",
    "        \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2009/DataFiles/ALQ_F.xpt\",\n",
    "        \"https://wwwn.cdc.gov/Nchs/Nhanes/2009-2010/ALQ_F.XPT\",\n",
    "    ],\n",
    "    \"2011-2012\": [\n",
    "        \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2011/DataFiles/ALQ_G.xpt\",\n",
    "        \"https://wwwn.cdc.gov/Nchs/Nhanes/2011-2012/ALQ_G.XPT\",\n",
    "    ],\n",
    "    \"2013-2014\": [\n",
    "        \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2013/DataFiles/ALQ_H.xpt\",\n",
    "        \"https://wwwn.cdc.gov/Nchs/Nhanes/2013-2014/ALQ_H.XPT\",\n",
    "    ],\n",
    "    \"2015-2016\": [\n",
    "        \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2015/DataFiles/ALQ_I.xpt\",\n",
    "        \"https://wwwn.cdc.gov/Nchs/Nhanes/2015-2016/ALQ_I.XPT\",\n",
    "    ],\n",
    "    \"2017-2018\": [\n",
    "        \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2017/DataFiles/ALQ_J.xpt\",\n",
    "        \"https://wwwn.cdc.gov/Nchs/Nhanes/2017-2018/ALQ_J.XPT\",\n",
    "    ],\n",
    "    \"2017-March 2020 (pre-pandemic)\": [\n",
    "        \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2017/DataFiles/P_ALQ.xpt\",\n",
    "        \"https://wwwn.cdc.gov/Nchs/Nhanes/2017-2020/P_ALQ.XPT\",\n",
    "    ],\n",
    "    \"August 2021–August 2023\": [\n",
    "        \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2021/DataFiles/ALQ_L.xpt\",\n",
    "        \"https://wwwn.cdc.gov/Nchs/Nhanes/2021-2023/ALQ_L.XPT\",\n",
    "        \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2021/DataFiles/ALQ_Q.xpt\",\n",
    "        \"https://wwwn.cdc.gov/Nchs/Nhanes/2021-2023/ALQ_Q.XPT\",\n",
    "    ],\n",
    "}\n",
    "CYCLES = list(ALQ_URLS.keys())\n",
    "\n",
    "# --------- helpers ---------\n",
    "def fetch(url: str, dest: Path, timeout=90):\n",
    "    headers={\"User-Agent\":\"nhanes-fetch/1.0\"}\n",
    "    with requests.get(url, headers=headers, stream=True, timeout=timeout) as r:\n",
    "        r.raise_for_status()\n",
    "        tmp = dest.with_suffix(dest.suffix + \".downloading\")\n",
    "        with open(tmp, \"wb\") as f:\n",
    "            for chunk in r.iter_content(1<<15):\n",
    "                if chunk: f.write(chunk)\n",
    "        tmp.rename(dest)\n",
    "    return dest\n",
    "\n",
    "def ensure_xpt(cycle: str) -> Path:\n",
    "    \"\"\"Return local ALQ .xpt for cycle stored under data/cov/alcohol/.\"\"\"\n",
    "    for url in ALQ_URLS[cycle]:\n",
    "        name = Path(url).name\n",
    "        local = ALC_STORE / name\n",
    "        if local.exists():\n",
    "            return local\n",
    "        try:\n",
    "            print(f\"⬇️  {cycle} → {name}\")\n",
    "            return fetch(url, local)\n",
    "        except Exception as e:\n",
    "            print(f\"   ⚠️ {e}\")\n",
    "    raise FileNotFoundError(f\"ALQ not available for {cycle}\")\n",
    "\n",
    "def read_xpt(p: Path) -> pd.DataFrame:\n",
    "    try:\n",
    "        import pyreadstat\n",
    "        df, _ = pyreadstat.read_xport(p)\n",
    "    except Exception:\n",
    "        df = pd.read_sas(p, format=\"xport\")\n",
    "    df.columns = [c.upper() for c in df.columns]\n",
    "    return df\n",
    "\n",
    "NH_MISS = {7, 9, 77, 99, 777, 999, 7777, 9999, 77777, 99999}\n",
    "def clean_num(s: pd.Series) -> pd.Series:\n",
    "    s = pd.to_numeric(s, errors=\"coerce\")\n",
    "    return s.mask(s.isin(NH_MISS))\n",
    "\n",
    "# --------- load ALQ across cycles ---------\n",
    "parts = []\n",
    "for cycle in CYCLES:\n",
    "    try:\n",
    "        fp = ensure_xpt(cycle)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"⚠️ {e}\"); continue\n",
    "    df = read_xpt(fp)\n",
    "    df[\"CYCLE\"] = cycle\n",
    "    keep = [c for c in [\"SEQN\",\"CYCLE\",\"ALQ110\",\"ALQ101\",\"ALQ151\",\"ALQ120Q\",\"ALQ120U\",\"ALQ130\"] if c in df.columns]\n",
    "    parts.append(df[keep])\n",
    "if not parts:\n",
    "    raise RuntimeError(\"No ALQ data available.\")\n",
    "alq = pd.concat(parts, ignore_index=True)\n",
    "\n",
    "# --------- derive DRINKS_PER_DAY (per proposal) ---------\n",
    "count = clean_num(alq.get(\"ALQ120Q\", pd.Series(np.nan, index=alq.index)))\n",
    "unit  = alq.get(\"ALQ120U\", pd.Series(np.nan, index=alq.index))\n",
    "per_year = pd.Series(np.nan, index=alq.index, dtype=\"float\")\n",
    "per_year = per_year.where(~(unit == 1), 365.0)\n",
    "per_year = per_year.where(~(unit == 2), 52.142)\n",
    "per_year = per_year.where(~(unit == 3), 12.0)\n",
    "per_year = per_year.where(~(unit == 4), 1.0)\n",
    "occasions_per_year = count * per_year\n",
    "\n",
    "drinks_per_occasion = clean_num(alq.get(\"ALQ130\", pd.Series(np.nan, index=alq.index)))\n",
    "dpd = (occasions_per_year * drinks_per_occasion) / 365.0\n",
    "dpd = dpd.where(occasions_per_year.notna() & drinks_per_occasion.notna())\n",
    "\n",
    "# lifetime <12 indicator (prefer ALQ110; fall back to ALQ151; else NA)\n",
    "life_lt12 = None\n",
    "if \"ALQ110\" in alq.columns:\n",
    "    life_lt12 = (alq[\"ALQ110\"] == 2).astype(\"Int8\")\n",
    "elif \"ALQ151\" in alq.columns:\n",
    "    life_lt12 = (alq[\"ALQ151\"] == 2).astype(\"Int8\")\n",
    "\n",
    "alc_min = alq[[\"SEQN\"]].copy()\n",
    "alc_min[\"DRINKS_PER_DAY\"] = dpd\n",
    "if life_lt12 is not None:\n",
    "    alc_min[\"LIFETIME_LT12\"] = life_lt12\n",
    "\n",
    "# --------- load DEMO sex (prefer your .pkl; else cov/demo9923.parquet; else build minimal) ---------\n",
    "if DEMO_PKL.exists():\n",
    "    demo = pd.read_pickle(DEMO_PKL)\n",
    "elif DEMO_PARQ.exists():\n",
    "    demo = pd.read_parquet(DEMO_PARQ)\n",
    "else:\n",
    "    # Minimal build (SEQN, RIAGENDR) only, stored exactly at data/cov/demo9923.parquet\n",
    "    print(\"ℹ️  DEMO not found — building minimal RIAGENDR stack into\", DEMO_PARQ)\n",
    "    DEMO_URLS = {\n",
    "        \"1999-2000\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/1999/DataFiles/DEMO.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/1999-2000/DEMO.XPT\",\n",
    "        ],\n",
    "        \"2001-2002\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2001/DataFiles/DEMO_B.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2001-2002/DEMO_B.XPT\",\n",
    "        ],\n",
    "        \"2003-2004\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2003/DataFiles/DEMO_C.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2003-2004/DEMO_C.XPT\",\n",
    "        ],\n",
    "        \"2005-2006\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2005/DataFiles/DEMO_D.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2005-2006/DEMO_D.XPT\",\n",
    "        ],\n",
    "        \"2007-2008\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2007/DataFiles/DEMO_E.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2007-2008/DEMO_E.XPT\",\n",
    "        ],\n",
    "        \"2009-2010\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2009/DataFiles/DEMO_F.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2009-2010/DEMO_F.XPT\",\n",
    "        ],\n",
    "        \"2011-2012\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2011/DataFiles/DEMO_G.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2011-2012/DEMO_G.XPT\",\n",
    "        ],\n",
    "        \"2013-2014\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2013/DataFiles/DEMO_H.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2013-2014/DEMO_H.XPT\",\n",
    "        ],\n",
    "        \"2015-2016\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2015/DataFiles/DEMO_I.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2015-2016/DEMO_I.XPT\",\n",
    "        ],\n",
    "        \"2017-2018\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2017/DataFiles/DEMO_J.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2017-2018/DEMO_J.XPT\",\n",
    "        ],\n",
    "        \"2017-March 2020 (pre-pandemic)\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2017/DataFiles/P_DEMO.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2017-2020/P_DEMO.XPT\",\n",
    "        ],\n",
    "        \"August 2021–August 2023\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2021/DataFiles/DEMO_L.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2021-2023/DEMO_L.XPT\",\n",
    "        ],\n",
    "    }\n",
    "    def read_xpt_min(p: Path) -> pd.DataFrame:\n",
    "        try:\n",
    "            import pyreadstat\n",
    "            df, _ = pyreadstat.read_xport(p)\n",
    "        except Exception:\n",
    "            df = pd.read_sas(p, format=\"xport\")\n",
    "        df.columns = [c.upper() for c in df.columns]\n",
    "        return df\n",
    "\n",
    "    demo_parts = []\n",
    "    for cyc, urls in DEMO_URLS.items():\n",
    "        got = None\n",
    "        for u in urls:\n",
    "            try:\n",
    "                name = Path(u).name\n",
    "                dest = DATA_COV / name  # store right under data/cov\n",
    "                if not dest.exists():\n",
    "                    print(f\"⬇️  DEMO {cyc} → {name}\")\n",
    "                    fetch(u, dest)\n",
    "                got = dest; break\n",
    "            except Exception as e:\n",
    "                print(\"   ⚠️\", e)\n",
    "        if got is None:\n",
    "            continue\n",
    "        demo_df = read_xpt_min(got)\n",
    "        keep = [c for c in [\"SEQN\",\"RIAGENDR\"] if c in demo_df.columns]\n",
    "        if keep:\n",
    "            demo_parts.append(demo_df[keep])\n",
    "    if not demo_parts:\n",
    "        raise RuntimeError(\"Could not build minimal DEMO (RIAGENDR).\")\n",
    "    demo = pd.concat(demo_parts, ignore_index=True).drop_duplicates(\"SEQN\")\n",
    "    demo.to_parquet(DEMO_PARQ, index=False)\n",
    "\n",
    "# standardize columns\n",
    "demo.columns = [c.upper() for c in demo.columns]\n",
    "demo = demo.drop_duplicates(\"SEQN\")\n",
    "\n",
    "# --------- categorize per proposal (safe dtype handling; no np.select) ---------\n",
    "sex  = pd.to_numeric(demo.set_index(\"SEQN\")[\"RIAGENDR\"].reindex(alc_min[\"SEQN\"]), errors=\"coerce\")  # 1=male, 2=female\n",
    "dpd  = pd.to_numeric(alc_min[\"DRINKS_PER_DAY\"], errors=\"coerce\")\n",
    "life = pd.to_numeric(alc_min.get(\"LIFETIME_LT12\", pd.Series(pd.NA, index=alc_min.index)), errors=\"coerce\")\n",
    "\n",
    "none_mask     = dpd.isna() | (dpd < 0.03) | (life == 1)                    # <12 lifetime OR <0.03/day\n",
    "heavy_mask    = ((sex == 2) & (dpd >= 1.0)) | ((sex == 1) & (dpd >= 2.0))  # women ≥1/day; men ≥2/day\n",
    "moderate_mask = (~none_mask) & (~heavy_mask)\n",
    "\n",
    "cat = pd.Series(\"none\", index=alc_min.index, dtype=\"object\")\n",
    "cat.loc[moderate_mask] = \"moderate\"\n",
    "cat.loc[heavy_mask]    = \"heavy\"\n",
    "\n",
    "alc_min[\"ALCOHOL_CAT\"] = pd.Categorical(cat, categories=[\"none\",\"moderate\",\"heavy\"], ordered=True)\n",
    "\n",
    "# --------- write to YOUR output folder ---------\n",
    "out_path = OUTPUT_DIR / \"cov_alc_1999_2023.parquet\"\n",
    "alc_min.to_parquet(out_path, index=False)\n",
    "print(\"✅ wrote\", out_path)\n",
    "print(alc_min[\"ALCOHOL_CAT\"].value_counts(dropna=False).rename_axis(\"ALCOHOL_CAT\").to_frame(\"n\").head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe51341f-44f3-40e2-901e-e86dd89ef196",
   "metadata": {},
   "source": [
    "<h2> Step2.1 add Alcohol back to core </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f57b4942-db00-446b-bd9c-1c75a738d1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ wrote /Users/dengshuyue/Desktop/SDOH/analysis/output/cov_core_1999_2023_v2.parquet\n",
      "                 n\n",
      "ALCOHOL_CAT       \n",
      "NaN          59948\n",
      "none         42792\n",
      "moderate     20674\n",
      "heavy         5395\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "BASE = Path(\"/Users/dengshuyue/Desktop/SDOH/analysis\")\n",
    "core_path = BASE / \"output\" / \"cov_core_1999_2023.parquet\"\n",
    "alc_path  = BASE / \"output\" / \"cov_alc_1999_2023.parquet\"\n",
    "\n",
    "core = pd.read_parquet(core_path)\n",
    "alc  = pd.read_parquet(alc_path)[[\"SEQN\",\"DRINKS_PER_DAY\",\"ALCOHOL_CAT\"]]\n",
    "\n",
    "core_v2 = core.merge(alc, on=\"SEQN\", how=\"left\")\n",
    "out_core_v2 = BASE / \"output\" / \"cov_core_1999_2023_v2.parquet\"\n",
    "core_v2.to_parquet(out_core_v2, index=False)\n",
    "\n",
    "print(\"✅ wrote\", out_core_v2)\n",
    "print(core_v2[\"ALCOHOL_CAT\"].value_counts(dropna=False).rename_axis(\"ALCOHOL_CAT\").to_frame(\"n\").head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7922cb-2c00-45d6-8b70-f294240b410b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7fcea7-5b23-44d0-a1c3-d6357dff1652",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15a353b-3ab4-49b5-9362-d55289d23ae5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8691be6b-4475-462c-b172-dabb69988ff6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2950ee17-8c65-4dfd-abb7-62053cdacfa1",
   "metadata": {},
   "source": [
    "<h2> Point config to your actual files (1999–2018)</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3db31755-cf17-4a7b-86ba-68c96628c660",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def _normalize_sex(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"Map common encodings to 'M'/'F'. Unknown -> <NA>.\"\"\"\n",
    "    s = series.copy()\n",
    "    # unify case\n",
    "    if s.dtype == \"O\" or pd.api.types.is_string_dtype(s):\n",
    "        s = s.astype(\"string\").str.strip().str.upper()\n",
    "        s = s.replace(\n",
    "            {\n",
    "                \"MALE\": \"M\", \"M\": \"M\", \"1\": \"M\", \"01\": \"M\",\n",
    "                \"FEMALE\": \"F\", \"F\": \"F\", \"2\": \"F\", \"02\": \"F\",\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        # numeric codes like 1/2\n",
    "        s = pd.to_numeric(s, errors=\"coerce\")\n",
    "        s = s.map({1: \"M\", 2: \"F\"}).astype(\"string\")\n",
    "    # anything else becomes <NA>\n",
    "    s = s.where(s.isin([\"M\", \"F\"]), pd.NA)\n",
    "    return s\n",
    "\n",
    "def _get_sex_from_demo(demo_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    d = demo_df.copy()\n",
    "    d.columns = [c.upper() for c in d.columns]\n",
    "    # try common candidates\n",
    "    cand = [c for c in d.columns if c.upper() in\n",
    "            {\"RIAGENDR\",\"SEX\",\"GENDER\",\"RIAGENDR_X\",\"RIAGENDR_Y\",\"SEX_CODE\"}]\n",
    "    if not cand:\n",
    "        # fuzzy search\n",
    "        cand = [c for c in d.columns if \"GEND\" in c.upper() or c.upper().startswith(\"SEX\")]\n",
    "    if cand:\n",
    "        sex = _normalize_sex(d[cand[0]])\n",
    "        return pd.DataFrame({\"SEQN\": d[\"SEQN\"], \"SEX\": sex})\n",
    "    else:\n",
    "        # no sex column; return NA-sex\n",
    "        return pd.DataFrame({\"SEQN\": d[\"SEQN\"], \"SEX\": pd.Series(pd.NA, index=d.index, dtype=\"string\")})\n",
    "\n",
    "def categorize_alcohol(drinks_per_day: pd.Series,\n",
    "                       sex: pd.Series,\n",
    "                       lifetime_lt12: pd.Series | None = None) -> pd.Series:\n",
    "    \"\"\"CDC cats; if sex unknown, only label NONE for zero/lifetime<12; else NA.\"\"\"\n",
    "    dpd = pd.to_numeric(drinks_per_day, errors=\"coerce\")\n",
    "    sex = sex.astype(\"string\").str.upper()\n",
    "\n",
    "    # Start as NA; fill to NONE for zero intake\n",
    "    cat = pd.Series(pd.NA, index=dpd.index, dtype=\"string\")\n",
    "\n",
    "    # Lifetime <12 drinks -> NONE\n",
    "    if lifetime_lt12 is not None:\n",
    "        cat = cat.where(~(lifetime_lt12 == 1), \"NONE\")\n",
    "\n",
    "    # 0 drinks/day -> NONE regardless of sex\n",
    "    cat = cat.where(~(dpd.fillna(0) == 0), \"NONE\")\n",
    "\n",
    "    # Apply sex-specific thresholds where sex is known and dpd>0\n",
    "    maskF = (sex == \"F\") & (dpd > 0)\n",
    "    maskM = (sex == \"M\") & (dpd > 0)\n",
    "\n",
    "    # Moderate\n",
    "    cat = cat.mask(maskF & (dpd <= 1), \"MODERATE\")\n",
    "    cat = cat.mask(maskM & (dpd <= 2), \"MODERATE\")\n",
    "\n",
    "    # Heavy (overwrites moderate where applicable)\n",
    "    cat = cat.mask(maskF & (dpd > 1), \"HEAVY\")\n",
    "    cat = cat.mask(maskM & (dpd > 2), \"HEAVY\")\n",
    "\n",
    "    return cat.str.upper()\n",
    "\n",
    "def build_alc(cfg: Config = CONFIG) -> pd.DataFrame:\n",
    "    ensure_dir(cfg.out_dir)\n",
    "\n",
    "    # Load DEMO (any of your configured/demo interim paths)\n",
    "    demo_p = None\n",
    "    if cfg.demo_9923 and Path(cfg.demo_9923).exists():\n",
    "        demo_p = Path(cfg.demo_9923)\n",
    "    elif cfg.demo_9918 and Path(cfg.demo_9918).exists():\n",
    "        demo_p = Path(cfg.demo_9918)\n",
    "    else:\n",
    "        demo_p = pick_first_existing(cfg.interim_dir / \"demo_9923.parquet\",\n",
    "                                     cfg.interim_dir / \"demo_9918.parquet\")\n",
    "    if demo_p is None:\n",
    "        raise FileNotFoundError(\"Need DEMO table for ALQ (demo_9923/demo_9918).\")\n",
    "\n",
    "    demo = pd.read_parquet(demo_p) if str(demo_p).endswith(\".parquet\") else pd.read_csv(demo_p, low_memory=False)\n",
    "    demo = demo.rename(columns={c: c.upper() for c in demo.columns})\n",
    "    demo_sex = _get_sex_from_demo(demo)  # -> SEQN, SEX ('M'/'F'/NA)\n",
    "\n",
    "    # Load ALQ stack (prefer 99–23, else 99–18)\n",
    "    alq_9923 = cfg.interim_dir / \"alq_9923.parquet\"\n",
    "    alq_9918 = cfg.interim_dir / \"alq_9918.parquet\"\n",
    "    if alq_9923.exists():\n",
    "        alq = pd.read_parquet(alq_9923)\n",
    "    elif alq_9918.exists():\n",
    "        alq = pd.read_parquet(alq_9918)\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Provide ./interim/alq_9923.parquet (preferred) or ./interim/alq_9918.parquet.\")\n",
    "    alq = alq.rename(columns={c: c.upper() for c in alq.columns})\n",
    "\n",
    "    # Drinks/day + lifetime <12 flag\n",
    "    drinks_per_day = _drinks_per_day_from_alq(alq)\n",
    "    lifetime_lt12 = None\n",
    "    if \"ALQ110\" in alq.columns:\n",
    "        # ALQ110: “Had at least 12 drinks in lifetime?” 1=Yes, 2=No -> lifetime_lt12 = 1 if 'No'\n",
    "        lifetime_lt12 = (alq[\"ALQ110\"] == 2).astype(\"Int8\")\n",
    "\n",
    "    # Bring in SEX where available\n",
    "    alq_sex = alq[[\"SEQN\"]].merge(demo_sex, on=\"SEQN\", how=\"left\")[\"SEX\"]\n",
    "\n",
    "    alc_cat = categorize_alcohol(drinks_per_day, sex=alq_sex, lifetime_lt12=lifetime_lt12)\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"SEQN\": alq[\"SEQN\"],\n",
    "        \"DRINKS_PER_DAY\": drinks_per_day,\n",
    "        \"ALCOHOL_CAT\": alc_cat,\n",
    "    })\n",
    "    out.to_parquet(cfg.out_dir / cfg.cov_alc, index=False)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8bfcabd8-e54f-4ca5-841c-e4be6f444089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PATCH: robust Physical Activity loader ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def _read_any(p: Path) -> pd.DataFrame:\n",
    "    return pd.read_parquet(p) if str(p).endswith(\".parquet\") else pd.read_csv(p, low_memory=False)\n",
    "\n",
    "def _find_first(dcols, candidates):\n",
    "    for c in candidates:\n",
    "        if c in dcols: \n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def _find_by_partials(dcols, must_have=(), any_of=()):\n",
    "    for c in dcols:\n",
    "        u = c.upper()\n",
    "        if all(m in u for m in must_have) and (not any_of or any(a in u for a in any_of)):\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def build_pa(cfg: Config = CONFIG) -> pd.DataFrame:\n",
    "    ensure_dir(cfg.out_dir)\n",
    "\n",
    "    # Load combined or 99–18 (+ optional 19–23)\n",
    "    pa_9923 = pick_first_existing(\n",
    "        cfg.interim_dir / \"totalpa_9923_imputed.parquet\",\n",
    "        cfg.interim_dir / \"totalpa_9923_imputed.csv\",\n",
    "    )\n",
    "    if pa_9923:\n",
    "        pa = _read_any(pa_9923)\n",
    "    else:\n",
    "        pa_9918 = pick_first_existing(\n",
    "            cfg.pa_9918_imputed,\n",
    "            cfg.interim_dir / \"totalpa_9918_imputed.parquet\",\n",
    "            cfg.interim_dir / \"totalpa_9918_imputed.csv\",\n",
    "        )\n",
    "        if pa_9918 is None:\n",
    "            raise FileNotFoundError(\"Provide totalpa_9923_imputed or totalpa_9918_imputed under ./interim/.\")\n",
    "        pa = _read_any(pa_9918)\n",
    "        pa_1923 = pick_first_existing(\n",
    "            cfg.pa_1923,\n",
    "            cfg.interim_dir / \"totalpa_1923_imputed.parquet\",\n",
    "            cfg.interim_dir / \"totalpa_1923_imputed.csv\",\n",
    "        )\n",
    "        if pa_1923:\n",
    "            pa = pd.concat([pa, _read_any(pa_1923)], ignore_index=True)\n",
    "\n",
    "    # Normalize columns\n",
    "    pa = pa.copy()\n",
    "    pa.columns = [c.upper() for c in pa.columns]\n",
    "    if \"SEQN\" not in pa.columns:\n",
    "        raise ValueError(\"PA table missing SEQN.\")\n",
    "\n",
    "    # 1) Find MET hours/week (or convert from minutes)\n",
    "    cols = pa.columns\n",
    "    met_hr_candidates_exact = [\n",
    "        \"LTPA_MET_HR_WK\", \"TOTALPA_MET_HR_WK\", \"TOTPA_MET_HR_WK\",\n",
    "        \"PA_MET_HR_WK\", \"MET_HR_WK\", \"TOTAL_PA_MET_HR_WK\"\n",
    "    ]\n",
    "    met_min_candidates_exact = [\n",
    "        \"LTPA_MET_MIN_WK\", \"TOTALPA_MET_MIN_WK\", \"TOTPA_MET_MIN_WK\",\n",
    "        \"PA_MET_MIN_WK\", \"MET_MIN_WK\", \"TOTAL_PA_MET_MIN_WK\"\n",
    "    ]\n",
    "    met_hr_col  = _find_first(cols, met_hr_candidates_exact)\n",
    "    met_min_col = _find_first(cols, met_min_candidates_exact)\n",
    "\n",
    "    # Fuzzy fallbacks\n",
    "    if met_hr_col is None:\n",
    "        met_hr_col = _find_by_partials(cols, must_have=(\"MET\",\"HR\"), any_of=(\"WK\",\"WEEK\"))\n",
    "    if met_min_col is None:\n",
    "        met_min_col = _find_by_partials(cols, must_have=(\"MET\",\"MIN\"), any_of=(\"WK\",\"WEEK\"))\n",
    "\n",
    "    if met_hr_col is not None:\n",
    "        met_hr = pd.to_numeric(pa[met_hr_col], errors=\"coerce\")\n",
    "    elif met_min_col is not None:\n",
    "        met_hr = pd.to_numeric(pa[met_min_col], errors=\"coerce\") / 60.0\n",
    "    else:\n",
    "        # last resort: if there is a total minutes col like \"TOTAL_MIN_WK\", convert; else create NA\n",
    "        tot_min_col = _find_by_partials(cols, must_have=(\"MIN\",), any_of=(\"WK\",\"WEEK\"))\n",
    "        if tot_min_col is not None:\n",
    "            met_hr = pd.to_numeric(pa[tot_min_col], errors=\"coerce\") / 60.0\n",
    "        else:\n",
    "            print(\"Warning: could not find MET hours or minutes; setting LTPA_MET_HR_WK to NaN.\")\n",
    "            met_hr = pd.Series(np.nan, index=pa.index, dtype=\"float\")\n",
    "\n",
    "    # 2) Find imputation flag (default 0)\n",
    "    flag_candidates_exact = [\n",
    "        \"LTPA_IMPUTED_FLAG\", \"PA_IMPUTED_FLAG\", \"TOTALPA_IMPUTED_FLAG\",\n",
    "        \"IMPUTED_FLAG\", \"IMPUTED\"\n",
    "    ]\n",
    "    flag_col = _find_first(cols, flag_candidates_exact)\n",
    "    if flag_col is None:\n",
    "        flag_col = _find_by_partials(cols, must_have=(\"IMPUT\",), any_of=(\"FLAG\",\"\"))\n",
    "\n",
    "    if flag_col is not None:\n",
    "        imputed = pd.to_numeric(pa[flag_col], errors=\"coerce\").fillna(0).astype(\"int8\")\n",
    "    else:\n",
    "        imputed = pd.Series(0, index=pa.index, dtype=\"int8\")\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"SEQN\": pa[\"SEQN\"],\n",
    "        \"LTPA_MET_HR_WK\": met_hr,\n",
    "        \"LTPA_IMPUTED_FLAG\": imputed,\n",
    "    })\n",
    "    out.to_parquet(cfg.out_dir / cfg.cov_pa, index=False)\n",
    "    return out\n",
    "# --- end patch ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f4f14167-8110-4bf1-95d2-67b5183d19d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PATCH: robust clinical builder (derives BMI_CLAS, HTN, HIGH_CHOL if missing) ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "\n",
    "def build_clinical(cfg: Config = CONFIG, thr: ClinicalThresholds = THR) -> pd.DataFrame:\n",
    "    ensure_dir(cfg.out_dir)\n",
    "\n",
    "    def _read_any(p: Path) -> pd.DataFrame:\n",
    "        return pd.read_parquet(p) if str(p).endswith(\".parquet\") else pd.read_csv(p, low_memory=False)\n",
    "\n",
    "    # Prefer combined 99–23 if present, else 99–18 (+ optional 19–23)\n",
    "    clin_9923 = pick_first_existing(\n",
    "        cfg.interim_dir / \"clinical_9923.parquet\",\n",
    "        cfg.interim_dir / \"clinical_9923.csv\",\n",
    "    )\n",
    "    if clin_9923:\n",
    "        clin = _read_any(clin_9923)\n",
    "    else:\n",
    "        p9918 = pick_first_existing(\n",
    "            cfg.clinical_9918,\n",
    "            cfg.interim_dir / \"clinical_9918.parquet\",\n",
    "            cfg.interim_dir / \"clinical_9918.csv\",\n",
    "            cfg.interim_dir / \"nhanes_primary_anal_full_singleimputation_v2.parquet\",\n",
    "            cfg.interim_dir / \"nhanes_primary_anal_full_singleimputation_v2.csv\",\n",
    "        )\n",
    "        p1923 = pick_first_existing(\n",
    "            cfg.clinical_1923,\n",
    "            cfg.interim_dir / \"clinical_1923.parquet\",\n",
    "            cfg.interim_dir / \"clinical_1923.csv\",\n",
    "        )\n",
    "        if p9918 is None:\n",
    "            raise FileNotFoundError(\"Provide clinical_9923 or clinical_9918 under ./interim/.\")\n",
    "        df_9918 = _read_any(p9918)\n",
    "        if p1923:\n",
    "            df_1923 = _read_any(p1923)\n",
    "            clin = pd.concat([df_9918, df_1923], ignore_index=True)\n",
    "        else:\n",
    "            clin = df_9918\n",
    "\n",
    "    clin = clin.copy()\n",
    "    clin.columns = [c.upper() for c in clin.columns]\n",
    "\n",
    "    # -------------------------\n",
    "    # BMI_CLAS (if missing)\n",
    "    # -------------------------\n",
    "    if \"BMI_CLAS\" not in clin.columns:\n",
    "        # Try BMI from BMX output; else any BMI in this table; else NA\n",
    "        bmi_series = None\n",
    "        try:\n",
    "            bmx_path = cfg.out_dir / cfg.cov_bmx\n",
    "            if bmx_path.exists():\n",
    "                bmx = pd.read_parquet(bmx_path)\n",
    "                bmx.columns = [c.upper() for c in bmx.columns]\n",
    "                if {\"SEQN\",\"BMI\"}.issubset(bmx.columns) and \"SEQN\" in clin.columns:\n",
    "                    bmi_map = bmx.set_index(\"SEQN\")[\"BMI\"]\n",
    "                    bmi_series = clin[\"SEQN\"].map(bmi_map)\n",
    "        except Exception:\n",
    "            pass\n",
    "        if bmi_series is None:\n",
    "            bmi_series = pd.to_numeric(clin.get(\"BMI\", np.nan), errors=\"coerce\")\n",
    "\n",
    "        def _bmi_class(x):\n",
    "            if pd.isna(x): return pd.NA\n",
    "            if x < 18.5:  return \"UNDER\"\n",
    "            if x < 25:    return \"NORMAL\"\n",
    "            if x < 30:    return \"OVER\"\n",
    "            return \"OBESE\"\n",
    "\n",
    "        clin[\"BMI_CLAS\"] = pd.Series([_bmi_class(v) for v in bmi_series], dtype=\"string\")\n",
    "\n",
    "    # -------------------------\n",
    "    # HTN (if missing)\n",
    "    # -------------------------\n",
    "    if \"HTN\" not in clin.columns:\n",
    "        # Look for common diagnosis/med flags\n",
    "        diag_col = next((c for c in clin.columns if ((\"HTN\" in c or \"HYPERT\" in c) and \"MED\" not in c and c != \"HTN\")), None)\n",
    "        med_col  = next((c for c in clin.columns if (\"MED\" in c and (\"BP\" in c or \"HYPER\" in c))), None)\n",
    "        sbp = pd.to_numeric(clin.get(\"SBP\", np.nan), errors=\"coerce\")\n",
    "        dbp = pd.to_numeric(clin.get(\"DBP\", np.nan), errors=\"coerce\")\n",
    "\n",
    "        htn = pd.Series(0, index=clin.index, dtype=\"Int8\")\n",
    "        if diag_col:\n",
    "            diag = pd.to_numeric(clin[diag_col], errors=\"coerce\")\n",
    "            htn = ((diag == 1) | (diag > 0)).astype(\"Int8\")\n",
    "        if med_col:\n",
    "            med = pd.to_numeric(clin[med_col], errors=\"coerce\")\n",
    "            htn = ((htn == 1) | (med == 1) | (med > 0)).astype(\"Int8\")\n",
    "        if (\"SBP\" in clin.columns) or (\"DBP\" in clin.columns):\n",
    "            htn = ((htn == 1) | (sbp >= thr.htn_sbp) | (dbp >= thr.htn_dbp)).astype(\"Int8\")\n",
    "\n",
    "        clin[\"HTN\"] = htn\n",
    "\n",
    "    # -------------------------\n",
    "    # HIGH_CHOL (if missing)\n",
    "    # -------------------------\n",
    "    if \"HIGH_CHOL\" not in clin.columns:\n",
    "        chol_med_col = next((c for c in clin.columns if (\"CHOL\" in c and \"MED\" in c)), None)\n",
    "        tch = pd.to_numeric(clin.get(\"TCHOL\", np.nan), errors=\"coerce\")\n",
    "        ldl = pd.to_numeric(clin.get(\"LDL\", np.nan), errors=\"coerce\")\n",
    "\n",
    "        high_chol = ((tch >= 240) | (ldl >= 160)).astype(\"Int8\")\n",
    "        if chol_med_col:\n",
    "            med = pd.to_numeric(clin[chol_med_col], errors=\"coerce\")\n",
    "            high_chol = ((high_chol == 1) | (med == 1) | (med > 0)).astype(\"Int8\")\n",
    "\n",
    "        clin[\"HIGH_CHOL\"] = high_chol\n",
    "\n",
    "    # Ensure remaining expected columns exist (fill as NA if absent)\n",
    "    keep = [\"SEQN\", \"BMI_CLAS\", \"DIABETES\", \"HTN\", \"HIGH_CHOL\", \"CVD\", \"CANCER\", \"SBP\", \"DBP\", \"TCHOL\", \"HDL\", \"LDL\", \"TG\"]\n",
    "    for col in keep:\n",
    "        if col not in clin.columns:\n",
    "            clin[col] = pd.Series(np.nan, index=clin.index)\n",
    "\n",
    "    out = clin[keep].copy()\n",
    "    for b in [\"DIABETES\", \"HTN\", \"HIGH_CHOL\", \"CVD\", \"CANCER\"]:\n",
    "        out[b] = pd.to_numeric(out[b], errors=\"coerce\").astype(\"Int8\")\n",
    "\n",
    "    out.to_parquet(cfg.out_dir / cfg.cov_clinical, index=False)\n",
    "    return out\n",
    "# --- end patch ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "db489fab-6b56-4d25-a2d6-75872c142831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PATCH: robust household builder ---\n",
    "import pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def _read_any_demo(p: Path) -> pd.DataFrame:\n",
    "    ext = p.suffix.lower()\n",
    "    if ext == \".parquet\": return pd.read_parquet(p)\n",
    "    if ext == \".csv\":     return pd.read_csv(p, low_memory=False)\n",
    "    # XPORT or sas7bdat\n",
    "    return pd.read_sas(p, format=\"xport\" if ext==\".xpt\" else None)\n",
    "\n",
    "def _stack_household_from_raw(search_root: Path) -> pd.DataFrame | None:\n",
    "    pats = [\"*DEMO*.xpt\",\"*DEMO*.sas7bdat\",\"*DEMO*.csv\",\"*DEMO*.parquet\"]\n",
    "    files = []\n",
    "    for pat in pats:\n",
    "        files += list(search_root.rglob(pat))\n",
    "    frames = []\n",
    "    seen = set()\n",
    "    for f in files:\n",
    "        if f in seen: \n",
    "            continue\n",
    "        seen.add(f)\n",
    "        try:\n",
    "            df = _read_any_demo(f)\n",
    "            df.columns = [c.upper() for c in df.columns]\n",
    "            if {\"SEQN\",\"DMDHHSIZ\"}.issubset(df.columns):\n",
    "                frames.append(df[[\"SEQN\",\"DMDHHSIZ\"]])\n",
    "        except Exception:\n",
    "            pass\n",
    "    if not frames:\n",
    "        return None\n",
    "    out = pd.concat(frames, ignore_index=True)\n",
    "    out = out.dropna(subset=[\"SEQN\"]).drop_duplicates(subset=[\"SEQN\"], keep=\"last\")\n",
    "    return out\n",
    "\n",
    "def build_household(cfg: Config = CONFIG) -> pd.DataFrame:\n",
    "    ensure_dir(cfg.out_dir)\n",
    "    # Pick the DEMO table you configured earlier (parquet or csv)\n",
    "    demo_p = None\n",
    "    if cfg.demo_9923 and Path(cfg.demo_9923).exists():\n",
    "        demo_p = Path(cfg.demo_9923)\n",
    "    elif cfg.demo_9918 and Path(cfg.demo_9918).exists():\n",
    "        demo_p = Path(cfg.demo_9918)\n",
    "    else:\n",
    "        demo_p = pick_first_existing(cfg.interim_dir / \"demo_9923.parquet\",\n",
    "                                     cfg.interim_dir / \"demo_9918.parquet\")\n",
    "    if demo_p is None:\n",
    "        raise FileNotFoundError(\"Need DEMO (demo_9923/demo_9918) to build household.\")\n",
    "\n",
    "    demo = pd.read_parquet(demo_p) if str(demo_p).endswith(\".parquet\") else pd.read_csv(demo_p, low_memory=False)\n",
    "    demo.columns = [c.upper() for c in demo.columns]\n",
    "    if \"SEQN\" not in demo.columns:\n",
    "        raise ValueError(\"DEMO missing SEQN.\")\n",
    "\n",
    "    # Case 1: DMDHHSIZ already present\n",
    "    if \"DMDHHSIZ\" in demo.columns:\n",
    "        out = demo[[\"SEQN\",\"DMDHHSIZ\"]].copy()\n",
    "        out.to_parquet(cfg.out_dir / cfg.cov_household, index=False)\n",
    "        return out\n",
    "\n",
    "    # Case 2: use DMDFMSIZ (family size) as proxy if available\n",
    "    if \"DMDFMSIZ\" in demo.columns:\n",
    "        out = demo[[\"SEQN\",\"DMDFMSIZ\"]].rename(columns={\"DMDFMSIZ\":\"DMDHHSIZ\"})\n",
    "        out.to_parquet(cfg.out_dir / cfg.cov_household, index=False)\n",
    "        print(\"Note: DMDHHSIZ not found; used DMDFMSIZ as proxy.\")\n",
    "        return out\n",
    "\n",
    "    # Case 3: scan raw DEMO files anywhere in your project for DMDHHSIZ and merge by SEQN\n",
    "    # Search roots: cfg.raw_dir (if exists) and the project root inferred from interim_dir\n",
    "    roots = []\n",
    "    if cfg.raw_dir and Path(cfg.raw_dir).exists(): \n",
    "        roots.append(Path(cfg.raw_dir))\n",
    "    # project root = parent of interim folder\n",
    "    try:\n",
    "        roots.append(Path(cfg.interim_dir).resolve().parent)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    hh_lookup = None\n",
    "    for r in roots:\n",
    "        hh_lookup = _stack_household_from_raw(r)\n",
    "        if hh_lookup is not None:\n",
    "            break\n",
    "\n",
    "    if hh_lookup is not None:\n",
    "        merged = demo[[\"SEQN\"]].merge(hh_lookup, on=\"SEQN\", how=\"left\")\n",
    "        merged.to_parquet(cfg.out_dir / cfg.cov_household, index=False)\n",
    "        print(f\"Household size recovered from raw files under {r}.\")\n",
    "        return merged\n",
    "\n",
    "    # Case 4: last resort — write NA column so pipeline can proceed\n",
    "    print(\"Warning: Could not locate DMDHHSIZ or DMDFMSIZ; writing NA household size.\")\n",
    "    out = pd.DataFrame({\"SEQN\": demo[\"SEQN\"], \"DMDHHSIZ\": pd.Series(np.nan, index=demo.index, dtype=\"float\")})\n",
    "    out.to_parquet(cfg.out_dir / cfg.cov_household, index=False)\n",
    "    return out\n",
    "# --- end patch ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bc04f21f-c273-46bf-bb25-5653b712548f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PATCH: robust survey design merger ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "SURVEY_KEEP = [\"SEQN\", \"SDDSRVYR\", \"SDMVPSU\", \"SDMVSTRA\", \"WTMEC2YR\"]\n",
    "\n",
    "def _read_any(p: Path) -> pd.DataFrame:\n",
    "    return pd.read_parquet(p) if str(p).endswith(\".parquet\") else pd.read_csv(p, low_memory=False)\n",
    "\n",
    "def _normalize_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    d = df.copy()\n",
    "    d.columns = [c.upper() for c in d.columns]\n",
    "    return d\n",
    "\n",
    "def _pick_weight(cols: list[str]) -> str | None:\n",
    "    up = [c.upper() for c in cols]\n",
    "    # Best to ok: WTMEC2YR\n",
    "    if \"WTMEC2YR\" in up: return cols[up.index(\"WTMEC2YR\")]\n",
    "    # Good fallbacks seen in NHANES pooled files\n",
    "    for cand in [\"WTINT2YR\",\"WTMEC4YR\",\"WTMECYR\",\"WTMEC_2YR\",\"WT_ME_C2YR\",\"WTSAF2YR\"]:\n",
    "        if cand in up: return cols[up.index(cand)]\n",
    "    # Sometimes duplicates like WTMEC2YR_X\n",
    "    for c in up:\n",
    "        if \"WTMEC\" in c and \"2YR\" in c:\n",
    "            return cols[up.index(c)]\n",
    "    return None\n",
    "\n",
    "def _has_all(d: pd.DataFrame, need=SURVEY_KEEP) -> bool:\n",
    "    return all(c in d.columns for c in need)\n",
    "\n",
    "def get_survey_core(cfg: Config = CONFIG) -> pd.DataFrame:\n",
    "    # Candidate DEMO tables (prefer 99–23, then 99–18, then interim)\n",
    "    candidates = []\n",
    "    for p in [cfg.demo_9923, cfg.demo_9918,\n",
    "              cfg.interim_dir / \"demo_9923.parquet\",\n",
    "              cfg.interim_dir / \"demo_9918.parquet\"]:\n",
    "        if p and Path(p).exists():\n",
    "            candidates.append(Path(p))\n",
    "\n",
    "    if not candidates:\n",
    "        raise FileNotFoundError(\"No DEMO table found for survey fields.\")\n",
    "\n",
    "    # Load base (prefer the first candidate)\n",
    "    base = _normalize_cols(_read_any(candidates[0]))\n",
    "    if \"SEQN\" not in base.columns:\n",
    "        raise ValueError(\"DEMO missing SEQN.\")\n",
    "\n",
    "    # Ensure SDDSRVYR exists if possible (some custom stacks omit it)\n",
    "    if \"SDDSRVYR\" not in base.columns:\n",
    "        # Try to take it from any backup candidate\n",
    "        for p in candidates[1:]:\n",
    "            d2 = _normalize_cols(_read_any(p))\n",
    "            if {\"SEQN\",\"SDDSRVYR\"}.issubset(d2.columns):\n",
    "                base = base.merge(d2[[\"SEQN\",\"SDDSRVYR\"]], on=\"SEQN\", how=\"left\")\n",
    "                break\n",
    "        if \"SDDSRVYR\" not in base.columns:\n",
    "            base[\"SDDSRVYR\"] = pd.Series(np.nan, index=base.index)\n",
    "\n",
    "    # Get/derive PSU & STRATA\n",
    "    for field, fuzzy in [(\"SDMVPSU\",\"PSU\"), (\"SDMVSTRA\",\"STRA\")]:\n",
    "        if field not in base.columns:\n",
    "            # Try from backups\n",
    "            pulled = False\n",
    "            for p in candidates[1:]:\n",
    "                d2 = _normalize_cols(_read_any(p))\n",
    "                if {\"SEQN\",field}.issubset(d2.columns):\n",
    "                    base = base.merge(d2[[\"SEQN\",field]], on=\"SEQN\", how=\"left\")\n",
    "                    pulled = True\n",
    "                    break\n",
    "            if not pulled:\n",
    "                # Fuzzy fallback from same table (e.g., SDMVPSU_X)\n",
    "                alt = next((c for c in base.columns if fuzzy in c and c != field), None)\n",
    "                if alt:\n",
    "                    base[field] = base[alt]\n",
    "                else:\n",
    "                    base[field] = pd.Series(np.nan, index=base.index)\n",
    "\n",
    "    # Get/derive WTMEC2YR (weight)\n",
    "    if \"WTMEC2YR\" not in base.columns:\n",
    "        # Try to copy from backups under any known alias\n",
    "        weight_name = _pick_weight(base.columns)\n",
    "        if weight_name and weight_name.upper() != \"WTMEC2YR\":\n",
    "            base[\"WTMEC2YR\"] = pd.to_numeric(base[weight_name], errors=\"coerce\")\n",
    "        else:\n",
    "            pulled = False\n",
    "            for p in candidates[1:]:\n",
    "                d2 = _normalize_cols(_read_any(p))\n",
    "                wn = _pick_weight(d2.columns)\n",
    "                if {\"SEQN\"}.issubset(d2.columns) and wn:\n",
    "                    base = base.merge(d2[[\"SEQN\", wn]].rename(columns={wn:\"WTMEC2YR\"}), on=\"SEQN\", how=\"left\")\n",
    "                    pulled = True\n",
    "                    break\n",
    "            if not pulled:\n",
    "                base[\"WTMEC2YR\"] = pd.Series(np.nan, index=base.index)\n",
    "\n",
    "    # Final subset\n",
    "    survey = base[[c for c in SURVEY_KEEP if c in base.columns]].copy()\n",
    "\n",
    "    # Assert presence (allow NaN values; we just need the columns)\n",
    "    missing_cols = [c for c in SURVEY_KEEP if c not in survey.columns]\n",
    "    if missing_cols:\n",
    "        # Create any still-missing as NaN so downstream can proceed\n",
    "        for c in missing_cols:\n",
    "            survey[c] = pd.Series(np.nan, index=survey.index)\n",
    "        print(f\"Note: created placeholder columns for: {missing_cols}\")\n",
    "\n",
    "    return survey\n",
    "# --- end patch ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f291dd31-032d-4f6e-88bb-e01c255ab7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Standardized Smoking → /Users/dengshuyue/Desktop/SDOH/analysis/interim/smk_9918.parquet\n",
      "SMK (raw) value counts:\n",
      "SMK\n",
      "1.0    29985\n",
      "2.0    13598\n",
      "3.0    11431\n",
      "NaN       67\n",
      "Name: count, dtype: int64\n",
      "\n",
      "SMK_STATUS (mapped) value counts:\n",
      "SMK_STATUS\n",
      "NEVER      29985\n",
      "FORMER     13598\n",
      "CURRENT    11431\n",
      "<NA>          67\n",
      "Name: count, dtype: Int64\n",
      "✓ Standardized PA → /Users/dengshuyue/Desktop/SDOH/analysis/interim/totalpa_9918_imputed.parquet\n",
      "\n",
      "Done. Non-missing now (% non-NA):\n",
      "{'SMK_STATUS': 0.4857791238774736, 'CIGS_PER_DAY': 0.10000971311004958, 'PACK_YEARS': 0.10717092424657171, 'LTPA_MET_HR_WK': 0.4863707405804908}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEQN</th>\n",
       "      <th>SMK_STATUS</th>\n",
       "      <th>CIGS_PER_DAY</th>\n",
       "      <th>PACK_YEARS</th>\n",
       "      <th>LTPA_MET_HR_WK</th>\n",
       "      <th>LTPA_IMPUTED_FLAG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>130378.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>130379.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>130380.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>130381.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>130382.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       SEQN SMK_STATUS  CIGS_PER_DAY  PACK_YEARS  LTPA_MET_HR_WK  \\\n",
       "0  130378.0       <NA>           NaN         NaN             NaN   \n",
       "1  130379.0       <NA>           NaN         NaN             NaN   \n",
       "2  130380.0       <NA>           NaN         NaN             NaN   \n",
       "3  130381.0       <NA>           NaN         NaN             NaN   \n",
       "4  130382.0       <NA>           NaN         NaN             NaN   \n",
       "\n",
       "   LTPA_IMPUTED_FLAG  \n",
       "0                NaN  \n",
       "1                NaN  \n",
       "2                NaN  \n",
       "3                NaN  \n",
       "4                NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "ROOT = Path(\"/Users/dengshuyue/Desktop/SDOH/analysis\")\n",
    "INTERIM = ROOT / \"interim\"\n",
    "OUT = ROOT / \"out\"\n",
    "INTERIM.mkdir(parents=True, exist_ok=True)\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ------------------\n",
    "# 1) Smoking mapping\n",
    "# ------------------\n",
    "smk_csv = ROOT / \"data\" / \"cov\" / \"smk_9918.csv\"\n",
    "smk = pd.read_csv(smk_csv, low_memory=False)\n",
    "smk.columns = [c.upper() for c in smk.columns]  # ['UNNAMED: 0','SEQN','SMK','SMK_YR','PACK','PACK_YR','SMK_AVG']\n",
    "\n",
    "out_smk = pd.DataFrame({\"SEQN\": smk[\"SEQN\"]})\n",
    "\n",
    "# CIGS_PER_DAY from SMK_AVG (cigs/day)\n",
    "out_smk[\"CIGS_PER_DAY\"] = pd.to_numeric(smk.get(\"SMK_AVG\"), errors=\"coerce\")\n",
    "\n",
    "# PACK_YEARS from PACK_YR\n",
    "out_smk[\"PACK_YEARS\"] = pd.to_numeric(smk.get(\"PACK_YR\"), errors=\"coerce\")\n",
    "\n",
    "# SMK_STATUS from SMK (numeric coding: 1=NEVER, 2=FORMER, 3=CURRENT)\n",
    "map_num = {1: \"NEVER\", 2: \"FORMER\", 3: \"CURRENT\"}\n",
    "s = pd.to_numeric(smk.get(\"SMK\"), errors=\"coerce\").map(map_num)\n",
    "out_smk[\"SMK_STATUS\"] = s.astype(\"string\")\n",
    "\n",
    "# FORMER_SMOKER (handle NA → False)\n",
    "out_smk[\"FORMER_SMOKER\"] = out_smk[\"SMK_STATUS\"].eq(\"FORMER\").fillna(False).astype(\"int8\")\n",
    "\n",
    "# Save standardized smoking parquet for the builder\n",
    "(out_smk[[\"SEQN\",\"SMK_STATUS\",\"CIGS_PER_DAY\",\"PACK_YEARS\",\"FORMER_SMOKER\"]]\n",
    " .to_parquet(INTERIM / \"smk_9918.parquet\", index=False))\n",
    "print(\"✓ Standardized Smoking →\", INTERIM / \"smk_9918.parquet\")\n",
    "\n",
    "# Quick sanity\n",
    "print(\"SMK (raw) value counts:\")\n",
    "print(smk[\"SMK\"].value_counts(dropna=False).head(10))\n",
    "print(\"\\nSMK_STATUS (mapped) value counts:\")\n",
    "print(out_smk[\"SMK_STATUS\"].value_counts(dropna=False).head(10))\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Physical activity mapping\n",
    "# -----------------------------\n",
    "# Your PA columns: ['SEQN','sddsrvyr','ltpa', ... 'imp']\n",
    "pa_csv = ROOT / \"data\" / \"cov\" / \"totalpa_9918_imputed.csv\"\n",
    "pa = pd.read_csv(pa_csv, low_memory=False)\n",
    "pa.columns = [c.upper() for c in pa.columns]  # includes 'LTPA' and 'IMP'\n",
    "\n",
    "out_pa = pd.DataFrame({\"SEQN\": pa[\"SEQN\"]})\n",
    "out_pa[\"LTPA_MET_HR_WK\"] = pd.to_numeric(pa.get(\"LTPA\"), errors=\"coerce\")\n",
    "out_pa[\"LTPA_IMPUTED_FLAG\"] = pd.to_numeric(pa.get(\"IMP\"), errors=\"coerce\").fillna(0).astype(\"int8\")\n",
    "\n",
    "out_pa.to_parquet(INTERIM / \"totalpa_9918_imputed.parquet\", index=False)\n",
    "print(\"✓ Standardized PA →\", INTERIM / \"totalpa_9918_imputed.parquet\")\n",
    "\n",
    "\n",
    "# -----------------------------------------\n",
    "# 3) Point CONFIG to these standardized files\n",
    "# -----------------------------------------\n",
    "CONFIG.smk_9918 = INTERIM / \"smk_9918.parquet\"\n",
    "CONFIG.pa_9918_imputed = INTERIM / \"totalpa_9918_imputed.parquet\"\n",
    "\n",
    "# --------------------------------\n",
    "# 4) Rebuild SMK, PA, and the core\n",
    "# --------------------------------\n",
    "smk_res = build_smk(CONFIG)\n",
    "pa_res  = build_pa(CONFIG)\n",
    "core    = build_core(CONFIG)\n",
    "\n",
    "print(\"\\nDone. Non-missing now (% non-NA):\")\n",
    "def nonmiss(s): return float(1 - s.isna().mean())\n",
    "print({\n",
    "    \"SMK_STATUS\":     nonmiss(core[\"SMK_STATUS\"]),\n",
    "    \"CIGS_PER_DAY\":   nonmiss(core[\"CIGS_PER_DAY\"]),\n",
    "    \"PACK_YEARS\":     nonmiss(core[\"PACK_YEARS\"]),\n",
    "    \"LTPA_MET_HR_WK\": nonmiss(core[\"LTPA_MET_HR_WK\"]),\n",
    "})\n",
    "\n",
    "# (Optional) Peek a few rows\n",
    "try:\n",
    "    display(core[[\"SEQN\",\"SMK_STATUS\",\"CIGS_PER_DAY\",\"PACK_YEARS\",\"LTPA_MET_HR_WK\",\"LTPA_IMPUTED_FLAG\"]].head())\n",
    "except NameError:\n",
    "    print(core[[\"SEQN\",\"SMK_STATUS\",\"CIGS_PER_DAY\",\"PACK_YEARS\",\"LTPA_MET_HR_WK\",\"LTPA_IMPUTED_FLAG\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cb8a614b-c873-4da5-a214-55ccde88c60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Household size recovered from raw files under /Users/dengshuyue/Desktop/SDOH/analysis.\n",
      "n_rows                     113249.000000\n",
      "n_unique_seqn              113249.000000\n",
      "missing_bmi_pct                 0.224726\n",
      "missing_alcohol_cat_pct         0.000000\n",
      "missing_smk_status_pct          0.514221\n",
      "has_weights                     1.000000\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'raw_dir': PosixPath('/Users/you/nhanes/raw'),\n",
       " 'interim_dir': PosixPath('interim'),\n",
       " 'out_dir': PosixPath('out'),\n",
       " 'smk_9918': PosixPath('/Users/dengshuyue/Desktop/SDOH/analysis/interim/smk_9918.parquet'),\n",
       " 'pa_9918_imputed': PosixPath('/Users/dengshuyue/Desktop/SDOH/analysis/interim/totalpa_9918_imputed.parquet'),\n",
       " 'clinical_9918': None,\n",
       " 'smk_1923': None,\n",
       " 'pa_1923': None,\n",
       " 'clinical_1923': None,\n",
       " 'demo_9923': PosixPath('/Users/dengshuyue/Desktop/SDOH/analysis/interim/survey_design_9923.parquet'),\n",
       " 'demo_9918': None,\n",
       " 'bmx_9923': None,\n",
       " 'cov_smk': 'cov_smk_1999_2023.parquet',\n",
       " 'cov_alc': 'cov_alc_1999_2023.parquet',\n",
       " 'cov_pa': 'cov_pa_1999_2023.parquet',\n",
       " 'cov_bmx': 'cov_bmx_1999_2023.parquet',\n",
       " 'cov_clinical': 'cov_clinical_1999_2023.parquet',\n",
       " 'cov_household': 'cov_household_1999_2023.parquet',\n",
       " 'cov_core': 'cov_core_1999_2023.parquet'}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = run_all(CONFIG)\n",
    "print(quick_checks(CONFIG))\n",
    "\n",
    "vars(CONFIG) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "64f1e4f2-6199-4eaf-bf8e-4f167cf3d702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True,\n",
       " PosixPath('/Users/dengshuyue/Desktop/SDOH/analysis/out/cov_core_1999_2023.parquet'))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "core_path = Path(\"/Users/dengshuyue/Desktop/SDOH/analysis/out/cov_core_1999_2023.parquet\")\n",
    "core_path.exists(), core_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "dd4c6d1a-1978-4f06-8ff5-de801b02b127",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged shape: (56253, 46) | Matched SEQN: 56253 / 56253\n",
      "\n",
      "Missingness (% NA) in merged:\n",
      "  SMK_STATUS         7.1%\n",
      "  CIGS_PER_DAY      80.8%\n",
      "  PACK_YEARS        79.5%\n",
      "  ALCOHOL_CAT        0.0%\n",
      "  LTPA_MET_HR_WK     7.1%\n",
      "  BMI                2.1%\n",
      "  DIABETES          85.1%\n",
      "  HTN                0.0%\n",
      "  HIGH_CHOL          0.0%\n",
      "  CVD                0.0%\n",
      "  CANCER             0.0%\n",
      "  SBP                0.0%\n",
      "  DBP                0.0%\n",
      "  TCHOL              0.0%\n",
      "  HDL                0.0%\n",
      "  LDL                0.0%\n",
      "  TG                 0.0%\n",
      "\n",
      "Preview:\n",
      "   SEQN SMK_STATUS  CIGS_PER_DAY  PACK_YEARS ALCOHOL_CAT  LTPA_MET_HR_WK  \\\n",
      "0     2      NEVER           NaN         NaN        NONE        0.000000   \n",
      "1     5     FORMER           NaN         NaN        NONE       41.066667   \n",
      "2     6       <NA>           NaN         NaN        NONE             NaN   \n",
      "3     7     FORMER           NaN      8030.0        NONE        3.033333   \n",
      "4    10    CURRENT           1.0         NaN        NONE        0.000000   \n",
      "5    12      NEVER           NaN         NaN        NONE        5.600000   \n",
      "6    13     FORMER           NaN      1095.0        NONE        0.000000   \n",
      "7    14    CURRENT           1.0         NaN        NONE       30.800000   \n",
      "8    15    CURRENT           2.0         NaN        NONE       38.577778   \n",
      "9    16      NEVER           NaN         NaN        NONE       48.533333   \n",
      "\n",
      "         BMI         SBP         DBP  \n",
      "0  24.904215  100.666667   56.666667  \n",
      "1  29.096386  122.000000   82.666667  \n",
      "2  22.557537  114.666667   68.000000  \n",
      "3  29.393577  125.333333   80.000000  \n",
      "4  30.936955  145.333333   96.000000  \n",
      "5  30.617284  176.666667  102.000000  \n",
      "6  25.573710  133.333333   70.000000  \n",
      "7  27.332850  138.000000   59.333333  \n",
      "8  26.675375  108.000000   68.666667  \n",
      "9  19.958026  147.333333   60.666667  \n"
     ]
    }
   ],
   "source": [
    "# === Merge cov_core into your existing master (LEFT join) ===\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def _load_any(p: Path) -> pd.DataFrame:\n",
    "    return pd.read_parquet(p) if p.suffix.lower()==\".parquet\" else pd.read_csv(p, low_memory=False)\n",
    "\n",
    "def merge_core_into_master(\n",
    "    master_path: Path,\n",
    "    core_path: Path,\n",
    "    out_path: Path,\n",
    "    keep_master_dups: bool = True,\n",
    "    prefer_core_for: list[str] | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    LEFT-join cov_core (1999–2023) onto your master (1999–2018).\n",
    "    - keep_master_dups=True: if a column exists in both, keep the master's version.\n",
    "      (Core duplicates get suffix \"_CORE\" temporarily and are then dropped.)\n",
    "    - prefer_core_for=['BMI', 'SBP', ...]: for listed columns, take values from core\n",
    "      (if present) and overwrite master.\n",
    "    \"\"\"\n",
    "    if not master_path.exists():\n",
    "        raise FileNotFoundError(f\"Master not found: {master_path}\")\n",
    "    if not core_path.exists():\n",
    "        raise FileNotFoundError(f\"Core not found:   {core_path}\\n\"\n",
    "                                \"Run the core builder first to create cov_core_1999_2023.parquet.\")\n",
    "\n",
    "    master = _load_any(master_path)\n",
    "    core   = _load_any(core_path)\n",
    "\n",
    "    # Normalize keys/columns\n",
    "    master.columns = master.columns.str.upper()\n",
    "    core.columns   = core.columns.str.upper()\n",
    "\n",
    "    master[\"SEQN\"] = pd.to_numeric(master[\"SEQN\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    core[\"SEQN\"]   = pd.to_numeric(core[\"SEQN\"],   errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    # Merge\n",
    "    merged = master.merge(core, on=\"SEQN\", how=\"left\", suffixes=(\"\", \"_CORE\"), validate=\"one_to_one\")\n",
    "\n",
    "    # Optionally overwrite some columns with core values (only where core non-null)\n",
    "    if prefer_core_for:\n",
    "        for col in prefer_core_for:\n",
    "            c_core = f\"{col}_CORE\"\n",
    "            if c_core in merged.columns:\n",
    "                if col not in merged.columns:\n",
    "                    # If master didn't have the column, just rename core's\n",
    "                    merged.rename(columns={c_core: col}, inplace=True)\n",
    "                else:\n",
    "                    # Overwrite master col with non-missing core values\n",
    "                    merged[col] = merged[col].where(merged[c_core].isna(), merged[c_core])\n",
    "                    merged.drop(columns=[c_core], inplace=True)\n",
    "\n",
    "    # If keeping master versions, drop remaining *_CORE duplicates\n",
    "    if keep_master_dups:\n",
    "        dup_cols = [c for c in merged.columns if c.endswith(\"_CORE\")]\n",
    "        merged.drop(columns=dup_cols, inplace=True)\n",
    "\n",
    "    # Save\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if out_path.suffix.lower()==\".parquet\":\n",
    "        merged.to_parquet(out_path, index=False)\n",
    "    else:\n",
    "        merged.to_csv(out_path, index=False)\n",
    "\n",
    "    # Log\n",
    "    n_rows = len(merged)\n",
    "    n_matched = int(merged[\"SEQN\"].isin(core[\"SEQN\"]).sum())\n",
    "    print(f\"Merged shape: {merged.shape} | Matched SEQN: {n_matched} / {n_rows}\")\n",
    "    return merged\n",
    "\n",
    "# ---- Configure paths for your project ----\n",
    "ROOT = Path(\"/Users/dengshuyue/Desktop/SDOH/analysis\")\n",
    "OUT  = ROOT / \"out\"\n",
    "\n",
    "master_path = OUT / \"nhanes_mort_demo_sdoh_1999_2018.parquet\"     # your existing master\n",
    "core_path   = OUT / \"cov_core_1999_2023.parquet\"                   # written by the core builder\n",
    "out_path    = OUT / \"nhanes_mort_demo_sdoh_core_1999_2023.parquet\" # new merged output\n",
    "\n",
    "# If you want certain variables to come from core, list them here; else leave [].\n",
    "prefer_core = []   # e.g., ['BMI','SBP','DBP']\n",
    "\n",
    "merged = merge_core_into_master(\n",
    "    master_path=master_path,\n",
    "    core_path=core_path,\n",
    "    out_path=out_path,\n",
    "    keep_master_dups=True,\n",
    "    prefer_core_for=prefer_core\n",
    ")\n",
    "\n",
    "# Quick NA audit on key covariates (only those present will be shown)\n",
    "audit_cols = [\"SMK_STATUS\",\"CIGS_PER_DAY\",\"PACK_YEARS\",\n",
    "              \"ALCOHOL_CAT\",\"LTPA_MET_HR_WK\",\"BMI\",\n",
    "              \"DIABETES\",\"HTN\",\"HIGH_CHOL\",\"CVD\",\"CANCER\",\n",
    "              \"SBP\",\"DBP\",\"TCHOL\",\"HDL\",\"LDL\",\"TG\"]\n",
    "present = [c for c in audit_cols if c in merged.columns]\n",
    "na_pct = {c: round(float(merged[c].isna().mean())*100, 1) for c in present}\n",
    "print(\"\\nMissingness (% NA) in merged:\")\n",
    "for k in present:\n",
    "    print(f\"  {k:16s} {na_pct[k]:5.1f}%\")\n",
    "\n",
    "# Peek a few columns\n",
    "show_cols = [\"SEQN\",\"SMK_STATUS\",\"CIGS_PER_DAY\",\"PACK_YEARS\",\n",
    "             \"ALCOHOL_CAT\",\"LTPA_MET_HR_WK\",\"BMI\",\"SBP\",\"DBP\"]\n",
    "print(\"\\nPreview:\")\n",
    "print(merged[[c for c in show_cols if c in merged.columns]].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b257e2-0845-4479-87ec-e37d68d87cfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9a2972-54b4-437e-a002-ffd2fa56e976",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552ee6ce-86a5-47fa-8f29-d9d32a063263",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68bbed88-cb99-41b8-b4c3-a21843220211",
   "metadata": {},
   "source": [
    "<h2>peek at the merged table HERE!!!!! understand the core table</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "66592bfa-a65b-437a-a575-ecdf2e71b787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (113249, 29)\n",
      "unique SEQN: 113249\n",
      "SEQN range: 1 → 142310\n",
      "first 40 columns: ['SEQN', 'SDDSRVYR', 'SDMVPSU', 'SDMVSTRA', 'WTMEC2YR', 'SMK_STATUS', 'CIGS_PER_DAY', 'PACK_YEARS', 'FORMER_SMOKER', 'DRINKS_PER_DAY', 'ALCOHOL_CAT', 'LTPA_MET_HR_WK', 'LTPA_IMPUTED_FLAG', 'BMXWT', 'BMXHT', 'BMI', 'BMI_CLAS', 'DIABETES', 'HTN', 'HIGH_CHOL', 'CVD', 'CANCER', 'SBP', 'DBP', 'TCHOL', 'HDL', 'LDL', 'TG', 'DMDHHSIZ']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEQN</th>\n",
       "      <th>SDDSRVYR</th>\n",
       "      <th>SDMVPSU</th>\n",
       "      <th>SDMVSTRA</th>\n",
       "      <th>WTMEC2YR</th>\n",
       "      <th>SMK_STATUS</th>\n",
       "      <th>CIGS_PER_DAY</th>\n",
       "      <th>PACK_YEARS</th>\n",
       "      <th>FORMER_SMOKER</th>\n",
       "      <th>DRINKS_PER_DAY</th>\n",
       "      <th>ALCOHOL_CAT</th>\n",
       "      <th>LTPA_MET_HR_WK</th>\n",
       "      <th>LTPA_IMPUTED_FLAG</th>\n",
       "      <th>BMXWT</th>\n",
       "      <th>BMXHT</th>\n",
       "      <th>BMI</th>\n",
       "      <th>BMI_CLAS</th>\n",
       "      <th>DIABETES</th>\n",
       "      <th>HTN</th>\n",
       "      <th>HIGH_CHOL</th>\n",
       "      <th>CVD</th>\n",
       "      <th>CANCER</th>\n",
       "      <th>SBP</th>\n",
       "      <th>DBP</th>\n",
       "      <th>TCHOL</th>\n",
       "      <th>HDL</th>\n",
       "      <th>LDL</th>\n",
       "      <th>TG</th>\n",
       "      <th>DMDHHSIZ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>52723</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10982.898896</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.5</td>\n",
       "      <td>91.6</td>\n",
       "      <td>14.897695</td>\n",
       "      <td>UNDER</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>91.333333</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>131.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52724</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28325.384898</td>\n",
       "      <td>NEVER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NONE</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>75.4</td>\n",
       "      <td>174.0</td>\n",
       "      <td>24.904215</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>100.666667</td>\n",
       "      <td>56.666667</td>\n",
       "      <td>215.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52725</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>46192.256945</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.9</td>\n",
       "      <td>136.6</td>\n",
       "      <td>17.631713</td>\n",
       "      <td>UNDER</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>108.666667</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>129.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52726</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10251.260020</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>95.333333</td>\n",
       "      <td>61.333333</td>\n",
       "      <td>211.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52727</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>99445.065735</td>\n",
       "      <td>FORMER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NONE</td>\n",
       "      <td>41.066667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>92.5</td>\n",
       "      <td>178.3</td>\n",
       "      <td>29.096386</td>\n",
       "      <td>OVER</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>82.666667</td>\n",
       "      <td>279.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>347.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52728</th>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>39656.600444</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>59.2</td>\n",
       "      <td>162.0</td>\n",
       "      <td>22.557537</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>114.666667</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>153.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52729</th>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>25525.423409</td>\n",
       "      <td>FORMER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8030.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NONE</td>\n",
       "      <td>3.033333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>162.9</td>\n",
       "      <td>29.393577</td>\n",
       "      <td>OVER</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>125.333333</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>245.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52730</th>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>31510.587866</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.7</td>\n",
       "      <td>162.0</td>\n",
       "      <td>15.508307</td>\n",
       "      <td>UNDER</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100.666667</td>\n",
       "      <td>49.333333</td>\n",
       "      <td>162.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52731</th>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7575.870247</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>45.5</td>\n",
       "      <td>156.9</td>\n",
       "      <td>18.482704</td>\n",
       "      <td>UNDER</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>109.333333</td>\n",
       "      <td>53.333333</td>\n",
       "      <td>148.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52732</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>22445.808572</td>\n",
       "      <td>CURRENT</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NONE</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>111.8</td>\n",
       "      <td>190.1</td>\n",
       "      <td>30.936955</td>\n",
       "      <td>OBESE</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>145.333333</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>140.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       SEQN  SDDSRVYR  SDMVPSU  SDMVSTRA      WTMEC2YR SMK_STATUS  CIGS_PER_DAY  PACK_YEARS  FORMER_SMOKER  DRINKS_PER_DAY ALCOHOL_CAT  LTPA_MET_HR_WK  LTPA_IMPUTED_FLAG  BMXWT  BMXHT        BMI  \\\n",
       "52723   1.0       1.0      1.0       5.0  10982.898896       <NA>           NaN         NaN            NaN             NaN        NONE             NaN                NaN   12.5   91.6  14.897695   \n",
       "52724   2.0       1.0      3.0       1.0  28325.384898      NEVER           NaN         NaN            0.0             NaN        NONE        0.000000                1.0   75.4  174.0  24.904215   \n",
       "52725   3.0       1.0      2.0       7.0  46192.256945       <NA>           NaN         NaN            NaN             NaN        NONE             NaN                NaN   32.9  136.6  17.631713   \n",
       "52726   4.0       1.0      1.0       2.0  10251.260020       <NA>           NaN         NaN            NaN             NaN        NONE             NaN                NaN   13.3    NaN        NaN   \n",
       "52727   5.0       1.0      2.0       8.0  99445.065735     FORMER           NaN         NaN            1.0             NaN        NONE       41.066667                1.0   92.5  178.3  29.096386   \n",
       "52728   6.0       1.0      2.0       2.0  39656.600444       <NA>           NaN         NaN            NaN             NaN        NONE             NaN                NaN   59.2  162.0  22.557537   \n",
       "52729   7.0       1.0      2.0       4.0  25525.423409     FORMER           NaN      8030.0            1.0             NaN        NONE        3.033333                1.0   78.0  162.9  29.393577   \n",
       "52730   8.0       1.0      1.0       6.0  31510.587866       <NA>           NaN         NaN            NaN             NaN        NONE             NaN                NaN   40.7  162.0  15.508307   \n",
       "52731   9.0       1.0      2.0       9.0   7575.870247       <NA>           NaN         NaN            NaN             NaN        NONE             NaN                NaN   45.5  156.9  18.482704   \n",
       "52732  10.0       1.0      1.0       7.0  22445.808572    CURRENT           1.0         NaN            0.0             NaN        NONE        0.000000                1.0  111.8  190.1  30.936955   \n",
       "\n",
       "      BMI_CLAS  DIABETES  HTN  HIGH_CHOL  CVD  CANCER         SBP        DBP  TCHOL    HDL    LDL     TG  DMDHHSIZ  \n",
       "52723    UNDER         0    0          0    0       0   91.333333  56.000000  131.0   59.0   54.0   99.0       3.0  \n",
       "52724   NORMAL         0    0          0    0       1  100.666667  56.666667  215.0   54.0  136.0  128.0       1.0  \n",
       "52725    UNDER         0    0          0    0       0  108.666667  62.000000  129.0   30.0   58.0  202.0       4.0  \n",
       "52726     <NA>         0    0          1    0       0   95.333333  61.333333  211.0   43.0  161.0   37.0       7.0  \n",
       "52727     OVER         0    1          1    0       0  122.000000  82.666667  279.0   42.0  168.0  347.0       3.0  \n",
       "52728   NORMAL         0    0          0    0       0  114.666667  68.000000  153.0   61.0   59.0  181.0       2.0  \n",
       "52729     OVER         0    0          1    0       0  125.333333  80.000000  245.0  105.0  127.0   62.0       1.0  \n",
       "52730    UNDER         0    0          0    0       0  100.666667  49.333333  162.0   67.0   88.0   33.0       7.0  \n",
       "52731    UNDER         0    0          0    0       0  109.333333  53.333333  148.0   58.0   79.0   56.0       4.0  \n",
       "52732    OBESE         0    1          0    0       0  145.333333  96.000000  140.0   51.0   80.0   45.0       1.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEQN</th>\n",
       "      <th>SDDSRVYR</th>\n",
       "      <th>SDMVPSU</th>\n",
       "      <th>SDMVSTRA</th>\n",
       "      <th>WTMEC2YR</th>\n",
       "      <th>SMK_STATUS</th>\n",
       "      <th>CIGS_PER_DAY</th>\n",
       "      <th>PACK_YEARS</th>\n",
       "      <th>FORMER_SMOKER</th>\n",
       "      <th>DRINKS_PER_DAY</th>\n",
       "      <th>ALCOHOL_CAT</th>\n",
       "      <th>LTPA_MET_HR_WK</th>\n",
       "      <th>LTPA_IMPUTED_FLAG</th>\n",
       "      <th>BMXWT</th>\n",
       "      <th>BMXHT</th>\n",
       "      <th>BMI</th>\n",
       "      <th>BMI_CLAS</th>\n",
       "      <th>DIABETES</th>\n",
       "      <th>HTN</th>\n",
       "      <th>HIGH_CHOL</th>\n",
       "      <th>CVD</th>\n",
       "      <th>CANCER</th>\n",
       "      <th>SBP</th>\n",
       "      <th>DBP</th>\n",
       "      <th>TCHOL</th>\n",
       "      <th>HDL</th>\n",
       "      <th>LDL</th>\n",
       "      <th>TG</th>\n",
       "      <th>DMDHHSIZ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11923</th>\n",
       "      <td>142301.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>17561.693314</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11924</th>\n",
       "      <td>142302.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>21064.211105</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11925</th>\n",
       "      <td>142303.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>47087.576098</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11926</th>\n",
       "      <td>142304.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>22000.421342</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11927</th>\n",
       "      <td>142305.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>43483.407534</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11928</th>\n",
       "      <td>142306.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>13459.129019</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11929</th>\n",
       "      <td>142307.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>64962.328962</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11930</th>\n",
       "      <td>142308.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>44367.534132</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11931</th>\n",
       "      <td>142309.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>46249.361849</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11932</th>\n",
       "      <td>142310.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>49647.225467</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           SEQN  SDDSRVYR  SDMVPSU  SDMVSTRA      WTMEC2YR SMK_STATUS  CIGS_PER_DAY  PACK_YEARS  FORMER_SMOKER  DRINKS_PER_DAY ALCOHOL_CAT  LTPA_MET_HR_WK  LTPA_IMPUTED_FLAG  BMXWT  BMXHT  BMI  \\\n",
       "11923  142301.0      12.0      1.0     179.0  17561.693314       <NA>           NaN         NaN            NaN             NaN        NONE             NaN                NaN    NaN    NaN  NaN   \n",
       "11924  142302.0      12.0      1.0     179.0  21064.211105       <NA>           NaN         NaN            NaN             NaN        NONE             NaN                NaN    NaN    NaN  NaN   \n",
       "11925  142303.0      12.0      1.0     181.0  47087.576098       <NA>           NaN         NaN            NaN             NaN        NONE             NaN                NaN    NaN    NaN  NaN   \n",
       "11926  142304.0      12.0      1.0     176.0  22000.421342       <NA>           NaN         NaN            NaN             NaN        NONE             NaN                NaN    NaN    NaN  NaN   \n",
       "11927  142305.0      12.0      2.0     180.0  43483.407534       <NA>           NaN         NaN            NaN             NaN        NONE             NaN                NaN    NaN    NaN  NaN   \n",
       "11928  142306.0      12.0      1.0     176.0  13459.129019       <NA>           NaN         NaN            NaN             NaN        NONE             NaN                NaN    NaN    NaN  NaN   \n",
       "11929  142307.0      12.0      1.0     181.0  64962.328962       <NA>           NaN         NaN            NaN             NaN        NONE             NaN                NaN    NaN    NaN  NaN   \n",
       "11930  142308.0      12.0      2.0     183.0  44367.534132       <NA>           NaN         NaN            NaN             NaN        NONE             NaN                NaN    NaN    NaN  NaN   \n",
       "11931  142309.0      12.0      1.0     176.0  46249.361849       <NA>           NaN         NaN            NaN             NaN        NONE             NaN                NaN    NaN    NaN  NaN   \n",
       "11932  142310.0      12.0      2.0     187.0  49647.225467       <NA>           NaN         NaN            NaN             NaN        NONE             NaN                NaN    NaN    NaN  NaN   \n",
       "\n",
       "      BMI_CLAS  DIABETES   HTN  HIGH_CHOL   CVD  CANCER  SBP  DBP  TCHOL  HDL  LDL  TG  DMDHHSIZ  \n",
       "11923     <NA>      <NA>  <NA>       <NA>  <NA>    <NA>  NaN  NaN    NaN  NaN  NaN NaN       1.0  \n",
       "11924     <NA>      <NA>  <NA>       <NA>  <NA>    <NA>  NaN  NaN    NaN  NaN  NaN NaN       2.0  \n",
       "11925     <NA>      <NA>  <NA>       <NA>  <NA>    <NA>  NaN  NaN    NaN  NaN  NaN NaN       2.0  \n",
       "11926     <NA>      <NA>  <NA>       <NA>  <NA>    <NA>  NaN  NaN    NaN  NaN  NaN NaN       5.0  \n",
       "11927     <NA>      <NA>  <NA>       <NA>  <NA>    <NA>  NaN  NaN    NaN  NaN  NaN NaN       4.0  \n",
       "11928     <NA>      <NA>  <NA>       <NA>  <NA>    <NA>  NaN  NaN    NaN  NaN  NaN NaN       2.0  \n",
       "11929     <NA>      <NA>  <NA>       <NA>  <NA>    <NA>  NaN  NaN    NaN  NaN  NaN NaN       5.0  \n",
       "11930     <NA>      <NA>  <NA>       <NA>  <NA>    <NA>  NaN  NaN    NaN  NaN  NaN NaN       3.0  \n",
       "11931     <NA>      <NA>  <NA>       <NA>  <NA>    <NA>  NaN  NaN    NaN  NaN  NaN NaN       5.0  \n",
       "11932     <NA>      <NA>  <NA>       <NA>  <NA>    <NA>  NaN  NaN    NaN  NaN  NaN NaN       2.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "core_path = \"/Users/dengshuyue/Desktop/SDOH/analysis/out/cov_core_1999_2023.parquet\"\n",
    "core = pd.read_parquet(core_path)\n",
    "\n",
    "# Quick stats\n",
    "print(\"shape:\", core.shape)\n",
    "print(\"unique SEQN:\", core[\"SEQN\"].nunique())\n",
    "print(\"SEQN range:\", int(core[\"SEQN\"].min()), \"→\", int(core[\"SEQN\"].max()))\n",
    "print(\"first 40 columns:\", core.columns.tolist()[:40])\n",
    "\n",
    "# Optional: widen display for many columns\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 200)\n",
    "\n",
    "# Sort once, then peek head/tail (ALL columns)\n",
    "sorted_core = core.sort_values(\"SEQN\", kind=\"mergesort\")\n",
    "\n",
    "# Try to use rich display if available (Jupyter); else print to stdout\n",
    "try:\n",
    "    from IPython.display import display  # only exists in notebooks\n",
    "    display(sorted_core.head(10))\n",
    "    display(sorted_core.tail(10))\n",
    "except Exception:\n",
    "    print(\"\\nHead (sorted by SEQN):\")\n",
    "    print(sorted_core.head(10).to_string(index=False))\n",
    "    print(\"\\nTail (sorted by SEQN):\")\n",
    "    print(sorted_core.tail(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330212c1-ed21-445d-a0ba-06b6dd196cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6304f2-b2ec-4385-910a-bfe6b9aad186",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecab367b-b85b-485f-9c5d-b698465ac998",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380ed83f-f9ea-41ac-906d-a03d02f2a3cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e25955-1bc0-46c8-8a78-e05fd45bd184",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9dff4bca-d9e9-4805-a35f-62432439b51a",
   "metadata": {},
   "source": [
    "<h2>check why smk is NA</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1cc86203-7c87-4605-8182-276e3e4122f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists: True /Users/dengshuyue/Desktop/SDOH/analysis/interim/smk_9918.parquet\n",
      "\n",
      "SEQN info:\n",
      "{'dtype': 'float64', 'rows': 55081, 'n_unique': 55081, 'min': 2.0, 'max': 102956.0, 'looks_float_with_decimals': False}\n",
      "\n",
      "SMK_STATUS NA %: 0.0012163904068553586\n",
      "\n",
      "SMK_STATUS value counts (top):\n",
      "SMK_STATUS\n",
      "NEVER      29985\n",
      "FORMER     13598\n",
      "CURRENT    11431\n",
      "<NA>          67\n",
      "Name: count, dtype: Int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEQN</th>\n",
       "      <th>SMK_STATUS</th>\n",
       "      <th>CIGS_PER_DAY</th>\n",
       "      <th>PACK_YEARS</th>\n",
       "      <th>FORMER_SMOKER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>NEVER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>FORMER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.0</td>\n",
       "      <td>FORMER</td>\n",
       "      <td>7300.0</td>\n",
       "      <td>8030.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0</td>\n",
       "      <td>CURRENT</td>\n",
       "      <td>1.0</td>\n",
       "      <td>388.85</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.0</td>\n",
       "      <td>NEVER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13.0</td>\n",
       "      <td>FORMER</td>\n",
       "      <td>1095.0</td>\n",
       "      <td>1095.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14.0</td>\n",
       "      <td>CURRENT</td>\n",
       "      <td>1.0</td>\n",
       "      <td>388.85</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15.0</td>\n",
       "      <td>CURRENT</td>\n",
       "      <td>2.0</td>\n",
       "      <td>777.70</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>16.0</td>\n",
       "      <td>NEVER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20.0</td>\n",
       "      <td>FORMER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SEQN SMK_STATUS  CIGS_PER_DAY  PACK_YEARS  FORMER_SMOKER\n",
       "0   2.0      NEVER           NaN         NaN              0\n",
       "1   5.0     FORMER           NaN         NaN              1\n",
       "2   7.0     FORMER        7300.0     8030.00              1\n",
       "3  10.0    CURRENT           1.0      388.85              0\n",
       "4  12.0      NEVER           NaN         NaN              0\n",
       "5  13.0     FORMER        1095.0     1095.00              1\n",
       "6  14.0    CURRENT           1.0      388.85              0\n",
       "7  15.0    CURRENT           2.0      777.70              0\n",
       "8  16.0      NEVER           NaN         NaN              0\n",
       "9  20.0     FORMER           NaN         NaN              1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "ROOT = Path(\"/Users/dengshuyue/Desktop/SDOH/analysis\")\n",
    "p = ROOT / \"interim\" / \"smk_9918.parquet\"\n",
    "\n",
    "print(\"File exists:\", p.exists(), p)\n",
    "smk = pd.read_parquet(p)\n",
    "\n",
    "# SEQN range & basics\n",
    "seqn = smk[\"SEQN\"]\n",
    "print(\"\\nSEQN info:\")\n",
    "print({\n",
    "    \"dtype\": str(seqn.dtype),\n",
    "    \"rows\": len(seqn),\n",
    "    \"n_unique\": int(seqn.nunique()),\n",
    "    \"min\": float(seqn.min()) if pd.api.types.is_numeric_dtype(seqn) else None,\n",
    "    \"max\": float(seqn.max()) if pd.api.types.is_numeric_dtype(seqn) else None,\n",
    "    \"looks_float_with_decimals\": bool(pd.api.types.is_float_dtype(seqn) and ((seqn % 1) != 0).any())\n",
    "})\n",
    "\n",
    "# SMK_STATUS NA percentage\n",
    "na_pct = float(smk[\"SMK_STATUS\"].isna().mean())\n",
    "print(\"\\nSMK_STATUS NA %:\", na_pct)\n",
    "\n",
    "print(\"\\nSMK_STATUS value counts (top):\")\n",
    "print(smk[\"SMK_STATUS\"].value_counts(dropna=False).head(10))\n",
    "\n",
    "# Peek a few rows sorted by SEQN\n",
    "cols = [c for c in [\"SEQN\",\"SMK_STATUS\",\"CIGS_PER_DAY\",\"PACK_YEARS\",\"FORMER_SMOKER\"] if c in smk.columns]\n",
    "\n",
    "try:\n",
    "    # Jupyter-friendly\n",
    "    display(smk.sort_values(\"SEQN\", kind=\"mergesort\")[cols].head(10))\n",
    "    # display(smk.sort_values(\"SEQN\", kind=\"mergesort\")[cols].tail(10))\n",
    "except NameError:\n",
    "    # Fallback if not in a notebook\n",
    "    print(\"\\nHead (sorted by SEQN):\")\n",
    "    print(smk.sort_values(\"SEQN\", kind=\"mergesort\")[cols].head(10).to_string(index=False))\n",
    "    # print(\"\\nTail (sorted by SEQN):\")\n",
    "    # print(smk.sort_values(\"SEQN\", kind=\"mergesort\")[cols].tail(10).to_string(index=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fdc9cb-0868-4e2a-87b5-2c89e01e18c1",
   "metadata": {},
   "source": [
    "<h2>check demo </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4973e0ce-a85d-4f4d-9b4d-43321030818b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65cf648-8251-4ca9-b0c7-f5175fbf8e08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94990f0-63bc-4036-aec2-03cba09f003d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a8ba9c-019c-42ec-be3f-a714fb098241",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec03efea-9ec0-4136-9e6c-3cecad083750",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e473696-369c-4eed-9180-1a938f4418a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32192be9-642f-40f2-8b30-fceb2a699b5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145d949a-9c02-47c4-92f6-a9da6678e725",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb33259-c4db-4587-ad91-522f86e639a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee321498-dc22-49d8-9a37-45d98bbe0f20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb73e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Read dietary score data\n",
    "scores = pd.read_excel(os.path.join(folder_path, \"i.scores.xlsx\"), engine=\"openpyxl\")\n",
    "\n",
    "# Rename columns to match desired output\n",
    "scores = scores.rename(columns={\n",
    "    \"seqn\": \"SEQN\",\n",
    "    \"i.FCS\": \"i_FCS\",\n",
    "    \"i.optup\": \"i_optup\",  # keep lowercase here first\n",
    "    \"i.HSR\": \"i_HSR\",\n",
    "    \"i.nutri\": \"i_nutri\"\n",
    "})\n",
    "\n",
    "# Then copy and rename for output\n",
    "scores2 = scores[[\"SEQN\", \"i_FCS\", \"i_optup\", \"i_HSR\", \"i_nutri\"]].copy()\n",
    "scores2 = scores2.rename(columns={\"i_optup\": \"i_Optup\"})\n",
    "scores2 = scores2.sort_values(\"SEQN\")\n",
    "\n",
    "\n",
    "# Step 2: Read covariates from Lu paper\n",
    "covar = pd.read_sas(os.path.join(folder_path, \"covar.sas7bdat\"), format=\"sas7bdat\")\n",
    "covar = covar.rename(columns=str.upper)  # make all column names uppercase to match SAS style\n",
    "# filter available variables only\n",
    "covar_vars = [\"SEQN\", \"RIDAGEYR\", \"SEX\", \"RACE\", \"EDU\", \"INDFMPIR\", \"SMK_AVG\", \"SMK_PAST\",\n",
    "              \"SMK\", \"ALCG2\", \"HEI2015_TOTAL_SCORE\", \"DIABE\"]\n",
    "covar = covar[[col for col in covar_vars if col in covar.columns]].copy()\n",
    "covar = covar.sort_values(\"SEQN\")\n",
    "\n",
    "# Step 3: Read covariates from Meghan paper\n",
    "covariates1_raw = pd.read_csv(os.path.join(folder_path, \"covariates.csv\"))\n",
    "covariates1 = covariates1_raw.rename(columns={\"seqn\": \"SEQN\"})\n",
    "covariates_vars = [\"SEQN\", \"sdmvpsu\", \"sdmvstra\", \"met_hr\", \"perE_alco\", \"dm_self\",\n",
    "                   \"tchol\", \"hdl\", \"ldl\", \"tg\", \"bmi\", \"CVD\", \"dm_rx\", \"chol_rx\",\n",
    "                   \"angina_rx\", \"lung_disease\", \"angina\", \"hba1c\", \"sbp\", \"dbp\", \"cancer\"]\n",
    "covariates1 = covariates1[[col for col in covariates_vars if col in covariates1.columns]].copy()\n",
    "covariates1 = covariates1.sort_values(\"SEQN\")\n",
    "\n",
    "# Step 4: Read dietary weight data (filter DAYS == 1)\n",
    "dietwt = pd.read_sas(os.path.join(folder_path, \"gg.sas7bdat\"), format=\"sas7bdat\")\n",
    "\n",
    "# Check for expected columns\n",
    "required_cols = [\"SEQN\", \"DAYS\", \"WTDRD1\", \"WTDR2D\", \"DR12DRST\"]\n",
    "missing = [col for col in required_cols if col not in dietwt.columns]\n",
    "if missing:\n",
    "    print(f\"Warning: Missing columns from gg.sas7bdat: {missing}\")\n",
    "\n",
    "# Filter and select\n",
    "dietwt = dietwt[dietwt[\"DAYS\"] == 1][[\"SEQN\", \"WTDRD1\", \"WTDR2D\", \"DR12DRST\"]].copy()\n",
    "dietwt = dietwt.sort_values(\"SEQN\")\n",
    "\n",
    "\n",
    "# Step 5: Read mortality data\n",
    "mort = pd.read_sas(os.path.join(folder_path, \"mortality9918.sas7bdat\"), format=\"sas7bdat\")\n",
    "mort = mort.sort_values(\"SEQN\")\n",
    "\n",
    "def summarize_df(name, df):\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Rows: {df.shape[0]}\")\n",
    "    print(f\"  Unique SEQN: {df['SEQN'].nunique()}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "summarize_df(\"scores2\", scores2)\n",
    "summarize_df(\"covar\", covar)\n",
    "summarize_df(\"covariates1\", covariates1)\n",
    "summarize_df(\"dietwt\", dietwt)\n",
    "summarize_df(\"mort\", mort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbd3a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔥🔥🔥🔥🔥🔥 NOW WORK AT HERE!!!!!!\n",
    "# try to extend covariates.csv to 99-18 currently is 03-18\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5bb8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Variable transformations\n",
    "score_mort[\"wt10\"] = score_mort[\"WTDRD1\"] / 10\n",
    "score_mort[\"wt\"] = score_mort[\"WTDR2D\"] / 8\n",
    "score_mort[\"i_FCS_sd\"] = score_mort[\"i_FCS\"] / 10.89\n",
    "score_mort[\"i_Optup_sd\"] = score_mort[\"i_Optup\"] / 8.17\n",
    "score_mort[\"i_nutri_sd\"] = -score_mort[\"i_nutri\"] / 3.17\n",
    "score_mort[\"i_HSR_sd\"] = score_mort[\"i_HSR\"] / 1.01\n",
    "score_mort[\"hei2015_sd\"] = score_mort[\"HEI2015_TOTAL_SCORE\"] / 13\n",
    "\n",
    "# Step 3: Recode death indicators\n",
    "for cause, code in {\n",
    "    \"death_heart\": \"001\", \"death_cancer\": \"002\", \"death_resp\": \"003\", \"Death_inj\": \"004\",\n",
    "    \"death_cerev\": \"005\", \"Death_alz\": \"006\", \"death_diabe\": \"007\",\n",
    "    \"Death_infl\": \"008\", \"Death_kid\": \"009\", \"death_other1\": \"010\"\n",
    "}.items():\n",
    "    score_mort[cause] = (score_mort[\"UCOD_LEADING\"] == code).astype(int)\n",
    "\n",
    "# Step 4: Composite categories\n",
    "score_mort[\"Death_other\"] = score_mort[[\"death_resp\", \"Death_inj\", \"Death_alz\", \"Death_infl\"]].sum(axis=1).clip(upper=1)\n",
    "score_mort[\"Death_oth2\"] = score_mort[[\"death_resp\", \"Death_inj\", \"Death_alz\", \"Death_infl\", \"death_other1\"]].sum(axis=1).clip(upper=1)\n",
    "score_mort[\"death_cvd\"] = score_mort[[\"death_heart\", \"death_cerev\"]].sum(axis=1).clip(upper=1)\n",
    "score_mort[\"death_cmd\"] = score_mort[[\"death_heart\", \"death_cerev\", \"death_diabe\"]].sum(axis=1).clip(upper=1)\n",
    "score_mort[\"death_cmdk\"] = score_mort[[\"death_heart\", \"death_cerev\", \"death_diabe\", \"Death_kid\"]].sum(axis=1).clip(upper=1)\n",
    "score_mort[\"death_cmdkh\"] = score_mort[\"death_cmdk\"]\n",
    "score_mort.loc[score_mort[\"DIABETES\"] == 1, \"death_cmdkh\"] = 1\n",
    "score_mort.loc[score_mort[\"HYPERTEN\"] == 1, \"death_cmdkh\"] = 1\n",
    "score_mort[\"death_cmd\"] = score_mort[\"death_cmd\"].fillna(0)\n",
    "score_mort.loc[score_mort[\"death_cmd\"] == 1, [\"Death_other\", \"Death_oth2\"]] = 0\n",
    "\n",
    "# Step 5: Multiple cause mortality\n",
    "score_mort[\"death_multi\"] = score_mort[\"MORTSTAT\"]\n",
    "score_mort.loc[score_mort[\"death_cmd\"] == 1, \"death_multi\"] = 1\n",
    "score_mort.loc[score_mort[\"death_cancer\"] == 1, \"death_multi\"] = 2\n",
    "score_mort.loc[score_mort[\"Death_oth2\"] == 1, \"death_multi\"] = 3\n",
    "\n",
    "# Step 6: Age & time vars\n",
    "score_mort[\"agesq\"] = score_mort[\"RIDAGEYR\"] ** 2\n",
    "score_mort[\"py\"] = score_mort[\"PERMTH_EXM\"] / 12\n",
    "score_mort[\"agestart\"] = score_mort[\"RIDAGEYR\"]\n",
    "score_mort[\"ageend\"] = score_mort[\"RIDAGEYR\"] + score_mort[\"py\"]\n",
    "\n",
    "# Step 7: Poverty\n",
    "score_mort[\"pir\"] = 5\n",
    "score_mort.loc[(score_mort[\"INDFMPIR\"] < 1.3) & (score_mort[\"INDFMPIR\"].notna()), \"pir\"] = 1\n",
    "score_mort.loc[(score_mort[\"INDFMPIR\"] >= 1.3), \"pir\"] = 2\n",
    "score_mort.loc[(score_mort[\"INDFMPIR\"] >= 3), \"pir\"] = 3\n",
    "\n",
    "# Step 8: Recode SNAP\n",
    "score_mort.loc[(score_mort[\"INDFMPIR\"].between(0, 1.3)) & (score_mort[\"SNAP\"] != 1), \"SNAP\"] = 2\n",
    "\n",
    "# Step 9: BMI categories\n",
    "score_mort[\"bmic\"] = pd.NA\n",
    "score_mort.loc[(score_mort[\"bmi\"] > 0) & (score_mort[\"bmi\"] < 18.5), \"bmic\"] = 0\n",
    "score_mort.loc[(score_mort[\"bmi\"] >= 18.5) & (score_mort[\"bmi\"] < 25), \"bmic\"] = 1\n",
    "score_mort.loc[(score_mort[\"bmi\"] >= 25), \"bmic\"] = 2\n",
    "score_mort.loc[(score_mort[\"bmi\"] >= 30), \"bmic\"] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa6d891",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/dengshuyue/Desktop/SDOH/analysis/code/Ref/2.1_Prepare data_covariates.sas', 'r', encoding='latin1') as f:\n",
    "    sas_code = f.read()\n",
    "\n",
    "print(sas_code[:20000])  # Preview the first X,000 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f398b907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diabetes Identification Note:\n",
    "# The final diabetes indicator variable used in the analysis is 'diabe2'.\n",
    "# This composite variable identifies diabetes based on the following criteria:\n",
    "# - Self-reported physician diagnosis (DIQ010), current insulin use (DIQ050), or oral medication use (DIQ070)\n",
    "# - Prescription drug data indicating diabetes treatment (dm_rx2)\n",
    "# - Fasting glucose ≥ 126 mg/dL (glu_dm)\n",
    "# - OGTT ≥ 200 mg/dL (ogtt_dm)\n",
    "# - HbA1c ≥ 6.5% (hb_dm)\n",
    "# Any one of these criteria being met will set 'diabe2' = 1, otherwise 0.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
