{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "719a46de",
   "metadata": {},
   "source": [
    "# 05 — Link SDOH (DEMO) + OCQ/HOQ/HIQ/FSQ with robust I/O (1999–2023)\n",
    "\n",
    "**Patches included**\n",
    "- Robust XPT reading with encoding fallbacks.\n",
    "- Use `pd.Int64Dtype()` for nullable integers instead of `'Int64'` strings.\n",
    "- Race/ethnicity combined as an aligned `Series`.\n",
    "- Adult filter uses safe numeric casting for `RIDAGEYR`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbe23004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: /Users/dengshuyue/Desktop/SDOH/analysis\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "ROOT = Path(\"/Users/dengshuyue/Desktop/SDOH/analysis\")\n",
    "DATA = ROOT / \"data\"\n",
    "OUT  = ROOT / \"output\"\n",
    "MOD  = DATA / \"nhanes_by_module\"\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "print(\"ROOT:\", ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cf0656c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_any(p: Path) -> pd.DataFrame:\n",
    "    s = p.suffix.lower()\n",
    "    if s == \".xpt\":\n",
    "        import pyreadstat\n",
    "        # Try default first, then latin-1 fallback (handles DEMO_L.xpt issue)\n",
    "        try:\n",
    "            df, _ = pyreadstat.read_xport(str(p))\n",
    "        except Exception:\n",
    "            df, _ = pyreadstat.read_xport(str(p), encoding=\"latin1\")\n",
    "    elif s == \".sas7bdat\":\n",
    "        df = pd.read_sas(str(p), format=\"sas7bdat\", encoding=\"latin1\")\n",
    "    elif s == \".csv\":\n",
    "        # Try utf-8, then latin-1\n",
    "        try:\n",
    "            df = pd.read_csv(p)\n",
    "        except UnicodeDecodeError:\n",
    "            df = pd.read_csv(p, encoding=\"latin1\")\n",
    "    elif s == \".parquet\":\n",
    "        df = pd.read_parquet(p)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file: {p}\")\n",
    "    df.columns = df.columns.str.upper()\n",
    "    return df\n",
    "\n",
    "def ensure_seqn(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    d = df.copy()\n",
    "    d.columns = d.columns.str.upper()\n",
    "    d[\"SEQN\"] = pd.to_numeric(d[\"SEQN\"], errors=\"coerce\").astype(pd.Int64Dtype())\n",
    "    return d.dropna(subset=[\"SEQN\"]).drop_duplicates(subset=[\"SEQN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2d011c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base file: /Users/dengshuyue/Desktop/SDOH/analysis/output/cov_core_mort_dep_all_1999_2023.parquet\n",
      "Base shape: (128809, 56)\n"
     ]
    }
   ],
   "source": [
    "BASE = OUT / \"cov_core_mort_dep_all_1999_2023.parquet\"\n",
    "if not BASE.exists():\n",
    "    for cand in [OUT/\"cov_core_mort_dep_1999_2023.parquet\", OUT/\"demo_cov_dep_1999_2023.parquet\"]:\n",
    "        if cand.exists():\n",
    "            BASE = cand\n",
    "            break\n",
    "print(\"Base file:\", BASE)\n",
    "base = pd.read_parquet(BASE)\n",
    "base.columns = base.columns.str.upper()\n",
    "base[\"SEQN\"] = pd.to_numeric(base[\"SEQN\"], errors=\"coerce\").astype(pd.Int64Dtype())\n",
    "print(\"Base shape:\", base.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff1f2dd",
   "metadata": {},
   "source": [
    "## DEMO across cycles → SDOH + Adult filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "401b10e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEMO shape: (128809, 13)\n",
      "SDOH tidy shape: (128809, 17)\n"
     ]
    }
   ],
   "source": [
    "DEMO_DIR = MOD / \"demo\"\n",
    "keep_cols = [\n",
    "    \"SEQN\",\"SDDSRVYR\",\"RIDAGEYR\",\"RIAGENDR\",\"WTMEC2YR\",\n",
    "    \"INDFMPIR\",\"INDFMINC\",\"DMDEDUC2\",\"DMDEDUC3\",\"DMDMARTL\",\n",
    "    \"RIDRETH1\",\"RIDRETH2\",\"RIDRETH3\",\n",
    "]\n",
    "\n",
    "cand = []\n",
    "if DEMO_DIR.exists():\n",
    "    for pat in [\"*DEMO*.xpt\",\"*DEMO*.XPT\",\"*demo*.sas7bdat\",\"*demo*.csv\",\"*demo*.parquet\"]:\n",
    "        cand.extend(sorted(DEMO_DIR.glob(pat)))\n",
    "\n",
    "parts = []\n",
    "for p in cand:\n",
    "    try:\n",
    "        d0 = read_any(p)\n",
    "        cols = [c for c in keep_cols if c in d0.columns]\n",
    "        if cols:\n",
    "            parts.append(d0[cols].copy())\n",
    "    except Exception as e:\n",
    "        print(\"Skip\", p.name, \"—\", e)\n",
    "\n",
    "demo = pd.concat(parts, ignore_index=True) if parts else pd.DataFrame(columns=keep_cols)\n",
    "demo = ensure_seqn(demo)\n",
    "print(\"DEMO shape:\", demo.shape)\n",
    "\n",
    "# Adult filter table (≥20)\n",
    "ridage = pd.to_numeric(demo.get(\"RIDAGEYR\"), errors=\"coerce\")\n",
    "age20 = demo.loc[ridage >= 20, [\"SEQN\",\"RIDAGEYR\",\"SDDSRVYR\"]].copy()\n",
    "\n",
    "# Derive SDOH harmonized\n",
    "d = demo.copy()\n",
    "d[\"PIR\"] = pd.to_numeric(d.get(\"INDFMPIR\", np.nan), errors=\"coerce\")\n",
    "d[\"PIR_CAT\"] = pd.cut(d[\"PIR\"], [-np.inf,1.3,2.0,4.0,np.inf], labels=[\"<1.30\",\"1.30–<2.00\",\"2.00–<4.00\",\"≥4.00\"]) \n",
    "d[\"EDU\"] = np.nan\n",
    "if \"DMDEDUC2\" in d: d.loc[d[\"DMDEDUC2\"].notna(), \"EDU\"] = d.loc[d[\"DMDEDUC2\"].notna(), \"DMDEDUC2\"]\n",
    "if \"DMDEDUC3\" in d: d.loc[d[\"EDU\"].isna() & d[\"DMDEDUC3\"].notna(), \"EDU\"] = d.loc[d[\"EDU\"].isna(), \"DMDEDUC3\"]\n",
    "d[\"EDU\"] = pd.to_numeric(d[\"EDU\"], errors=\"coerce\").astype(pd.Int64Dtype())\n",
    "\n",
    "edu2_map = {1:\"<9th grade\", 2:\"9–11th (incl. 12th, no diploma)\", 3:\"High school/GED\", 4:\"Some college or AA\", 5:\"College graduate or above\"}\n",
    "edu3_map = {1:\"<9th grade\", 2:\"9–11th\", 3:\"High school/GED\", 4:\"Some college or AA\", 5:\"College graduate or above\"}\n",
    "def label_edu(row):\n",
    "    v2, v3 = row.get(\"DMDEDUC2\", np.nan), row.get(\"DMDEDUC3\", np.nan)\n",
    "    if pd.notna(v2): return edu2_map.get(int(v2), np.nan)\n",
    "    if pd.notna(v3): return edu3_map.get(int(v3), np.nan)\n",
    "    return np.nan\n",
    "d[\"EDU_CAT\"] = d.apply(label_edu, axis=1)\n",
    "\n",
    "# Race/ethnicity as an aligned Series\n",
    "r3 = d.get(\"RIDRETH3\") if \"RIDRETH3\" in d else pd.Series(index=d.index, dtype=float)\n",
    "r2 = d.get(\"RIDRETH2\") if \"RIDRETH2\" in d else pd.Series(index=d.index, dtype=float)\n",
    "r1 = d.get(\"RIDRETH1\") if \"RIDRETH1\" in d else pd.Series(index=d.index, dtype=float)\n",
    "race_series = r3.where(r3.notna(), r2.where(r2.notna(), r1))\n",
    "race_series = pd.to_numeric(race_series, errors=\"coerce\").astype(pd.Int64Dtype())\n",
    "d[\"RACE_ETH_CODE\"] = race_series\n",
    "\n",
    "race_map_r3 = {1:\"Mexican American\",2:\"Other Hispanic\",3:\"NH White\",4:\"NH Black\",6:\"NH Asian\",7:\"NH Other/Multi\"}\n",
    "race_map_r2 = {1:\"Mexican American\",2:\"Other Hispanic\",3:\"NH White\",4:\"NH Black\",5:\"Other (incl. Multi)\"}\n",
    "race_map_r1 = {1:\"Mexican American\",2:\"Other Hispanic\",3:\"NH White\",4:\"NH Black\",5:\"Other (incl. Multi)\"}\n",
    "def label_race(row):\n",
    "    if pd.notna(row.get(\"RIDRETH3\", np.nan)): return race_map_r3.get(int(row[\"RIDRETH3\"]), np.nan)\n",
    "    if pd.notna(row.get(\"RIDRETH2\", np.nan)): return race_map_r2.get(int(row[\"RIDRETH2\"]), np.nan)\n",
    "    if pd.notna(row.get(\"RIDRETH1\", np.nan)): return race_map_r1.get(int(row[\"RIDRETH1\"]), np.nan)\n",
    "    return np.nan\n",
    "d[\"RACE_ETH\"] = d.apply(label_race, axis=1)\n",
    "\n",
    "d[\"MARITAL\"] = pd.to_numeric(d.get(\"DMDMARTL\", np.nan), errors=\"coerce\").astype(pd.Int64Dtype())\n",
    "mar_map = {1:\"Married\",2:\"Widowed\",3:\"Divorced\",4:\"Separated\",5:\"Never married\",6:\"Living with partner\"}\n",
    "d[\"MARITAL_CAT\"] = d[\"MARITAL\"].map(mar_map)\n",
    "\n",
    "sdoh = d[[c for c in [\n",
    "    \"SEQN\",\"SDDSRVYR\",\"INDFMPIR\",\"INDFMINC\",\"DMDEDUC2\",\"DMDEDUC3\",\"DMDMARTL\",\n",
    "    \"RIDRETH1\",\"RIDRETH2\",\"RIDRETH3\",\"PIR\",\"PIR_CAT\",\"EDU\",\"EDU_CAT\",\"RACE_ETH\",\"MARITAL\",\"MARITAL_CAT\"\n",
    "] if c in d.columns]].drop_duplicates(subset=[\"SEQN\"]).copy()\n",
    "sdoh = ensure_seqn(sdoh)\n",
    "print(\"SDOH tidy shape:\", sdoh.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573690ae",
   "metadata": {},
   "source": [
    "## Continue with OCQ/HOQ/HIQ/FSQ merge as in prior notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db2b8a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For brevity here, you can paste the OCQ/HOQ/HIQ/FSQ sections from the previous notebook.\n",
    "# They will now benefit from the robust helpers above (encoding fallbacks, Int64 dtype, adult filter)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e38023-9c8a-4746-bcc3-2cbd99eb0045",
   "metadata": {},
   "source": [
    "## HERE! next need extend those to 99-23 and check column name and content to match lu !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d62eb77-3684-491a-ae70-7cf48af1a62b",
   "metadata": {},
   "source": [
    "#### OCQ — Employment status (1999–2002 early + 2003–2018 main; adult filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40cda9fd-586c-45ff-8d0b-01709899c2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCQ shape: (55081, 4)\n"
     ]
    }
   ],
   "source": [
    "# --- OCQ (Employment): build EMPLOY + UNEMPLOYMENT --------------------------------\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "OCQ_DIR = MOD / \"ocq\"\n",
    "\n",
    "# Collect candidate files\n",
    "ocq_files = []\n",
    "if OCQ_DIR.exists():\n",
    "    for pat in [\"OCQ*.xpt\", \"ocq*.sas7bdat\", \"ocq*.parquet\", \"ocq*.csv\"]:\n",
    "        ocq_files.extend(sorted(OCQ_DIR.glob(pat)))\n",
    "\n",
    "# Simple cycle inference when SDDSRVYR is missing (suffix A=1 ... J=10)\n",
    "_CYCLE_HINT = {\"A\":1,\"B\":2,\"C\":3,\"D\":4,\"E\":5,\"F\":6,\"G\":7,\"H\":8,\"I\":9,\"J\":10,\"K\":11,\"L\":12}\n",
    "\n",
    "def _infer_cycle_if_missing(df: pd.DataFrame, path: Path) -> pd.DataFrame:\n",
    "    if \"SDDSRVYR\" not in df.columns:\n",
    "        name = path.stem.upper()\n",
    "        cyc = None\n",
    "        for k, v in _CYCLE_HINT.items():\n",
    "            if name.endswith(f\"_{k}\"):\n",
    "                cyc = v\n",
    "                break\n",
    "        if cyc is not None:\n",
    "            df = df.copy()\n",
    "            df[\"SDDSRVYR\"] = cyc\n",
    "    return df\n",
    "\n",
    "def _recode_employment(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"EMPLOY\"] = np.nan\n",
    "    if \"OCD150\" in df.columns:\n",
    "        df.loc[df[\"OCD150\"] == 1, \"EMPLOY\"] = 1  # working\n",
    "        df.loc[df[\"OCD150\"] == 3, \"EMPLOY\"] = 2  # not working\n",
    "    if \"OCQ380\" in df.columns:\n",
    "        df.loc[df[\"OCQ380\"] == 5, \"EMPLOY\"] = 2  # looking for work\n",
    "        df.loc[df[\"OCQ380\"] == 3, \"EMPLOY\"] = 3  # retired\n",
    "        df.loc[df[\"OCQ380\"].isin([4, 6]), \"EMPLOY\"] = 4  # disabled/other\n",
    "        df.loc[df[\"OCQ380\"].isin([1, 2, 7]), \"EMPLOY\"] = 5  # working or unknown\n",
    "    df[\"UNEMPLOYMENT\"] = (df[\"EMPLOY\"] == 2).astype(pd.Int64Dtype())\n",
    "    keep = [c for c in [\"SEQN\", \"SDDSRVYR\", \"EMPLOY\", \"UNEMPLOYMENT\"] if c in df.columns]\n",
    "    return df[keep]\n",
    "\n",
    "ocq_parts = []\n",
    "for p in ocq_files:\n",
    "    try:\n",
    "        df = read_any(p)\n",
    "        df = _infer_cycle_if_missing(df, p)\n",
    "        # adult filter (≥20) using DEMO-based age20\n",
    "        df = df.merge(age20[[\"SEQN\"]], on=\"SEQN\", how=\"inner\")\n",
    "        ocq_parts.append(_recode_employment(df))\n",
    "    except Exception as e:\n",
    "        print(\"OCQ skip\", p.name, \"—\", e)\n",
    "\n",
    "ocq = pd.concat(ocq_parts, ignore_index=True) if ocq_parts else pd.DataFrame(columns=[\"SEQN\",\"SDDSRVYR\",\"EMPLOY\",\"UNEMPLOYMENT\"])\n",
    "ocq = ensure_seqn(ocq)\n",
    "print(\"OCQ shape:\", ocq.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65319c8e-0bfc-4e9a-8d4e-c1d4a5580174",
   "metadata": {},
   "source": [
    "#### HOQ — Housing (subset) (1999–2002 early + 2003–2018 main; adult filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0072996d-f3d8-4253-b359-df1ed79eb454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HOQ shape: (55081, 4)\n"
     ]
    }
   ],
   "source": [
    "# --- HOQ (Housing): keep HOD050 + HOQ065 (7/9 -> NA) ------------------------------\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "HOQ_DIR = MOD / \"hoq\"\n",
    "\n",
    "hoq_files = []\n",
    "if HOQ_DIR.exists():\n",
    "    for pat in [\"HOQ*.xpt\", \"hoq*.sas7bdat\", \"hoq*.parquet\", \"hoq*.csv\"]:\n",
    "        hoq_files.extend(sorted(HOQ_DIR.glob(pat)))\n",
    "\n",
    "hoq_parts = []\n",
    "for p in hoq_files:\n",
    "    try:\n",
    "        df = read_any(p)\n",
    "        df = _infer_cycle_if_missing(df, p)\n",
    "        df = df.merge(age20[[\"SEQN\"]], on=\"SEQN\", how=\"inner\")\n",
    "        if \"HOQ065\" in df.columns:\n",
    "            df.loc[df[\"HOQ065\"].isin([7, 9]), \"HOQ065\"] = np.nan\n",
    "        keep = [c for c in [\"SEQN\", \"SDDSRVYR\", \"HOD050\", \"HOQ065\"] if c in df.columns]\n",
    "        if keep:\n",
    "            hoq_parts.append(df[keep])\n",
    "    except Exception as e:\n",
    "        print(\"HOQ skip\", p.name, \"—\", e)\n",
    "\n",
    "hoq_all = pd.concat(hoq_parts, ignore_index=True) if hoq_parts else pd.DataFrame(columns=[\"SEQN\",\"SDDSRVYR\",\"HOD050\",\"HOQ065\"])\n",
    "hoq_all = ensure_seqn(hoq_all)\n",
    "print(\"HOQ shape:\", hoq_all.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542b1d99-27db-408f-ae6c-7bf3e9cbe8c7",
   "metadata": {},
   "source": [
    "#### HIQ/HIQS — Health Insurance → INS recode (adult filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b92d0783-552c-4062-a46a-6d31d8e7d796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INS shape: (55081, 3)\n"
     ]
    }
   ],
   "source": [
    "# --- HIQ/HIQS (Insurance): build INS ---------------------------------------------\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "HIQ_DIR = MOD / \"hiq\"\n",
    "\n",
    "hiq_files = []\n",
    "if HIQ_DIR.exists():\n",
    "    for pat in [\"HIQ*.xpt\", \"hiq*.sas7bdat\", \"hiq*.parquet\", \"hiq*.csv\", \"HIQS*.sas7bdat\"]:\n",
    "        hiq_files.extend(sorted(HIQ_DIR.glob(pat)))\n",
    "\n",
    "hiq_parts = []\n",
    "for p in hiq_files:\n",
    "    try:\n",
    "        df = read_any(p)\n",
    "        df = _infer_cycle_if_missing(df, p)\n",
    "        df = df.merge(age20[[\"SEQN\"]], on=\"SEQN\", how=\"inner\")\n",
    "        hiq_parts.append(df)\n",
    "    except Exception as e:\n",
    "        print(\"HIQ skip\", p.name, \"—\", e)\n",
    "\n",
    "hiq_all = pd.concat(hiq_parts, ignore_index=True) if hiq_parts else pd.DataFrame()\n",
    "hiq_all.columns = hiq_all.columns.str.upper()\n",
    "\n",
    "# helper to safely access a column as a Series aligned to index\n",
    "def s(df: pd.DataFrame, col: str, default_val=np.nan):\n",
    "    return df[col] if col in df.columns else pd.Series(default_val, index=df.index)\n",
    "\n",
    "ins = pd.DataFrame({\"SEQN\": s(hiq_all, \"SEQN\")})\n",
    "if \"SDDSRVYR\" in hiq_all:\n",
    "    ins[\"SDDSRVYR\"] = hiq_all[\"SDDSRVYR\"]\n",
    "ins[\"INS\"] = np.nan  # will cast at end\n",
    "\n",
    "# Private\n",
    "cond_private = s(hiq_all,\"HIQ031A\").eq(14) | s(hiq_all,\"HID030A\").eq(1)\n",
    "ins.loc[cond_private, \"INS\"] = 1\n",
    "\n",
    "# Medicare\n",
    "cond_med = (s(hiq_all,\"HIQ031B\").eq(15) & ~s(hiq_all,\"HIQ031D\").eq(17) & ~s(hiq_all,\"HIQ031E\").eq(18)) | \\\n",
    "           (s(hiq_all,\"HID030B\").eq(1) & ~s(hiq_all,\"HID030C\").eq(1))\n",
    "ins.loc[cond_med, \"INS\"] = 2\n",
    "\n",
    "# Medicaid only or both → 3\n",
    "cond_mcaid_only = ((s(hiq_all,\"HIQ031D\").eq(17) | s(hiq_all,\"HIQ031E\").eq(18)) & ~s(hiq_all,\"HIQ031B\").eq(15)) | \\\n",
    "                  (~s(hiq_all,\"HID030B\").eq(1) & s(hiq_all,\"HID030C\").eq(1))\n",
    "cond_both = (s(hiq_all,\"HIQ031B\").eq(15) & s(hiq_all,\"HIQ031D\").eq(17)) | \\\n",
    "            (s(hiq_all,\"HID030B\").eq(1) & s(hiq_all,\"HID030C\").eq(1))\n",
    "ins.loc[cond_mcaid_only | cond_both, \"INS\"] = 3\n",
    "\n",
    "# Other\n",
    "other_cols = [c for c in [\"HIQ031C\",\"HIQ031F\",\"HIQ031G\",\"HIQ031H\",\"HIQ031I\",\"HID030D\"] if c in hiq_all.columns]\n",
    "cond_other = hiq_all[other_cols].eq(1).any(axis=1) if other_cols else pd.Series(False, index=hiq_all.index)\n",
    "ins.loc[cond_other, \"INS\"] = 5\n",
    "\n",
    "# None\n",
    "none_conds = []\n",
    "if \"HIQ011\" in hiq_all: none_conds.append(hiq_all[\"HIQ011\"].eq(2))\n",
    "if \"HID010\" in hiq_all: none_conds.append(hiq_all[\"HID010\"].eq(2))\n",
    "if none_conds:\n",
    "    from functools import reduce\n",
    "    ins.loc[reduce(lambda a,b: a|b, none_conds), \"INS\"] = 0\n",
    "\n",
    "ins = ensure_seqn(ins)\n",
    "ins[\"INS\"] = pd.to_numeric(ins[\"INS\"], errors=\"coerce\").astype(pd.Int64Dtype())\n",
    "print(\"INS shape:\", ins.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509b9230-00dc-4b48-93bb-8feaccc1de02",
   "metadata": {},
   "source": [
    "#### FSQ/FSQS — SNAP & Food Security (adult filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1bff35c8-a671-41ca-bd39-b1880346acdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNAP/FS shape: (55081, 5)\n",
      "Non-missing rates: {'FSDHH': np.float64(0.7943755559993464), 'SNAP': np.float64(0.8117681233093081), 'FS': np.float64(0.7943755559993464)}\n",
      "FS=1 (insecure) share: 0.1901268426465547\n"
     ]
    }
   ],
   "source": [
    "# --- FSQ/FSQS (SNAP & Food Security): build `snap` with SEQN, SDDSRVYR, FSDHH, SNAP, FS ----\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "FSQ_DIR = MOD / \"fsq\"  # e.g., Path(\"/Users/.../modules/fsq\")\n",
    "\n",
    "# 1) Collect files\n",
    "fsq_files = []\n",
    "if FSQ_DIR.exists():\n",
    "    for pat in [\"FSQ*.xpt\", \"fsq*.sas7bdat\", \"fsq*.parquet\", \"fsq*.csv\", \"FSQS*.sas7bdat\"]:\n",
    "        fsq_files.extend(sorted(FSQ_DIR.glob(pat)))\n",
    "\n",
    "# 2) Read & harmonize per-file, restrict to your analysis universe (age20)\n",
    "fsq_parts = []\n",
    "for p in fsq_files:\n",
    "    try:\n",
    "        df = read_any(p)                             # your generic reader\n",
    "        df = _infer_cycle_if_missing(df, p)          # should add SDDSRVYR if missing\n",
    "        if \"SEQN\" not in df.columns:\n",
    "            print(\"FSQ skip\", p.name, \"— no SEQN\")\n",
    "            continue\n",
    "        # keep only participants you care about (optional but keeps things tidy)\n",
    "        if 'age20' in globals() and isinstance(age20, pd.DataFrame) and \"SEQN\" in age20.columns:\n",
    "            df = df.merge(age20[[\"SEQN\"]], on=\"SEQN\", how=\"inner\")\n",
    "        fsq_parts.append(df)\n",
    "    except Exception as e:\n",
    "        print(\"FSQ skip\", p.name, \"—\", e)\n",
    "\n",
    "fsq_all = pd.concat(fsq_parts, ignore_index=True) if fsq_parts else pd.DataFrame()\n",
    "fsq_all.columns = fsq_all.columns.str.upper()\n",
    "\n",
    "# 3) Build the harmonized `snap` piece\n",
    "def col_or_nan(df, col):\n",
    "    return df[col] if col in df.columns else pd.Series(np.nan, index=df.index)\n",
    "\n",
    "snap = pd.DataFrame({\"SEQN\": col_or_nan(fsq_all, \"SEQN\")})\n",
    "if \"SDDSRVYR\" in fsq_all.columns:\n",
    "    snap[\"SDDSRVYR\"] = fsq_all[\"SDDSRVYR\"]\n",
    "\n",
    "# FSDHH (household food security: 1=High, 2=Marginal, 3=Low, 4=Very low)\n",
    "if \"FSDHH\" in fsq_all.columns:\n",
    "    snap[\"FSDHH\"] = pd.to_numeric(fsq_all[\"FSDHH\"], errors=\"coerce\")\n",
    "else:\n",
    "    snap[\"FSDHH\"] = np.nan\n",
    "\n",
    "# SNAP (program participation; multiple instrument variants across years)\n",
    "snap[\"SNAP\"] = np.nan\n",
    "\n",
    "# map common variants to 0/1; unknown/missing stay NaN\n",
    "if \"FSQ165\" in fsq_all.columns:  # sometimes a negative wording (e.g., not received)\n",
    "    snap.loc[fsq_all[\"FSQ165\"].eq(2), \"SNAP\"] = 0\n",
    "if \"FSQ012\" in fsq_all.columns:\n",
    "    snap.loc[fsq_all[\"FSQ012\"].eq(1), \"SNAP\"] = 1\n",
    "    snap.loc[fsq_all[\"FSQ012\"].eq(2), \"SNAP\"] = 0\n",
    "if \"FSQ171\" in fsq_all.columns:\n",
    "    snap.loc[fsq_all[\"FSQ171\"].eq(1), \"SNAP\"] = 1\n",
    "    snap.loc[fsq_all[\"FSQ171\"].eq(2), \"SNAP\"] = 0\n",
    "if \"FSD170N\" in fsq_all.columns:\n",
    "    # number of months on SNAP; >=1 implies participated\n",
    "    snap.loc[pd.to_numeric(fsq_all[\"FSD170N\"], errors=\"coerce\").ge(1), \"SNAP\"] = 1\n",
    "if \"FSQ170\" in fsq_all.columns:\n",
    "    snap.loc[fsq_all[\"FSQ170\"].eq(1), \"SNAP\"] = 1\n",
    "    snap.loc[(fsq_all[\"FSQ170\"].eq(2)) & (pd.to_numeric(col_or_nan(fsq_all, \"FSD170N\"), errors=\"coerce\").fillna(0).lt(1)), \"SNAP\"] = 0\n",
    "if \"FSD200\" in fsq_all.columns:\n",
    "    snap.loc[fsq_all[\"FSD200\"].eq(1), \"SNAP\"] = 1\n",
    "\n",
    "# FS (binary food insecurity) — CORRECT mapping:\n",
    "# 1,2 = food secure (0); 3,4 = food insecure (1)\n",
    "snap[\"FS\"] = snap[\"FSDHH\"].map({1:0, 2:0, 3:1, 4:1})\n",
    "\n",
    "# 4) Types, dedupe, key hygiene\n",
    "snap = ensure_seqn(snap)  # your helper; ensures Int64 SEQN and sorts/indexes\n",
    "for c in [\"FSDHH\", \"SNAP\", \"FS\"]:\n",
    "    if c in snap.columns:\n",
    "        snap[c] = pd.to_numeric(snap[c], errors=\"coerce\").astype(\"Int64\")\n",
    "snap = snap.drop_duplicates(\"SEQN\")\n",
    "\n",
    "print(\"SNAP/FS shape:\", snap.shape)\n",
    "print(\"Non-missing rates:\", {c: snap[c].notna().mean() if c in snap else None for c in [\"FSDHH\",\"SNAP\",\"FS\"]})\n",
    "print(\"FS=1 (insecure) share:\", (snap[\"FS\"]==1).mean() if \"FS\" in snap else None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3dfbba-d3e5-488f-bbdb-473d267ee368",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf329dc-17f8-46b6-9f92-e3c460b1d8bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "30bb8ab2-14e8-4ff0-8493-c91ca212f1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNAP/FS shape: (55081, 5)\n"
     ]
    }
   ],
   "source": [
    "# === FSQ (SNAP & Food Security) ===============================================\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "FSQ_DIR = MOD / \"fsq\"\n",
    "\n",
    "# 1) gather files\n",
    "fsq_files = []\n",
    "if FSQ_DIR.exists():\n",
    "    for pat in [\"FSQ*.xpt\",\"fsq*.sas7bdat\",\"fsq*.parquet\",\"fsq*.csv\",\"FSQS*.sas7bdat\"]:\n",
    "        fsq_files.extend(sorted(FSQ_DIR.glob(pat)))\n",
    "\n",
    "# 2) read, add cycle, restrict to analysis universe\n",
    "fsq_parts = []\n",
    "for p in fsq_files:\n",
    "    try:\n",
    "        df = read_any(p)\n",
    "        df = _infer_cycle_if_missing(df, p)\n",
    "        if \"SEQN\" not in df.columns:\n",
    "            print(\"FSQ skip\", p.name, \"— no SEQN\"); \n",
    "            continue\n",
    "        if 'age20' in globals() and isinstance(age20, pd.DataFrame) and \"SEQN\" in age20.columns:\n",
    "            df = df.merge(age20[[\"SEQN\"]], on=\"SEQN\", how=\"inner\")\n",
    "        fsq_parts.append(df)\n",
    "    except Exception as e:\n",
    "        print(\"FSQ skip\", p.name, \"—\", e)\n",
    "\n",
    "fsq_all = pd.concat(fsq_parts, ignore_index=True) if fsq_parts else pd.DataFrame()\n",
    "fsq_all.columns = fsq_all.columns.str.upper()\n",
    "\n",
    "# 3) harmonize -> snap (SEQN, SDDSRVYR, FSDHH, SNAP, FS)\n",
    "def col_or_nan(df, col):\n",
    "    return df[col] if col in df.columns else pd.Series(np.nan, index=df.index)\n",
    "\n",
    "snap = pd.DataFrame({\"SEQN\": col_or_nan(fsq_all, \"SEQN\")})\n",
    "if \"SDDSRVYR\" in fsq_all.columns:\n",
    "    snap[\"SDDSRVYR\"] = fsq_all[\"SDDSRVYR\"]\n",
    "\n",
    "# FSDHH (1=High, 2=Marginal, 3=Low, 4=Very low)\n",
    "snap[\"FSDHH\"] = pd.to_numeric(col_or_nan(fsq_all, \"FSDHH\"), errors=\"coerce\")\n",
    "\n",
    "# SNAP (map variants to 0/1; leave others NaN)\n",
    "snap[\"SNAP\"] = np.nan\n",
    "if \"FSQ165\" in fsq_all.columns: snap.loc[fsq_all[\"FSQ165\"].eq(2), \"SNAP\"] = 0\n",
    "if \"FSQ012\" in fsq_all.columns:\n",
    "    snap.loc[fsq_all[\"FSQ012\"].eq(1), \"SNAP\"] = 1\n",
    "    snap.loc[fsq_all[\"FSQ012\"].eq(2), \"SNAP\"] = 0\n",
    "if \"FSQ171\" in fsq_all.columns:\n",
    "    snap.loc[fsq_all[\"FSQ171\"].eq(1), \"SNAP\"] = 1\n",
    "    snap.loc[fsq_all[\"FSQ171\"].eq(2), \"SNAP\"] = 0\n",
    "if \"FSD170N\" in fsq_all.columns:\n",
    "    snap.loc[pd.to_numeric(fsq_all[\"FSD170N\"], errors=\"coerce\").ge(1), \"SNAP\"] = 1\n",
    "if \"FSQ170\" in fsq_all.columns:\n",
    "    snap.loc[fsq_all[\"FSQ170\"].eq(1), \"SNAP\"] = 1\n",
    "    snap.loc[(fsq_all[\"FSQ170\"].eq(2)) & (pd.to_numeric(col_or_nan(fsq_all,\"FSD170N\"),errors=\"coerce\").fillna(0).lt(1)), \"SNAP\"] = 0\n",
    "if \"FSD200\" in fsq_all.columns: snap.loc[fsq_all[\"FSD200\"].eq(1), \"SNAP\"] = 1\n",
    "\n",
    "# FS binary (1 = food insecure for FSDHH 3/4; 0 for 1/2)\n",
    "snap[\"FS\"] = snap[\"FSDHH\"].map({1:0, 2:0, 3:1, 4:1})\n",
    "\n",
    "# types + dedupe\n",
    "snap = ensure_seqn(snap)\n",
    "for c in [\"FSDHH\",\"SNAP\",\"FS\"]:\n",
    "    if c in snap.columns:\n",
    "        snap[c] = pd.to_numeric(snap[c], errors=\"coerce\").astype(\"Int64\")\n",
    "snap = snap.drop_duplicates(\"SEQN\")\n",
    "\n",
    "print(\"SNAP/FS shape:\", snap.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3615ee2f-808d-4308-b9fe-b0a340ea7fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Quick audit — cycles seen & coverage if you want a pulse check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "de4967b5-8a02-4793-806a-bb81b906430b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDOH cycles: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8), np.int64(9), np.int64(10), np.int64(12), np.int64(66)]\n",
      "SDOH | base=128,809 | addon(seqn)=128,809 | inner keeps=128,809\n",
      "OCQ cycles: [np.int64(2), np.int64(3)]\n",
      " OCQ | base=128,809 | addon(seqn)=55,081 | inner keeps=55,081\n",
      "HOQ cycles: [np.int64(2)]\n",
      " HOQ | base=128,809 | addon(seqn)=55,081 | inner keeps=55,081\n",
      "HIQ cycles: [np.int64(2)]\n",
      " HIQ | base=128,809 | addon(seqn)=55,081 | inner keeps=55,081\n",
      "FSQ cycles: [np.int64(2)]\n",
      " FSQ | base=128,809 | addon(seqn)=55,081 | inner keeps=55,081\n"
     ]
    }
   ],
   "source": [
    "# === Audit helpers ============================================================\n",
    "def _as_int64(s): \n",
    "    return pd.to_numeric(s, errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "def _cycles(df: pd.DataFrame, name: str):\n",
    "    if df is None or df.empty: print(f\"{name} cycles: (empty)\"); return\n",
    "    if \"SDDSRVYR\" in df.columns and df[\"SDDSRVYR\"].notna().any():\n",
    "        cyc = pd.to_numeric(df[\"SDDSRVYR\"], errors=\"coerce\").dropna().astype(int).unique()\n",
    "        print(f\"{name} cycles:\", sorted(cyc))\n",
    "    else:\n",
    "        print(f\"{name} cycles: (none)\")\n",
    "\n",
    "def _coverage(base: pd.DataFrame, addon: pd.DataFrame, name: str):\n",
    "    if addon is None or addon.empty: print(f\"{name:>4} | (addon empty)\"); return\n",
    "    b = base.copy(); a = addon.copy()\n",
    "    if \"SEQN\" in b: b[\"SEQN\"] = _as_int64(b[\"SEQN\"])\n",
    "    if \"SEQN\" in a: a[\"SEQN\"] = _as_int64(a[\"SEQN\"])\n",
    "    base_u  = b[\"SEQN\"].dropna().nunique()\n",
    "    add_u   = a[\"SEQN\"].dropna().nunique()\n",
    "    inner_u = b.merge(a[[\"SEQN\"]].drop_duplicates(), on=\"SEQN\", how=\"inner\")[\"SEQN\"].nunique()\n",
    "    print(f\"{name:>4} | base={base_u:,} | addon(seqn)={add_u:,} | inner keeps={inner_u:,}\")\n",
    "\n",
    "# pick base to compare to\n",
    "_base_for_audit = base if \"base\" in globals() else (master if \"master\" in globals() else sdoh)\n",
    "\n",
    "for (n, df_) in [(\"SDOH\", sdoh), (\"OCQ\", ocq), (\"HOQ\", hoq_all), (\"HIQ\", ins), (\"FSQ\", snap)]:\n",
    "    _cycles(df_, n)\n",
    "    _coverage(_base_for_audit, df_, n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930cac6d-10c4-4093-a087-8d8fb7b9e031",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b7c5b2-00f3-477c-96ad-5d3b0a1a9d11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cab9b3d0-e13f-417e-b409-baa2b26e84ca",
   "metadata": {},
   "source": [
    "## Merge these modules onto your base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "57ae3439-c082-4700-9441-9062dc73d24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged shape: (128809, 72)\n"
     ]
    }
   ],
   "source": [
    "# === Merge SDOH + modules (left-join with upsert) =============================\n",
    "master = ensure_seqn(base)\n",
    "if master[\"SEQN\"].dtype != \"Int64\":\n",
    "    master[\"SEQN\"] = pd.to_numeric(master[\"SEQN\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# SDOH first\n",
    "sdoh_cols = [\"SEQN\",\"PIR\",\"PIR_CAT\",\"INDFMINC\",\"EDU\",\"EDU_CAT\",\"RACE_ETH\",\"MARITAL\",\"MARITAL_CAT\"]\n",
    "sdoh_piece = sdoh[[c for c in sdoh_cols if c in sdoh.columns]].drop_duplicates(\"SEQN\")\n",
    "\n",
    "def _left_upsert(m: pd.DataFrame, piece: pd.DataFrame, tag: str):\n",
    "    if piece is None or piece.empty:\n",
    "        print(f\"ℹ️ {tag}: empty — skipped\"); \n",
    "        return m\n",
    "    p = ensure_seqn(piece)\n",
    "    if p[\"SEQN\"].duplicated().any():\n",
    "        dups = p.loc[p[\"SEQN\"].duplicated(), \"SEQN\"].unique()[:5]\n",
    "        raise RuntimeError(f\"{tag}: duplicate SEQN detected (e.g., {dups.tolist()} …).\")\n",
    "    overlap = (set(p.columns) & set(m.columns)) - {\"SEQN\"}\n",
    "    tmp = m.merge(p, on=\"SEQN\", how=\"left\", validate=\"one_to_one\", suffixes=(\"\", f\"_{tag.lower()}\"))\n",
    "    for c in sorted(overlap):\n",
    "        tmp[c] = tmp[c].fillna(tmp[f\"{c}_{tag.lower()}\"])\n",
    "        tmp.drop(columns=[f\"{c}_{tag.lower()}\"], inplace=True)\n",
    "    return tmp\n",
    "\n",
    "if not sdoh_piece.empty:\n",
    "    master = _left_upsert(master, sdoh_piece, \"SDOH\")\n",
    "else:\n",
    "    print(\"ℹ️ SDOH: empty — skipped\")\n",
    "\n",
    "# Modules (including FSQ built in Cell A)\n",
    "pieces = {\n",
    "    \"OCQ\": ocq[[c for c in [\"SEQN\",\"EMPLOY\",\"UNEMPLOYMENT\"] if c in ocq.columns]],\n",
    "    \"HOQ\": hoq_all[[c for c in [\"SEQN\",\"HOD050\",\"HOQ065\"] if c in hoq_all.columns]],\n",
    "    \"HIQ\": ins[[c for c in [\"SEQN\",\"INS\"] if c in ins.columns]],\n",
    "    \"FSQ\": snap[[c for c in [\"SEQN\",\"SNAP\",\"FSDHH\",\"FS\"] if c in snap.columns]],\n",
    "}\n",
    "\n",
    "for name, piece in pieces.items():\n",
    "    master = _left_upsert(master, piece, name)\n",
    "\n",
    "print(\"Merged shape:\", master.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d52574e9-292e-410c-8645-3bc9287fa3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FS NA rates: {'SNAP': 0.6528736346062776, 'FSDHH': 0.6603110031131365, 'FS': 0.6603110031131365}\n",
      "Saved:\n",
      "  /Users/dengshuyue/Desktop/SDOH/analysis/out/demo_mt_cv_dp_sdoh.parquet \n",
      "  /Users/dengshuyue/Desktop/SDOH/analysis/out/demo_mt_cv_dp_sdoh_20250916_2033.parquet\n"
     ]
    }
   ],
   "source": [
    "# FS sanity\n",
    "if all(c in master.columns for c in [\"SNAP\",\"FSDHH\",\"FS\"]):\n",
    "    print(\"FS NA rates:\", master[[\"SNAP\",\"FSDHH\",\"FS\"]].isna().mean().to_dict())\n",
    "\n",
    "# finalize & save\n",
    "demo_mt_cv_dp_sdoh = master.copy()\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "out_dir = Path(\"/Users/dengshuyue/Desktop/SDOH/analysis/out\"); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "canon = out_dir / \"demo_mt_cv_dp_sdoh.parquet\"\n",
    "ver   = out_dir / f\"demo_mt_cv_dp_sdoh_{datetime.now().strftime('%Y%m%d_%H%M')}.parquet\"\n",
    "\n",
    "demo_mt_cv_dp_sdoh.to_parquet(canon, index=False)\n",
    "demo_mt_cv_dp_sdoh.to_parquet(ver, index=False)\n",
    "print(\"Saved:\\n \", canon, \"\\n \", ver)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f8cc54b6-57e3-406a-a13a-84e33dabc201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename \n",
    "demo_mt_cv_dp_sdoh = master\n",
    "del master  # optional: drop old handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8cb89fc6-9607-4136-84b7-27d6f3fc3f04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     <NA>\n",
       "1     <NA>\n",
       "2     <NA>\n",
       "3     <NA>\n",
       "4     <NA>\n",
       "      ... \n",
       "95    <NA>\n",
       "96    <NA>\n",
       "97    <NA>\n",
       "98    <NA>\n",
       "99    <NA>\n",
       "Name: FS, Length: 100, dtype: Int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_mt_cv_dp_sdoh[\"FS\"].head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bc07b103-67aa-4025-8f76-0b0a8fa7974f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SDDSRVYR\n",
       "1.0     0.000\n",
       "2.0     0.000\n",
       "3.0     0.476\n",
       "4.0     0.476\n",
       "5.0     0.579\n",
       "6.0     0.583\n",
       "7.0     0.567\n",
       "8.0     0.559\n",
       "9.0     0.553\n",
       "10.0    0.567\n",
       "12.0    0.000\n",
       "66.0    0.000\n",
       "Name: FS, dtype: float64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3) Coverage by cycle (which cycles actually have FS data?)\n",
    "demo_mt_cv_dp_sdoh.groupby(\"SDDSRVYR\")[\"FS\"].apply(lambda s: s.notna().mean()).round(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ab42caf9-4904-4a7e-9ec7-a0e618afb9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEQN\n",
      "SDDSRVYR\n",
      "SDMVPSU\n",
      "SDMVSTRA\n",
      "WTMEC2YR\n",
      "AGE_YR\n",
      "RIAGENDR\n",
      "SEX\n",
      "FEMALE\n",
      "SMK_STATUS\n",
      "CIGS_PER_DAY\n",
      "PACK_YEARS\n",
      "FORMER_SMOKER\n",
      "DRINKS_PER_DAY\n",
      "ALCOHOL_CAT\n",
      "LTPA\n",
      "METSCORE\n",
      "IMP\n",
      "BMXWT\n",
      "BMXHT\n",
      "BMI\n",
      "BMI_CLAS\n",
      "DIABETES\n",
      "HTN\n",
      "HIGH_CHOL\n",
      "CVD\n",
      "CANCER\n",
      "SBP\n",
      "DBP\n",
      "TCHOL\n",
      "HDL\n",
      "LDL\n",
      "TG\n",
      "DMDHHSIZ\n",
      "ELIGSTAT\n",
      "MORTSTAT\n",
      "PERMTH_EXM\n",
      "PERMTH_INT\n",
      "UCOD_LEADING\n",
      "IS_POST2018\n",
      "IS_ADULT\n",
      "MORTALITY_COVERED\n",
      "EVENT\n",
      "CENSORED\n",
      "FU_YRS_EXM\n",
      "FU_YRS_INT\n",
      "UCOD_LABEL\n",
      "PHQ9\n",
      "PHQ9_GE10\n",
      "DPQ_CAT\n",
      "DEP_IMP\n",
      "CIDI_SCORE_RAW\n",
      "CIDI_12M_MDE\n",
      "WTSCI2YR\n",
      "DEP_HARMONIZED\n",
      "DEP_SOURCE\n",
      "PIR\n",
      "PIR_CAT\n",
      "INDFMINC\n",
      "EDU\n",
      "EDU_CAT\n",
      "RACE_ETH\n",
      "MARITAL\n",
      "MARITAL_CAT\n",
      "EMPLOY\n",
      "UNEMPLOYMENT\n",
      "HOD050\n",
      "HOQ065\n",
      "INS\n",
      "SNAP\n",
      "FSDHH\n",
      "FS\n"
     ]
    }
   ],
   "source": [
    "demo_mt_cv_dp_sdoh.columns.tolist()\n",
    "print(\"\\n\".join(demo_mt_cv_dp_sdoh.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354b3575-7982-492f-bdca-852b7b607e29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41af8f6-cc74-4eb1-8eae-664a66b98544",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaead903-b28e-47b0-ae89-705bef7648b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3178b8c-ee6d-4eca-b918-fc13abb6c12d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba32523f-3d87-4446-b229-8f73b7e9fc56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99f6272-b957-4185-8eff-1c9d2a4a3faf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9cf05551-4cf6-4e75-a112-776d750adb66",
   "metadata": {},
   "source": [
    "#### check my column name with previous coworker's name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6481c8c5-7a4c-4701-8db9-efb3e8c7e546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo_mt_cv_dp_sdoh.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9005ed5d-63cf-4717-8edd-dde68ed2ff67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Demo vs Ref85 (column names) ===\n",
      "Ref85 count: 85\n",
      "Your count:  72\n",
      "Exact matches (case-sensitive): 16\n",
      "\n",
      "Missing in your df:\n",
      "['ALCG2', 'DR12DRST', 'Death_alz', 'Death_cancer', 'Death_cerev', 'Death_diabe', 'Death_heart', 'Death_infl', 'Death_inj', 'Death_kid', 'Death_oth2', 'Death_other', 'Death_resp', 'HEI2015_TOTAL_SCORE', 'HYPERTEN', 'INDFMPIR', 'RACE', 'RIDAGEYR', 'SMK', 'SMK_AVG', 'WTDR2D', 'WTDRD1', 'ageend', 'agesq', 'agestart', 'angina', 'angina_rx', 'bmi', 'bmic', 'cancer', 'chol_rx', 'dbp', 'death_cmd', 'death_cmdk', 'death_cmdkh', 'death_cvd', 'death_multi', 'death_other1', 'dm_rx', 'dm_self', 'employ', 'hba1c', 'hdl', 'hei2015_sd', 'i_FCS', 'i_FCS_sd', 'i_HSR', 'i_HSR_sd', 'i_Optup', 'i_Optup_sd', 'i_nutri', 'i_nutri_sd', 'include', 'ins', 'ldl', 'lung_disease', 'marriage', 'met_hr', 'perE_alco', 'pir', 'py', 'sbp', 'sdmvpsu', 'sdmvstra', 'tchol', 'tg', 'unemployment', 'wt', 'wt10']\n",
      "\n",
      "Extras in your df:\n",
      "['AGE_YR', 'ALCOHOL_CAT', 'BMI', 'BMI_CLAS', 'BMXHT', 'BMXWT', 'CANCER', 'CENSORED', 'CIDI_12M_MDE', 'CIDI_SCORE_RAW', 'CIGS_PER_DAY', 'DBP', 'DEP_HARMONIZED', 'DEP_IMP', 'DEP_SOURCE', 'DMDHHSIZ', 'DPQ_CAT', 'DRINKS_PER_DAY', 'EDU_CAT', 'EMPLOY', 'EVENT', 'FEMALE', 'FORMER_SMOKER', 'FU_YRS_EXM', 'FU_YRS_INT', 'HDL', 'HIGH_CHOL', 'HTN', 'IMP', 'INDFMINC', 'INS', 'IS_ADULT', 'IS_POST2018', 'LDL', 'LTPA', 'MARITAL', 'MARITAL_CAT', 'METSCORE', 'MORTALITY_COVERED', 'PACK_YEARS', 'PHQ9', 'PHQ9_GE10', 'PIR', 'PIR_CAT', 'RACE_ETH', 'RIAGENDR', 'SBP', 'SDMVPSU', 'SDMVSTRA', 'SMK_STATUS', 'TCHOL', 'TG', 'UCOD_LABEL', 'UNEMPLOYMENT', 'WTMEC2YR', 'WTSCI2YR']\n",
      "\n",
      "Proposed renames (old → new):\n",
      "  SDMVPSU → sdmvpsu\n",
      "  SDMVSTRA → sdmvstra\n",
      "  BMI → bmi\n",
      "  CANCER → cancer\n",
      "  SBP → sbp\n",
      "  DBP → dbp\n",
      "  TCHOL → tchol\n",
      "  HDL → hdl\n",
      "  LDL → ldl\n",
      "  TG → tg\n",
      "  PIR → pir\n",
      "  EMPLOY → employ\n",
      "  UNEMPLOYMENT → unemployment\n",
      "  INS → ins\n",
      "  HTN → HYPERTEN\n",
      "\n",
      "Still missing after proposed renames:\n",
      "['ALCG2', 'DR12DRST', 'Death_alz', 'Death_cancer', 'Death_cerev', 'Death_diabe', 'Death_heart', 'Death_infl', 'Death_inj', 'Death_kid', 'Death_oth2', 'Death_other', 'Death_resp', 'HEI2015_TOTAL_SCORE', 'INDFMPIR', 'RACE', 'RIDAGEYR', 'SMK', 'SMK_AVG', 'WTDR2D', 'WTDRD1', 'ageend', 'agesq', 'agestart', 'angina', 'angina_rx', 'bmic', 'chol_rx', 'death_cmd', 'death_cmdk', 'death_cmdkh', 'death_cvd', 'death_multi', 'death_other1', 'dm_rx', 'dm_self', 'hba1c', 'hei2015_sd', 'i_FCS', 'i_FCS_sd', 'i_HSR', 'i_HSR_sd', 'i_Optup', 'i_Optup_sd', 'i_nutri', 'i_nutri_sd', 'include', 'lung_disease', 'marriage', 'met_hr', 'perE_alco', 'py', 'wt', 'wt10']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 85-column reference (from your CSV, after dropping DIABE, ins2, unemployment2,\n",
    "# and the lowercase duplicates: death_heart, death_cancer, death_resp, death_cerev, death_diabe)\n",
    "REF85 = [\n",
    " 'SEQN','marriage','SDDSRVYR','HOD050','HOQ065','employ','unemployment','ELIGSTAT','MORTSTAT',\n",
    " 'UCOD_LEADING','DIABETES','HYPERTEN','PERMTH_INT','PERMTH_EXM','Death_heart','Death_cancer',\n",
    " 'Death_resp','Death_cerev','Death_diabe','Death_other','death_cvd','death_cmd','SNAP','FSDHH','FS',\n",
    " 'ins','RIDAGEYR','SEX','RACE','EDU','INDFMPIR','SMK_AVG','SMK','ALCG2','HEI2015_TOTAL_SCORE',\n",
    " 'WTDRD1','WTDR2D','DR12DRST','i_FCS','i_Optup','i_HSR','i_nutri','sdmvpsu','sdmvstra','met_hr',\n",
    " 'perE_alco','dm_self','tchol','hdl','ldl','tg','bmi','CVD','dm_rx','chol_rx','angina_rx','lung_disease',\n",
    " 'angina','hba1c','sbp','dbp','cancer','wt10','wt','i_FCS_sd','i_Optup_sd','i_nutri_sd','i_HSR_sd',\n",
    " 'hei2015_sd','Death_inj','Death_alz','Death_infl','Death_kid','death_other1','Death_oth2','death_cmdk',\n",
    " 'death_cmdkh','death_multi','agesq','py','agestart','ageend','pir','bmic','include'\n",
    "]\n",
    "\n",
    "df = demo_mt_cv_dp_sdoh.copy()  # your DataFrame\n",
    "\n",
    "# Case-insensitive helpers\n",
    "ref_upper = {c.upper(): c for c in REF85}\n",
    "df_upper  = {c.upper(): c for c in df.columns}\n",
    "\n",
    "# Exact, missing, extras (case-sensitive)\n",
    "exact = sorted(set(REF85) & set(df.columns))\n",
    "missing = sorted(set(REF85) - set(df.columns))\n",
    "extras = sorted(set(df.columns) - set(REF85))\n",
    "\n",
    "# Build an OLD->NEW rename map:\n",
    "# 1) case-only differences: if df has SBP but ref expects 'sbp', suggest SBP->sbp\n",
    "rename_map = {}\n",
    "for up, df_name in df_upper.items():\n",
    "    if up in ref_upper:\n",
    "        ref_name = ref_upper[up]\n",
    "        if df_name != ref_name:\n",
    "            rename_map[df_name] = ref_name\n",
    "\n",
    "# 2) common synonym fixes (ONLY if present in df and missing in ref)\n",
    "synonyms = {\n",
    "    # examples: tweak as you find more\n",
    "    'HTN': 'HYPERTEN',\n",
    "    'TOTAL_CHOL' : 'tchol',\n",
    "    'HDL_CHOL'   : 'hdl',\n",
    "    'LDL_CHOL'   : 'ldl',\n",
    "    'TRIG'       : 'tg',\n",
    "    'BMI'        : 'bmi',\n",
    "    'SBP'        : 'sbp',\n",
    "    'DBP'        : 'dbp',\n",
    "    'INS': 'ins',\n",
    "    'EMPLOYMENT_STATUS': 'employ',\n",
    "    'UNEMPLOYED': 'unemployment',\n",
    "    'WT': 'wt',\n",
    "    'WT10': 'wt10',\n",
    "    'MET_HR': 'met_hr',\n",
    "}\n",
    "for old, new in synonyms.items():\n",
    "    if old in df.columns and new in REF85 and new not in df.columns:\n",
    "        rename_map[old] = new\n",
    "\n",
    "# Preview post-rename set\n",
    "preview_cols = [rename_map.get(c, c) for c in df.columns]\n",
    "missing_after = sorted(set(REF85) - set(preview_cols))\n",
    "\n",
    "print(\"=== Demo vs Ref85 (column names) ===\")\n",
    "print(f\"Ref85 count: {len(REF85)}\")\n",
    "print(f\"Your count:  {len(df.columns)}\")\n",
    "print(f\"Exact matches (case-sensitive): {len(exact)}\")\n",
    "print(\"\\nMissing in your df:\")\n",
    "print(missing)\n",
    "print(\"\\nExtras in your df:\")\n",
    "print(extras)\n",
    "print(\"\\nProposed renames (old → new):\")\n",
    "for k, v in rename_map.items():\n",
    "    print(f\"  {k} → {v}\")\n",
    "print(\"\\nStill missing after proposed renames:\")\n",
    "print(missing_after)\n",
    "\n",
    "# When ready to apply:\n",
    "# df = df.rename(columns=rename_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be640130-ff59-451d-89e8-5d387ce9bf2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12a8d3bc-b954-4c63-b58c-53728ebcba0e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
