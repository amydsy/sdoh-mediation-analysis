{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23e4e0a1",
   "metadata": {},
   "source": [
    "<h1> 00 — merge demo, mort, sdoh</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24edfeb",
   "metadata": {},
   "source": [
    "<h2>Shared environment and helper functions used across notebooks.</h2>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7908d2b4-cccd-4a07-bcae-4a6bed5e87b5",
   "metadata": {},
   "source": [
    "## Step 1: NHANES demographic data load and find need column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abee722b-5904-4471-8178-f220f758f3da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0b6d89-d3f8-4e28-a162-28f08752f101",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67450437-0913-4bd8-9743-304328edec12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22223cfd-b25b-4c10-9451-64c0a18fcfb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b28ed9-82e6-4915-84ea-ee1dd1389ee4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d68a636-9a64-424c-8434-029dcfed26ae",
   "metadata": {},
   "source": [
    "## merge above needed column to demo_cov_depression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5190243-2d5b-4019-a2c4-da204c075b18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a362a6-d325-4bc9-b76a-0d6261990b28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbca17ab-b4d2-443d-9e84-0083448f1a8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fceb15f-f051-4f4d-bb66-0936de39864f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50aff7dc-77ed-4d06-be26-be021a58156a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a9ad7e-d9e0-4488-a042-b17f884dbb95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6df6670d-1b16-413f-a629-93b05ac9befb",
   "metadata": {},
   "source": [
    "## Step 3: merge demo_mort_depr with below sdoh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf215fc8-2203-4b3b-8627-6cd85b7df2c7",
   "metadata": {},
   "source": [
    "\n",
    "<h3> OCQ / HOQ / HIQ / FSQ — early (1999–2002) + main (2003–2018), adult filter, tidy outputs </h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fafb19b5-bf54-4410-bf49-c7478074760c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Prereqs & helpers\n",
    "import pandas as pd, numpy as np, os\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path(\"/Users/dengshuyue/Desktop/SDOH/analysis\")\n",
    "DATA = ROOT / \"data\"\n",
    "MOD  = DATA / \"nhanes_by_module\"\n",
    "\n",
    "def read_any(p: Path) -> pd.DataFrame:\n",
    "    s = p.suffix.lower()\n",
    "    if s == \".xpt\":\n",
    "        import pyreadstat\n",
    "        df, _ = pyreadstat.read_xport(str(p))\n",
    "    elif s == \".sas7bdat\":\n",
    "        df = pd.read_sas(str(p), format=\"sas7bdat\", encoding=\"latin1\")\n",
    "    elif s == \".csv\":\n",
    "        df = pd.read_csv(p)\n",
    "    elif s == \".parquet\":\n",
    "        df = pd.read_parquet(p)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file: {p}\")\n",
    "    df.columns = df.columns.str.upper()\n",
    "    return df\n",
    "\n",
    "def filter_adults(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Adults ≥20 using demo9923 already in memory (SEQN, RIDAGEYR).\"\"\"\n",
    "    if \"demo9923\" not in globals():\n",
    "        raise RuntimeError(\"demo9923 not found in memory; load/build it first.\")\n",
    "    demo = demo9923.copy()\n",
    "    demo.columns = demo.columns.str.upper()\n",
    "    age = demo[[\"SEQN\",\"RIDAGEYR\"]].dropna()\n",
    "    age = age[age[\"RIDAGEYR\"] >= 20]\n",
    "    return df.merge(age, on=\"SEQN\", how=\"inner\")\n",
    "\n",
    "def s_or_false(df: pd.DataFrame, col: str):\n",
    "    return df[col] if col in df.columns else pd.Series(False, index=df.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "45c69f37-41a6-4c07-b2f9-e74b3fd0da9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCQ: (55081, 4)\n"
     ]
    }
   ],
   "source": [
    "# %% OCQ: 1999–2018 (employment recode)\n",
    "OCQ = MOD / \"ocq\"\n",
    "ocq_main = OCQ / \"ocq.sas7bdat\"\n",
    "ocq_early_files = [(OCQ/\"OCQ.xpt\",1), (OCQ/\"OCQ_B.xpt\",2)]\n",
    "\n",
    "def recode_employment(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"EMPLOY\"] = np.nan\n",
    "    if \"OCD150\" in df:\n",
    "        df.loc[df[\"OCD150\"] == 1, \"EMPLOY\"] = 1\n",
    "        df.loc[df[\"OCD150\"] == 3, \"EMPLOY\"] = 2\n",
    "    if \"OCQ380\" in df:\n",
    "        df.loc[df[\"OCQ380\"] == 5, \"EMPLOY\"] = 2\n",
    "        df.loc[df[\"OCQ380\"] == 3, \"EMPLOY\"] = 3\n",
    "        df.loc[df[\"OCQ380\"].isin([4,6]), \"EMPLOY\"] = 4\n",
    "        df.loc[df[\"OCQ380\"].isin([1,2,7]), \"EMPLOY\"] = 5\n",
    "    df[\"UNEMPLOYMENT\"] = (df[\"EMPLOY\"] == 2).astype(\"Int64\")\n",
    "    keep = [c for c in [\"SEQN\",\"EMPLOY\",\"UNEMPLOYMENT\",\"SDDSRVYR\"] if c in df.columns]\n",
    "    return df[keep]\n",
    "\n",
    "# early (adult filter applied)\n",
    "parts = []\n",
    "for p, cyc in ocq_early_files:\n",
    "    if p.exists():\n",
    "        df = read_any(p)\n",
    "        df[\"SDDSRVYR\"] = cyc\n",
    "        df = filter_adults(df)\n",
    "        parts.append(recode_employment(df))\n",
    "ocq_early = pd.concat(parts, ignore_index=True) if parts else pd.DataFrame()\n",
    "\n",
    "# main (adult filter applied)\n",
    "if not ocq_main.exists():\n",
    "    raise FileNotFoundError(f\"Missing {ocq_main}\")\n",
    "ocq_main_df = filter_adults(read_any(ocq_main))\n",
    "ocq_main_df = recode_employment(ocq_main_df)\n",
    "\n",
    "# combine + hygiene\n",
    "ocq = pd.concat([ocq_early, ocq_main_df], ignore_index=True)\n",
    "ocq[\"SEQN\"] = pd.to_numeric(ocq[\"SEQN\"], errors=\"coerce\").astype(\"Int64\")\n",
    "if \"SDDSRVYR\" in ocq: ocq[\"SDDSRVYR\"] = pd.to_numeric(ocq[\"SDDSRVYR\"], errors=\"coerce\").astype(\"Int64\")\n",
    "ocq = ocq.dropna(subset=[\"SEQN\"]).drop_duplicates(subset=[\"SEQN\"])\n",
    "print(\"OCQ:\", ocq.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2ddc2a1-84de-4fe3-bde2-a1e34e5f69a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HOQ: (55081, 4)\n"
     ]
    }
   ],
   "source": [
    "# %% HOQ: 1999–2018 (housing subset)\n",
    "HOQ = MOD / \"hoq\"\n",
    "hoq_main = HOQ / \"hoq.sas7bdat\"\n",
    "hoq_early_files = [(HOQ/\"HOQ.xpt\",1), (HOQ/\"HOQ_B.xpt\",2), (HOQ/\"hoq.sas7bdat\",1), (HOQ/\"hoq_b.sas7bdat\",2)]\n",
    "\n",
    "def preprocess_hoq(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    if \"HOQ065\" in df: df.loc[df[\"HOQ065\"].isin([7,9]), \"HOQ065\"] = np.nan\n",
    "    keep = [c for c in [\"SEQN\",\"HOD050\",\"HOQ065\",\"SDDSRVYR\"] if c in df.columns]\n",
    "    return df[keep]\n",
    "\n",
    "parts = []\n",
    "seen = set()\n",
    "for p, cyc in hoq_early_files:\n",
    "    if p.exists() and cyc not in seen:\n",
    "        df = read_any(p); df[\"SDDSRVYR\"] = cyc\n",
    "        df = filter_adults(df)\n",
    "        parts.append(preprocess_hoq(df)); seen.add(cyc)\n",
    "hoq_early = pd.concat(parts, ignore_index=True) if parts else pd.DataFrame()\n",
    "\n",
    "if not hoq_main.exists():\n",
    "    raise FileNotFoundError(f\"Missing {hoq_main}\")\n",
    "hoq_main_df = preprocess_hoq(filter_adults(read_any(hoq_main)))\n",
    "\n",
    "hoq_all = pd.concat([hoq_early, hoq_main_df], ignore_index=True)\n",
    "for c in (\"SEQN\",\"SDDSRVYR\"):\n",
    "    if c in hoq_all: hoq_all[c] = pd.to_numeric(hoq_all[c], errors=\"coerce\").astype(\"Int64\")\n",
    "hoq_all = hoq_all.dropna(subset=[\"SEQN\"]).drop_duplicates(subset=[\"SEQN\"])\n",
    "print(\"HOQ:\", hoq_all.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c2d62239-e5d0-4ab7-9239-72247529cc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INS: (55081, 3)\n"
     ]
    }
   ],
   "source": [
    "# %% HIQ/HIQS: 1999–2018 (insurance category INS)\n",
    "HIQ = MOD / \"hiq\"\n",
    "hiq_main = HIQ / \"hiqs.sas7bdat\"\n",
    "hiq_early_files = [(HIQ/\"HIQ.xpt\",1), (HIQ/\"HIQ_B.xpt\",2), (HIQ/\"hiq.sas7bdat\",1), (HIQ/\"hiq_b.sas7bdat\",2)]\n",
    "\n",
    "# stack early + main, adult filter\n",
    "parts, seen = [], set()\n",
    "for p, cyc in hiq_early_files:\n",
    "    if p.exists() and cyc not in seen:\n",
    "        df = read_any(p); df[\"SDDSRVYR\"] = cyc\n",
    "        parts.append(filter_adults(df)); seen.add(cyc)\n",
    "hiq_early = pd.concat(parts, ignore_index=True) if parts else pd.DataFrame()\n",
    "\n",
    "if not hiq_main.exists():\n",
    "    raise FileNotFoundError(f\"Missing {hiq_main}\")\n",
    "hiqs = filter_adults(read_any(hiq_main))\n",
    "\n",
    "hiq_all = pd.concat([hiq_early, hiqs], ignore_index=True)\n",
    "hiq_all.columns = hiq_all.columns.str.upper()\n",
    "\n",
    "# build INS (0 none, 1 private, 2 Medicare, 3 Medicaid/both, 5 other)\n",
    "ins = pd.DataFrame({\"SEQN\": hiq_all[\"SEQN\"]})\n",
    "if \"SDDSRVYR\" in hiq_all: ins[\"SDDSRVYR\"] = hiq_all[\"SDDSRVYR\"]\n",
    "ins[\"INS\"] = np.nan\n",
    "\n",
    "cond_private = (s_or_false(hiq_all,\"HIQ031A\") == 14) | (s_or_false(hiq_all,\"HID030A\") == 1)\n",
    "ins.loc[cond_private, \"INS\"] = 1\n",
    "\n",
    "cond_med = (\n",
    "    (s_or_false(hiq_all,\"HIQ031B\") == 15) &\n",
    "    (s_or_false(hiq_all,\"HIQ031D\") != 17) &\n",
    "    (s_or_false(hiq_all,\"HIQ031E\") != 18)\n",
    ") | (\n",
    "    (s_or_false(hiq_all,\"HID030B\") == 1) &\n",
    "    (s_or_false(hiq_all,\"HID030C\") != 1)\n",
    ")\n",
    "ins.loc[cond_med, \"INS\"] = 2\n",
    "\n",
    "cond_mcaid_only = (\n",
    "    ((s_or_false(hiq_all,\"HIQ031D\") == 17) | (s_or_false(hiq_all,\"HIQ031E\") == 18)) &\n",
    "    (s_or_false(hiq_all,\"HIQ031B\") != 15)\n",
    ") | (\n",
    "    (s_or_false(hiq_all,\"HID030B\") != 1) &\n",
    "    (s_or_false(hiq_all,\"HID030C\") == 1)\n",
    ")\n",
    "cond_both = (\n",
    "    (s_or_false(hiq_all,\"HIQ031B\") == 15) & (s_or_false(hiq_all,\"HIQ031D\") == 17)\n",
    ") | (\n",
    "    (s_or_false(hiq_all,\"HID030B\") == 1) & (s_or_false(hiq_all,\"HID030C\") == 1)\n",
    ")\n",
    "ins.loc[cond_mcaid_only | cond_both, \"INS\"] = 3\n",
    "\n",
    "other_cols = [c for c in [\"HIQ031C\",\"HIQ031F\",\"HIQ031G\",\"HIQ031H\",\"HIQ031I\"] if c in hiq_all.columns]\n",
    "cond_other = hiq_all[other_cols].eq(1).any(axis=1) if other_cols else pd.Series(False, index=hiq_all.index)\n",
    "cond_other = cond_other | (s_or_false(hiq_all,\"HID030D\") == 1)\n",
    "ins.loc[cond_other, \"INS\"] = 5\n",
    "\n",
    "none_conds = []\n",
    "if \"HIQ011\" in hiq_all: none_conds.append(hiq_all[\"HIQ011\"] == 2)\n",
    "if \"HID010\" in hiq_all: none_conds.append(hiq_all[\"HID010\"] == 2)\n",
    "if none_conds: ins.loc[np.logical_or.reduce(none_conds), \"INS\"] = 0\n",
    "\n",
    "for c in (\"SEQN\",\"SDDSRVYR\"):\n",
    "    if c in ins: ins[c] = pd.to_numeric(ins[c], errors=\"coerce\").astype(\"Int64\")\n",
    "ins = ins.dropna(subset=[\"SEQN\"]).drop_duplicates(subset=[\"SEQN\"])\n",
    "print(\"INS:\", ins.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b6cbb417-9a50-4bc7-a6e5-956d4e310f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNAP/FS: (55081, 5)\n"
     ]
    }
   ],
   "source": [
    "# %% FSQ/FSQS: 1999–2018 (SNAP & FS)\n",
    "FSQ = MOD / \"fsq\"\n",
    "fsq_main = FSQ / \"fsqs.sas7bdat\"\n",
    "fsq_early_files = [(FSQ/\"FSQ.xpt\",1), (FSQ/\"FSQ_B.xpt\",2), (FSQ/\"fsq.sas7bdat\",1), (FSQ/\"fsq_b.sas7bdat\",2)]\n",
    "\n",
    "parts, seen = [], set()\n",
    "for p, cyc in fsq_early_files:\n",
    "    if p.exists() and cyc not in seen:\n",
    "        df = read_any(p); df[\"SDDSRVYR\"] = cyc\n",
    "        parts.append(filter_adults(df)); seen.add(cyc)\n",
    "fsq_early = pd.concat(parts, ignore_index=True) if parts else pd.DataFrame()\n",
    "\n",
    "if not fsq_main.exists():\n",
    "    raise FileNotFoundError(f\"Missing {fsq_main}\")\n",
    "fsqs = filter_adults(read_any(fsq_main))\n",
    "\n",
    "fsq_all = pd.concat([fsq_early, fsqs], ignore_index=True)\n",
    "fsq_all.columns = fsq_all.columns.str.upper()\n",
    "\n",
    "snap = pd.DataFrame({\"SEQN\": fsq_all[\"SEQN\"]})\n",
    "if \"SDDSRVYR\" in fsq_all: snap[\"SDDSRVYR\"] = fsq_all[\"SDDSRVYR\"]\n",
    "if \"FSDHH\" in fsq_all:   snap[\"FSDHH\"] = fsq_all[\"FSDHH\"]\n",
    "\n",
    "snap[\"SNAP\"] = np.nan\n",
    "if \"FSQ165\" in fsq_all: snap.loc[fsq_all[\"FSQ165\"] == 2, \"SNAP\"] = 0\n",
    "if \"FSQ012\" in fsq_all:\n",
    "    snap.loc[fsq_all[\"FSQ012\"] == 1, \"SNAP\"] = 1\n",
    "    snap.loc[fsq_all[\"FSQ012\"] == 2, \"SNAP\"] = 0\n",
    "if \"FSQ171\" in fsq_all:\n",
    "    snap.loc[fsq_all[\"FSQ171\"] == 1, \"SNAP\"] = 1\n",
    "    snap.loc[fsq_all[\"FSQ171\"] == 2, \"SNAP\"] = 0\n",
    "if \"FSD170N\" in fsq_all: snap.loc[fsq_all[\"FSD170N\"] >= 1, \"SNAP\"] = 1\n",
    "if \"FSQ170\" in fsq_all:\n",
    "    snap.loc[fsq_all[\"FSQ170\"] == 1, \"SNAP\"] = 1\n",
    "    snap.loc[(fsq_all[\"FSQ170\"] == 2) & (fsq_all.get(\"FSD170N\", pd.Series(index=fsq_all.index)) < 1), \"SNAP\"] = 0\n",
    "if \"FSD200\" in fsq_all: snap.loc[fsq_all[\"FSD200\"] == 1, \"SNAP\"] = 1\n",
    "\n",
    "snap[\"FS\"] = np.nan\n",
    "if \"FSDHH\" in fsq_all:\n",
    "    snap.loc[fsq_all[\"FSDHH\"].isin([1,2]), \"FS\"] = 1\n",
    "    snap.loc[fsq_all[\"FSDHH\"] > 2,        \"FS\"] = 0\n",
    "\n",
    "for c in (\"SEQN\",\"SDDSRVYR\"):\n",
    "    if c in snap: snap[c] = pd.to_numeric(snap[c], errors=\"coerce\").astype(\"Int64\")\n",
    "snap = snap[[c for c in [\"SEQN\",\"SNAP\",\"FSDHH\",\"FS\",\"SDDSRVYR\"] if c in snap.columns]]\n",
    "snap = snap.dropna(subset=[\"SEQN\"]).drop_duplicates(subset=[\"SEQN\"])\n",
    "print(\"SNAP/FS:\", snap.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8d26ca-28f2-4757-a289-e0f49eeddc92",
   "metadata": {},
   "source": [
    "<h3> Audit — cycles present & inner vs left-merge coverage </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "25bbe1e7-f589-40bc-ab50-48471b5a2d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCQ cycles: [np.int64(1), np.int64(2)]\n",
      "  OCQ | base=56,253 | inner keeps=52,287 | left keeps=56,253\n",
      "HOQ cycles: [np.int64(1), np.int64(2)]\n",
      "  HOQ | base=56,253 | inner keeps=52,287 | left keeps=56,253\n",
      "HIQ cycles: [np.int64(1), np.int64(2)]\n",
      "  HIQ | base=56,253 | inner keeps=52,287 | left keeps=56,253\n",
      "FSQ cycles: [np.int64(1), np.int64(2)]\n",
      "  FSQ | base=56,253 | inner keeps=52,287 | left keeps=56,253\n"
     ]
    }
   ],
   "source": [
    "# %% Audit helpers + run\n",
    "def cycles(df, name):\n",
    "    if \"SDDSRVYR\" in df.columns and df[\"SDDSRVYR\"].notna().any():\n",
    "        print(f\"{name} cycles:\", sorted(df[\"SDDSRVYR\"].dropna().astype(int).unique()))\n",
    "    else:\n",
    "        tmp = df.merge(demo9923[[\"SEQN\",\"SDDSRVYR\"]], on=\"SEQN\", how=\"left\")\n",
    "        if tmp[\"SDDSRVYR\"].notna().any():\n",
    "            print(f\"{name} cycles (via DEMO):\", sorted(tmp[\"SDDSRVYR\"].dropna().astype(int).unique()))\n",
    "        else:\n",
    "            print(f\"{name} cycles: (none found)\")\n",
    "\n",
    "def coverage(base, addon, name):\n",
    "    base_u = base[\"SEQN\"].dropna().nunique()\n",
    "    inner_u = base.merge(addon[[\"SEQN\"]].drop_duplicates(), on=\"SEQN\", how=\"inner\")[\"SEQN\"].nunique()\n",
    "    left_u  = base.merge(addon[[\"SEQN\"]].drop_duplicates(),  on=\"SEQN\", how=\"left\")[\"SEQN\"].nunique()\n",
    "    print(f\"{name:>5} | base={base_u:,} | inner keeps={inner_u:,} | left keeps={left_u:,}\")\n",
    "\n",
    "# Choose a base for audit (prefers mortality, else diet)\n",
    "if \"mort_with_demo\" in globals():\n",
    "    base_df = mort_with_demo\n",
    "elif \"SODH_diet_mort\" in globals():\n",
    "    base_df = SODH_diet_mort\n",
    "else:\n",
    "    base_df = demo9923\n",
    "\n",
    "for (n, df) in [(\"OCQ\", ocq), (\"HOQ\", hoq_all), (\"HIQ\", ins), (\"FSQ\", snap)]:\n",
    "    cycles(df, n)\n",
    "    coverage(base_df, df, n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d1787b-6ade-4b30-b68d-a0541415c66a",
   "metadata": {},
   "source": [
    "<h3> Left-merge onto mortality base and save </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eea6dd35-0057-47fa-84dd-f7ac60d033ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT = /Users/dengshuyue/Desktop/SDOH/analysis\n",
      "OUT  = /Users/dengshuyue/Desktop/SDOH/analysis/output\n",
      "SDOH_ROOT env = None\n",
      "OUT (fixed) = /Users/dengshuyue/Desktop/SDOH/analysis/output\n"
     ]
    }
   ],
   "source": [
    "# check if output path is correct \n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"ROOT =\", ROOT)\n",
    "print(\"OUT  =\", OUT)\n",
    "print(\"SDOH_ROOT env =\", os.environ.get(\"SDOH_ROOT\"))\n",
    "\n",
    "# ✅ Force the correct OUT (one-time reassignment in this kernel)\n",
    "OUT = ROOT / \"output\"\n",
    "print(\"OUT (fixed) =\", OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "17d6efd7-ae4c-4335-bbcf-8f6c4721e9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ master shape: (56253, 20)\n",
      "\n",
      "Non-missing coverage (%):\n",
      "  UNEMPLOYMENT: 92.9%\n",
      "  HOD050: 91.6%\n",
      "  HOQ065: 91.5%\n",
      "  INS: 87.7%\n",
      "  EMPLOY: 86.8%\n",
      "  SNAP: 76.3%\n",
      "  FS: 74.7%\n",
      "\n",
      "Saved:\n",
      "  /Users/dengshuyue/Desktop/SDOH/analysis/output/nhanes_mort_demo_sdoh_1999_2018.parquet \n",
      "  /Users/dengshuyue/Desktop/SDOH/analysis/output/nhanes_mort_demo_sdoh_1999_2018.csv\n"
     ]
    }
   ],
   "source": [
    "# %% Build master (LEFT merges onto mort_with_demo) and save — refined\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "if \"mort_with_demo\" not in globals():\n",
    "    print(\"⚠️ mort_with_demo not found — skip merge/save.\")\n",
    "else:\n",
    "    def _key(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        d = df.copy()\n",
    "        d.columns = d.columns.str.upper()\n",
    "        d[\"SEQN\"] = pd.to_numeric(d[\"SEQN\"], errors=\"coerce\").astype(\"Int64\")\n",
    "        return d.dropna(subset=[\"SEQN\"]).drop_duplicates(subset=[\"SEQN\"])\n",
    "\n",
    "    base = _key(mort_with_demo)\n",
    "\n",
    "    # Pick only needed columns if present\n",
    "    ocq_m  = _key(ocq)      [[c for c in (\"SEQN\",\"EMPLOY\",\"UNEMPLOYMENT\") if c in ocq.columns]]      if \"ocq\"      in globals() else None\n",
    "    hoq_m  = _key(hoq_all)  [[c for c in (\"SEQN\",\"HOD050\",\"HOQ065\")        if c in hoq_all.columns]]  if \"hoq_all\"  in globals() else None\n",
    "    ins_m  = _key(ins)      [[c for c in (\"SEQN\",\"INS\")                    if c in ins.columns]]      if \"ins\"      in globals() else None\n",
    "    snap_m = _key(snap)     [[c for c in (\"SEQN\",\"SNAP\",\"FSDHH\",\"FS\")      if c in snap.columns]]     if \"snap\"     in globals() else None\n",
    "\n",
    "    # Merge in sequence with one-to-one validation\n",
    "    master = base\n",
    "    for name, piece in [(\"OCQ\", ocq_m), (\"HOQ\", hoq_m), (\"INS\", ins_m), (\"FSQ\", snap_m)]:\n",
    "        if piece is None:\n",
    "            print(f\"ℹ️  {name}: not available — skipped\")\n",
    "            continue\n",
    "        try:\n",
    "            master = master.merge(piece, on=\"SEQN\", how=\"left\", validate=\"one_to_one\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Merge failed for {name} (check duplicate SEQN).\") from e\n",
    "\n",
    "    print(\"✅ master shape:\", master.shape)\n",
    "\n",
    "    # Quick audit (non-missing coverage for key SDOH fields)\n",
    "    audit_cols = [c for c in [\"EMPLOY\",\"UNEMPLOYMENT\",\"HOD050\",\"HOQ065\",\"INS\",\"SNAP\",\"FS\"] if c in master.columns]\n",
    "    if audit_cols:\n",
    "        cov = (master[audit_cols].notna().mean()*100).round(1).sort_values(ascending=False)\n",
    "        print(\"\\nNon-missing coverage (%):\")\n",
    "        for k, v in cov.items(): print(f\"  {k}: {v}%\")\n",
    "\n",
    "    # Save (use predefined OUT)\n",
    "    OUT.mkdir(parents=True, exist_ok=True)\n",
    "    base_name = \"nhanes_mort_demo_sdoh_1999_2018\"\n",
    "    master.to_parquet(OUT / f\"{base_name}.parquet\", index=False)\n",
    "    master.to_csv(    OUT / f\"{base_name}.csv\",     index=False)\n",
    "    print(\"\\nSaved:\\n \", OUT / f\"{base_name}.parquet\", \"\\n \", OUT / f\"{base_name}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "da359655-d392-4f88-9443-c1f3dd2a21c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ master shape: (56253, 20)\n",
      "\n",
      "Non-missing coverage (%):\n",
      "  UNEMPLOYMENT: 92.9%\n",
      "  HOD050: 91.6%\n",
      "  HOQ065: 91.5%\n",
      "  INS: 87.7%\n",
      "  EMPLOY: 86.8%\n",
      "  SNAP: 76.3%\n",
      "  FS: 74.7%\n",
      "\n",
      "Saved:\n",
      "  /Users/dengshuyue/Desktop/SDOH/analysis/out/nhanes_mort_demo_sdoh_1999_2018.parquet \n",
      "  /Users/dengshuyue/Desktop/SDOH/analysis/out/nhanes_mort_demo_sdoh_1999_2018.csv\n"
     ]
    }
   ],
   "source": [
    "# %% Build master (LEFT merges onto mort_with_demo) and save — refined\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# If OUT wasn't defined earlier, set it here\n",
    "ROOT = Path(\"/Users/dengshuyue/Desktop/SDOH/analysis\")\n",
    "OUT = ROOT / \"out\"\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if \"mort_with_demo\" not in globals():\n",
    "    print(\"⚠️ mort_with_demo not found — skip merge/save.\")\n",
    "else:\n",
    "    def _key(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        d = df.copy()\n",
    "        d.columns = d.columns.str.upper()\n",
    "        # normalize SEQN\n",
    "        d[\"SEQN\"] = pd.to_numeric(d[\"SEQN\"], errors=\"coerce\")\n",
    "        # if it's float but all .0, move to Int64\n",
    "        if pd.api.types.is_float_dtype(d[\"SEQN\"]) and ((d[\"SEQN\"] % 1) == 0).all():\n",
    "            d[\"SEQN\"] = d[\"SEQN\"].astype(\"Int64\")\n",
    "        else:\n",
    "            d[\"SEQN\"] = d[\"SEQN\"].astype(\"Int64\")\n",
    "        return d.dropna(subset=[\"SEQN\"]).drop_duplicates(subset=[\"SEQN\"])\n",
    "\n",
    "    base = _key(mort_with_demo)\n",
    "\n",
    "    # helper to pick columns AFTER uppercasing\n",
    "    def _pick_cols(df_up: pd.DataFrame, candidates: tuple[str, ...]) -> pd.DataFrame:\n",
    "        keep = [c for c in candidates if c in df_up.columns]\n",
    "        return df_up[keep] if keep else df_up[[\"SEQN\"]]\n",
    "\n",
    "    ocq_m  = _pick_cols(_key(ocq),      (\"SEQN\",\"EMPLOY\",\"UNEMPLOYMENT\")) if \"ocq\"      in globals() else None\n",
    "    hoq_m  = _pick_cols(_key(hoq_all),  (\"SEQN\",\"HOD050\",\"HOQ065\"))        if \"hoq_all\"  in globals() else None\n",
    "    ins_m  = _pick_cols(_key(ins),      (\"SEQN\",\"INS\"))                    if \"ins\"      in globals() else None\n",
    "    snap_m = _pick_cols(_key(snap),     (\"SEQN\",\"SNAP\",\"FSDHH\",\"FS\"))      if \"snap\"     in globals() else None\n",
    "\n",
    "    # Merge in sequence with one-to-one validation\n",
    "    master = base\n",
    "    for name, piece in [(\"OCQ\", ocq_m), (\"HOQ\", hoq_m), (\"INS\", ins_m), (\"FSQ\", snap_m)]:\n",
    "        if piece is None:\n",
    "            print(f\"ℹ️  {name}: not available — skipped\")\n",
    "            continue\n",
    "        # make sure no dupes in piece (already handled by _key → but safe)\n",
    "        if piece[\"SEQN\"].duplicated().any():\n",
    "            raise RuntimeError(f\"{name}: duplicate SEQN detected after cleaning.\")\n",
    "        try:\n",
    "            master = master.merge(piece, on=\"SEQN\", how=\"left\", validate=\"one_to_one\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Merge failed for {name} (check duplicate SEQN).\") from e\n",
    "\n",
    "    print(\"✅ master shape:\", master.shape)\n",
    "\n",
    "    # Quick audit (non-missing coverage for key SDOH fields)\n",
    "    audit_cols = [c for c in [\"EMPLOY\",\"UNEMPLOYMENT\",\"HOD050\",\"HOQ065\",\"INS\",\"SNAP\",\"FS\"] if c in master.columns]\n",
    "    if audit_cols:\n",
    "        cov = (master[audit_cols].notna().mean()*100).round(1).sort_values(ascending=False)\n",
    "        print(\"\\nNon-missing coverage (%):\")\n",
    "        for k, v in cov.items():\n",
    "            print(f\"  {k}: {v}%\")\n",
    "    else:\n",
    "        print(\"ℹ️ No audit columns present among EMPLOY/UNEMPLOYMENT/HOD050/HOQ065/INS/SNAP/FS.\")\n",
    "\n",
    "    # Save\n",
    "    base_name = \"nhanes_mort_demo_sdoh_1999_2018\"\n",
    "    master.to_parquet(OUT / f\"{base_name}.parquet\", index=False)\n",
    "    master.to_csv(    OUT / f\"{base_name}.csv\",     index=False)\n",
    "    print(\"\\nSaved:\\n \", OUT / f\"{base_name}.parquet\", \"\\n \", OUT / f\"{base_name}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7326a1f1-6b6a-4391-aace-6d2c8fcf670e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep a clear in-memory handle for downstream cells\n",
    "nhanes_mort_demo_sdoh = master.copy()\n",
    "\n",
    "# TEMP compatibility alias for older notebooks\n",
    "nhanes_mort_demo_soc_9918 = nhanes_mort_demo_sdoh\n",
    "df = nhanes_mort_demo_sdoh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b1712ce8-a21b-43e4-ac3e-a2236e85d935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 56,253 | Unique SEQN: 56,253 | Missing SEQN: 0 | Duplicates: 0\n",
      "SEQN range: 2 → 102956\n",
      "Cycles:\n",
      " SDDSRVYR\n",
      "1.0     4973\n",
      "2.0     5586\n",
      "3.0     5293\n",
      "4.0     5332\n",
      "5.0     5989\n",
      "6.0     6346\n",
      "7.0     5603\n",
      "8.0     5913\n",
      "9.0     5720\n",
      "10.0    5498\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Use in-memory object if available; else load from disk\n",
    "try:\n",
    "    df = nhanes_mort_demo_soc_9918\n",
    "except NameError:\n",
    "    ROOT = Path(\"/Users/dengshuyue/Desktop/SDOH/analysis\")\n",
    "    candidates = [\n",
    "        ROOT / \"analysis\" / \"output\" / \"nhanes_mort_demo_soc_1999_2018.parquet\",\n",
    "        ROOT / \"analysis\" / \"output\" / \"mort_with_demo_plus_soc.parquet\",\n",
    "    ]\n",
    "    for p in candidates:\n",
    "        if p.exists():\n",
    "            df = pd.read_parquet(p)\n",
    "            print(f\"Loaded from: {p}\")\n",
    "            break\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Couldn’t find the saved table in expected locations.\")\n",
    "\n",
    "# Ensure SEQN exists and is numeric\n",
    "if \"SEQN\" not in df.columns:\n",
    "    raise KeyError(\"SEQN column not found.\")\n",
    "s = pd.to_numeric(df[\"SEQN\"], errors=\"coerce\")\n",
    "\n",
    "print(f\"Rows: {len(df):,} | Unique SEQN: {s.nunique(dropna=True):,} | \"\n",
    "      f\"Missing SEQN: {s.isna().sum():,} | Duplicates: {df.duplicated('SEQN').sum():,}\")\n",
    "print(f\"SEQN range: {int(s.min())} → {int(s.max())}\")\n",
    "\n",
    "# Optional: cycles present (if available)\n",
    "if \"SDDSRVYR\" in df.columns:\n",
    "    print(\"Cycles:\\n\", df[\"SDDSRVYR\"].value_counts(dropna=False).sort_index())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c78a15-4795-4edc-8809-7bb045eea1c8",
   "metadata": {},
   "source": [
    "<h3> Optional Check for old files and moved to old </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "601af0d1-57a1-44af-8504-946fcc80752f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in: /Users/dengshuyue/Desktop/SDOH/analysis/output\n",
      "  ✓ nhanes_mort_demo_sdoh_1999_2018.parquet  | modified: 2025-09-09 13:20:01 | size: 774,526 bytes\n",
      "  ✓ nhanes_mort_demo_sdoh_1999_2018.csv      | modified: 2025-09-09 13:20:01 | size: 4,562,121 bytes\n",
      "  - nhanes_mort_demo_soc_1999_2018.parquet   (missing)\n",
      "  - nhanes_mort_demo_soc_1999_2018.csv       (missing)\n",
      "\n",
      "Loaded: /Users/dengshuyue/Desktop/SDOH/analysis/output/nhanes_mort_demo_sdoh_1999_2018.parquet\n",
      "Shape: (56253, 20)\n",
      "Cols (first 10): ['SEQN', 'ELIGSTAT', 'MORTSTAT', 'PERMTH_EXM', 'PERMTH_INT', 'UCOD_LEADING', 'DIABETES', 'HYPERTEN', 'TIME_Y', 'EVENT']\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Correct output folder (no double \"analysis\")\n",
    "out = Path(\"/Users/dengshuyue/Desktop/SDOH/analysis/output\")\n",
    "\n",
    "# Try new names first, then legacy\n",
    "candidates = [\n",
    "    out / \"nhanes_mort_demo_sdoh_1999_2018.parquet\",\n",
    "    out / \"nhanes_mort_demo_sdoh_1999_2018.csv\",\n",
    "    out / \"nhanes_mort_demo_soc_1999_2018.parquet\",  # legacy\n",
    "    out / \"nhanes_mort_demo_soc_1999_2018.csv\",      # legacy\n",
    "]\n",
    "\n",
    "# Show which of the candidates exist, with timestamps and sizes\n",
    "print(\"Looking in:\", out)\n",
    "found_any = False\n",
    "for p in candidates:\n",
    "    if p.exists():\n",
    "        ts = datetime.fromtimestamp(p.stat().st_mtime).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        print(f\"  ✓ {p.name:40s} | modified: {ts} | size: {p.stat().st_size:,} bytes\")\n",
    "        found_any = True\n",
    "    else:\n",
    "        print(f\"  - {p.name:40s} (missing)\")\n",
    "\n",
    "# Load the first existing file (prefer Parquet)\n",
    "df = None\n",
    "for p in candidates:\n",
    "    if p.exists():\n",
    "        df = pd.read_parquet(p) if p.suffix == \".parquet\" else pd.read_csv(p)\n",
    "        print(\"\\nLoaded:\", p)\n",
    "        break\n",
    "\n",
    "if df is None:\n",
    "    print(\"\\n⚠️ No saved table found in expected locations. \"\n",
    "          \"Re-run the merge/save cell in 00_demo_mort_sdoh.ipynb.\")\n",
    "else:\n",
    "    print(\"Shape:\", df.shape)\n",
    "    print(\"Cols (first 10):\", df.columns[:10].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "245bb8a9-44b2-4e51-8aff-573fb33ad195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Keeping (newest per extension):\n",
      "  ✓ /Users/dengshuyue/Desktop/SDOH/analysis/data/SODH_diet_mort.pkl  |  2025-07-18 10:39:20  |  27 MB\n",
      "  ✓ /Users/dengshuyue/Desktop/SDOH/analysis/data/SODH_diet_mort_depr2.csv  |  2025-09-08 16:16:55  |  21 MB\n",
      "  ↳ total size kept: 48 MB\n",
      "\n",
      "🗑️ Candidates to delete (older versions):\n",
      "  - /Users/dengshuyue/Desktop/SDOH/analysis/data/SODH_diet_mort6.csv  |  2025-08-10 11:39:07  |  14 MB\n",
      "  ↳ total size to delete: 14 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "ROOT = Path(\"/Users/dengshuyue/Desktop/SDOH/analysis\")\n",
    "OUT  = ROOT / \"analysis\" / \"output\"\n",
    "DATA = ROOT / \"data\"\n",
    "\n",
    "patterns = {\n",
    "    OUT:  [\"mort_with_demo_plus_soc.*\",\n",
    "           \"nhanes_mort_demo_soc_*.parquet\",\n",
    "           \"nhanes_mort_demo_soc_*.csv\"],\n",
    "    DATA: [\"SODH_diet_mort*\"],\n",
    "}\n",
    "\n",
    "def keep_newest_per_ext(paths):\n",
    "    by_ext = {}\n",
    "    for p in paths:\n",
    "        by_ext.setdefault(p.suffix.lower(), []).append(p)\n",
    "    keep = set()\n",
    "    for ext, files in by_ext.items():\n",
    "        keep.add(max(files, key=lambda x: x.stat().st_mtime))\n",
    "    return keep\n",
    "\n",
    "def fmt_size(num_bytes: int) -> str:\n",
    "    for unit in (\"B\",\"KB\",\"MB\",\"GB\",\"TB\"):\n",
    "        if num_bytes < 1024 or unit == \"TB\":\n",
    "            return f\"{num_bytes:,.0f} {unit}\"\n",
    "        num_bytes /= 1024\n",
    "\n",
    "# gather matches\n",
    "all_matches = []\n",
    "for base, globs in patterns.items():\n",
    "    for pat in globs:\n",
    "        all_matches.extend(sorted(base.glob(pat)))\n",
    "\n",
    "# decide which to keep/delete\n",
    "TO_KEEP = keep_newest_per_ext(all_matches)\n",
    "TO_DELETE = [p for p in all_matches if p not in TO_KEEP]\n",
    "\n",
    "print(\"✅ Keeping (newest per extension):\")\n",
    "keep_total = 0\n",
    "for p in sorted(TO_KEEP):\n",
    "    stat = p.stat()\n",
    "    ts = datetime.fromtimestamp(stat.st_mtime).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    sz = fmt_size(stat.st_size); keep_total += stat.st_size\n",
    "    print(f\"  ✓ {p}  |  {ts}  |  {sz}\")\n",
    "print(f\"  ↳ total size kept: {fmt_size(keep_total)}\")\n",
    "\n",
    "print(\"\\n🗑️ Candidates to delete (older versions):\")\n",
    "del_total = 0\n",
    "for p in sorted(TO_DELETE):\n",
    "    stat = p.stat()\n",
    "    ts = datetime.fromtimestamp(stat.st_mtime).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    sz = fmt_size(stat.st_size); del_total += stat.st_size\n",
    "    print(f\"  - {p}  |  {ts}  |  {sz}\")\n",
    "print(f\"  ↳ total size to delete: {fmt_size(del_total)}\")\n",
    "\n",
    "(len(TO_KEEP), len(TO_DELETE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fe424ecf-2cda-43aa-a185-ffa5b259ee5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nothing to move.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "\n",
    "ROOT = Path(\"/Users/dengshuyue/Desktop/SDOH/analysis\")\n",
    "DATA = ROOT / \"data\"\n",
    "OLD  = DATA / \"old\"\n",
    "OLD.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1) Find the SODH diet CSVs under data/\n",
    "cands = sorted(DATA.glob(\"SODH_diet_mort*.csv\"))\n",
    "\n",
    "# 2) Exclude the two you want to keep\n",
    "exclude_stems = {\"SODH_diet_mort_depr2\", \"SODH_diet_mort6\"}\n",
    "to_move = [p for p in cands if p.stem not in exclude_stems]\n",
    "\n",
    "# --- Preview ---\n",
    "def fmt_size(n):\n",
    "    for u in (\"B\",\"KB\",\"MB\",\"GB\",\"TB\"):\n",
    "        if n < 1024: return f\"{n:,.0f} {u}\"\n",
    "        n /= 1024\n",
    "\n",
    "if not to_move:\n",
    "    print(\"Nothing to move.\")\n",
    "else:\n",
    "    total = 0\n",
    "    print(\"Will move to /data/old:\")\n",
    "    for p in to_move:\n",
    "        st = p.stat()\n",
    "        total += st.st_size\n",
    "        ts = datetime.fromtimestamp(st.st_mtime).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        print(f\"  • {p.name}  |  {fmt_size(st.st_size)}  |  {ts}\")\n",
    "    print(f\"Total: {fmt_size(total)}\")\n",
    "\n",
    "# --- Move (flip to True to execute) ---\n",
    "CONFIRM_MOVE = False  # <-- set True to actually move\n",
    "\n",
    "if CONFIRM_MOVE and to_move:\n",
    "    for p in to_move:\n",
    "        target = OLD / p.name\n",
    "        if target.exists():  # avoid overwrite if a same-named file already there\n",
    "            stamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "            target = OLD / f\"{p.stem}_{stamp}{p.suffix}\"\n",
    "        shutil.move(str(p), str(target))\n",
    "        print(f\"📦 moved: {p.name} -> {target.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "70e5c9d8-f638-4366-90ee-3656a92a0903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Now in /data/old:\n",
      " - SODH_diet_mort.csv\n",
      " - SODH_diet_mort2.csv\n",
      " - SODH_diet_mort3.csv\n",
      " - SODH_diet_mort4.csv\n",
      " - SODH_diet_mort5.csv\n",
      " - SODH_diet_mort_depr.csv\n"
     ]
    }
   ],
   "source": [
    "# Recompute to_move safely\n",
    "cands = sorted(DATA.glob(\"SODH_diet_mort*.csv\"))\n",
    "exclude_stems = {\"SODH_diet_mort_depr2\", \"SODH_diet_mort6\"}\n",
    "to_move = [p for p in cands if p.stem not in exclude_stems]\n",
    "\n",
    "# Execute move\n",
    "for p in to_move:\n",
    "    target = OLD / p.name\n",
    "    if target.exists():  # avoid overwrite\n",
    "        stamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        target = OLD / f\"{p.stem}_{stamp}{p.suffix}\"\n",
    "    shutil.move(str(p), str(target))\n",
    "    print(f\"📦 moved: {p.name} -> {target}\")\n",
    "\n",
    "# Quick verify\n",
    "print(\"\\nNow in /data/old:\")\n",
    "for p in sorted(OLD.glob(\"SODH_diet_mort*.csv\")):\n",
    "    print(\" -\", p.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000354ec-72a5-4cc2-af26-0b39f60ca7f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eafba0-9b2a-4717-8567-39b39106ecd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0586e5-1589-4a78-b8a4-831ef52a5134",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb01e1e-0a5c-4d06-a00f-9a826a8b8560",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477948b0-0cd8-410d-9ece-8b0353e78ca2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6028ba-d85d-4795-ad13-5252b8432f36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110c3011-3051-435e-9318-9e23a7cc943d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
