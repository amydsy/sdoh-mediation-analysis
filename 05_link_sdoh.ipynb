{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "719a46de",
   "metadata": {},
   "source": [
    "# 05 — Link SDOH (DEMO) + OCQ/HOQ/HIQ/FSQ with robust I/O (1999–2023)\n",
    "\n",
    "**Patches included**\n",
    "- Robust XPT reading with encoding fallbacks.\n",
    "- Use `pd.Int64Dtype()` for nullable integers instead of `'Int64'` strings.\n",
    "- Race/ethnicity combined as an aligned `Series`.\n",
    "- Adult filter uses safe numeric casting for `RIDAGEYR`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbe23004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: /Users/dengshuyue/Desktop/SDOH/analysis\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "ROOT = Path(\"/Users/dengshuyue/Desktop/SDOH/analysis\")\n",
    "DATA = ROOT / \"data\"\n",
    "OUT  = ROOT / \"output\"\n",
    "MOD  = DATA / \"nhanes_by_module\"\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "print(\"ROOT:\", ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cf0656c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_any(p: Path) -> pd.DataFrame:\n",
    "    s = p.suffix.lower()\n",
    "    if s == \".xpt\":\n",
    "        import pyreadstat\n",
    "        # Try default first, then latin-1 fallback (handles DEMO_L.xpt issue)\n",
    "        try:\n",
    "            df, _ = pyreadstat.read_xport(str(p))\n",
    "        except Exception:\n",
    "            df, _ = pyreadstat.read_xport(str(p), encoding=\"latin1\")\n",
    "    elif s == \".sas7bdat\":\n",
    "        df = pd.read_sas(str(p), format=\"sas7bdat\", encoding=\"latin1\")\n",
    "    elif s == \".csv\":\n",
    "        # Try utf-8, then latin-1\n",
    "        try:\n",
    "            df = pd.read_csv(p)\n",
    "        except UnicodeDecodeError:\n",
    "            df = pd.read_csv(p, encoding=\"latin1\")\n",
    "    elif s == \".parquet\":\n",
    "        df = pd.read_parquet(p)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file: {p}\")\n",
    "    df.columns = df.columns.str.upper()\n",
    "    return df\n",
    "\n",
    "def ensure_seqn(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    d = df.copy()\n",
    "    d.columns = d.columns.str.upper()\n",
    "    d[\"SEQN\"] = pd.to_numeric(d[\"SEQN\"], errors=\"coerce\").astype(pd.Int64Dtype())\n",
    "    return d.dropna(subset=[\"SEQN\"]).drop_duplicates(subset=[\"SEQN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2d011c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base file: /Users/dengshuyue/Desktop/SDOH/analysis/output/cov_core_mort_dep_all_1999_2023.parquet\n",
      "Base shape: (128809, 56)\n"
     ]
    }
   ],
   "source": [
    "BASE = OUT / \"cov_core_mort_dep_all_1999_2023.parquet\"\n",
    "if not BASE.exists():\n",
    "    for cand in [OUT/\"cov_core_mort_dep_1999_2023.parquet\", OUT/\"demo_cov_dep_1999_2023.parquet\"]:\n",
    "        if cand.exists():\n",
    "            BASE = cand\n",
    "            break\n",
    "print(\"Base file:\", BASE)\n",
    "base = pd.read_parquet(BASE)\n",
    "base.columns = base.columns.str.upper()\n",
    "base[\"SEQN\"] = pd.to_numeric(base[\"SEQN\"], errors=\"coerce\").astype(pd.Int64Dtype())\n",
    "print(\"Base shape:\", base.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff1f2dd",
   "metadata": {},
   "source": [
    "## DEMO across cycles → SDOH + Adult filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "401b10e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEMO shape: (128809, 13)\n",
      "SDOH tidy shape: (128809, 17)\n"
     ]
    }
   ],
   "source": [
    "DEMO_DIR = MOD / \"demo\"\n",
    "keep_cols = [\n",
    "    \"SEQN\",\"SDDSRVYR\",\"RIDAGEYR\",\"RIAGENDR\",\"WTMEC2YR\",\n",
    "    \"INDFMPIR\",\"INDFMINC\",\"DMDEDUC2\",\"DMDEDUC3\",\"DMDMARTL\",\n",
    "    \"RIDRETH1\",\"RIDRETH2\",\"RIDRETH3\",\n",
    "]\n",
    "\n",
    "cand = []\n",
    "if DEMO_DIR.exists():\n",
    "    for pat in [\"*DEMO*.xpt\",\"*DEMO*.XPT\",\"*demo*.sas7bdat\",\"*demo*.csv\",\"*demo*.parquet\"]:\n",
    "        cand.extend(sorted(DEMO_DIR.glob(pat)))\n",
    "\n",
    "parts = []\n",
    "for p in cand:\n",
    "    try:\n",
    "        d0 = read_any(p)\n",
    "        cols = [c for c in keep_cols if c in d0.columns]\n",
    "        if cols:\n",
    "            parts.append(d0[cols].copy())\n",
    "    except Exception as e:\n",
    "        print(\"Skip\", p.name, \"—\", e)\n",
    "\n",
    "demo = pd.concat(parts, ignore_index=True) if parts else pd.DataFrame(columns=keep_cols)\n",
    "demo = ensure_seqn(demo)\n",
    "print(\"DEMO shape:\", demo.shape)\n",
    "\n",
    "# Adult filter table (≥20)\n",
    "ridage = pd.to_numeric(demo.get(\"RIDAGEYR\"), errors=\"coerce\")\n",
    "age20 = demo.loc[ridage >= 20, [\"SEQN\",\"RIDAGEYR\",\"SDDSRVYR\"]].copy()\n",
    "\n",
    "# Derive SDOH harmonized\n",
    "d = demo.copy()\n",
    "d[\"PIR\"] = pd.to_numeric(d.get(\"INDFMPIR\", np.nan), errors=\"coerce\")\n",
    "d[\"PIR_CAT\"] = pd.cut(d[\"PIR\"], [-np.inf,1.3,2.0,4.0,np.inf], labels=[\"<1.30\",\"1.30–<2.00\",\"2.00–<4.00\",\"≥4.00\"]) \n",
    "d[\"EDU\"] = np.nan\n",
    "if \"DMDEDUC2\" in d: d.loc[d[\"DMDEDUC2\"].notna(), \"EDU\"] = d.loc[d[\"DMDEDUC2\"].notna(), \"DMDEDUC2\"]\n",
    "if \"DMDEDUC3\" in d: d.loc[d[\"EDU\"].isna() & d[\"DMDEDUC3\"].notna(), \"EDU\"] = d.loc[d[\"EDU\"].isna(), \"DMDEDUC3\"]\n",
    "d[\"EDU\"] = pd.to_numeric(d[\"EDU\"], errors=\"coerce\").astype(pd.Int64Dtype())\n",
    "\n",
    "edu2_map = {1:\"<9th grade\", 2:\"9–11th (incl. 12th, no diploma)\", 3:\"High school/GED\", 4:\"Some college or AA\", 5:\"College graduate or above\"}\n",
    "edu3_map = {1:\"<9th grade\", 2:\"9–11th\", 3:\"High school/GED\", 4:\"Some college or AA\", 5:\"College graduate or above\"}\n",
    "def label_edu(row):\n",
    "    v2, v3 = row.get(\"DMDEDUC2\", np.nan), row.get(\"DMDEDUC3\", np.nan)\n",
    "    if pd.notna(v2): return edu2_map.get(int(v2), np.nan)\n",
    "    if pd.notna(v3): return edu3_map.get(int(v3), np.nan)\n",
    "    return np.nan\n",
    "d[\"EDU_CAT\"] = d.apply(label_edu, axis=1)\n",
    "\n",
    "# Race/ethnicity as an aligned Series\n",
    "r3 = d.get(\"RIDRETH3\") if \"RIDRETH3\" in d else pd.Series(index=d.index, dtype=float)\n",
    "r2 = d.get(\"RIDRETH2\") if \"RIDRETH2\" in d else pd.Series(index=d.index, dtype=float)\n",
    "r1 = d.get(\"RIDRETH1\") if \"RIDRETH1\" in d else pd.Series(index=d.index, dtype=float)\n",
    "race_series = r3.where(r3.notna(), r2.where(r2.notna(), r1))\n",
    "race_series = pd.to_numeric(race_series, errors=\"coerce\").astype(pd.Int64Dtype())\n",
    "d[\"RACE_ETH_CODE\"] = race_series\n",
    "\n",
    "race_map_r3 = {1:\"Mexican American\",2:\"Other Hispanic\",3:\"NH White\",4:\"NH Black\",6:\"NH Asian\",7:\"NH Other/Multi\"}\n",
    "race_map_r2 = {1:\"Mexican American\",2:\"Other Hispanic\",3:\"NH White\",4:\"NH Black\",5:\"Other (incl. Multi)\"}\n",
    "race_map_r1 = {1:\"Mexican American\",2:\"Other Hispanic\",3:\"NH White\",4:\"NH Black\",5:\"Other (incl. Multi)\"}\n",
    "def label_race(row):\n",
    "    if pd.notna(row.get(\"RIDRETH3\", np.nan)): return race_map_r3.get(int(row[\"RIDRETH3\"]), np.nan)\n",
    "    if pd.notna(row.get(\"RIDRETH2\", np.nan)): return race_map_r2.get(int(row[\"RIDRETH2\"]), np.nan)\n",
    "    if pd.notna(row.get(\"RIDRETH1\", np.nan)): return race_map_r1.get(int(row[\"RIDRETH1\"]), np.nan)\n",
    "    return np.nan\n",
    "d[\"RACE_ETH\"] = d.apply(label_race, axis=1)\n",
    "\n",
    "d[\"MARITAL\"] = pd.to_numeric(d.get(\"DMDMARTL\", np.nan), errors=\"coerce\").astype(pd.Int64Dtype())\n",
    "mar_map = {1:\"Married\",2:\"Widowed\",3:\"Divorced\",4:\"Separated\",5:\"Never married\",6:\"Living with partner\"}\n",
    "d[\"MARITAL_CAT\"] = d[\"MARITAL\"].map(mar_map)\n",
    "\n",
    "sdoh = d[[c for c in [\n",
    "    \"SEQN\",\"SDDSRVYR\",\"INDFMPIR\",\"INDFMINC\",\"DMDEDUC2\",\"DMDEDUC3\",\"DMDMARTL\",\n",
    "    \"RIDRETH1\",\"RIDRETH2\",\"RIDRETH3\",\"PIR\",\"PIR_CAT\",\"EDU\",\"EDU_CAT\",\"RACE_ETH\",\"MARITAL\",\"MARITAL_CAT\"\n",
    "] if c in d.columns]].drop_duplicates(subset=[\"SEQN\"]).copy()\n",
    "sdoh = ensure_seqn(sdoh)\n",
    "print(\"SDOH tidy shape:\", sdoh.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573690ae",
   "metadata": {},
   "source": [
    "## Continue with OCQ/HOQ/HIQ/FSQ merge as in prior notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db2b8a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For brevity here, you can paste the OCQ/HOQ/HIQ/FSQ sections from the previous notebook.\n",
    "# They will now benefit from the robust helpers above (encoding fallbacks, Int64 dtype, adult filter)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e38023-9c8a-4746-bcc3-2cbd99eb0045",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d62eb77-3684-491a-ae70-7cf48af1a62b",
   "metadata": {},
   "source": [
    "#### OCQ — Employment status (99–18 main; 17-20, 21-23 added; adult filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "40cda9fd-586c-45ff-8d0b-01709899c2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCQ shape: (72122, 4)\n",
      "\n",
      "SDDSRVYR counts:\n",
      " 1     4880 rows\n",
      " 2     5411 rows\n",
      " 3     5041 rows\n",
      " 4     4979 rows\n",
      " 5     5935 rows\n",
      " 6     6218 rows\n",
      " 7     5560 rows\n",
      " 8     5769 rows\n",
      " 9     5719 rows\n",
      "10     5569 rows\n",
      "11     9232 rows\n",
      "12     7809 rows\n"
     ]
    }
   ],
   "source": [
    "# --- NHANES OCQ: download all cycles (with corrected folders) ----------------\n",
    "from pathlib import Path\n",
    "import requests, time, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assumes MOD, read_any, age20, ensure_seqn already exist\n",
    "OCQ_DIR = (MOD / \"ocq\"); OCQ_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "BASE = \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/{folder}/DataFiles/{fname}\"\n",
    "\n",
    "# All cycles + multiple folder candidates where needed\n",
    "OCQ_SPECS = [\n",
    "    {\"label\":\"1999-2000\", \"sddsrvyr\":1,  \"fname\":\"OCQ.xpt\",\n",
    "     \"folders\":[\"1999\"]},\n",
    "\n",
    "    {\"label\":\"2001-2002\", \"sddsrvyr\":2,  \"fname\":\"OCQ_B.xpt\",\n",
    "     \"folders\":[\"2001\"]},\n",
    "    {\"label\":\"2003-2004\", \"sddsrvyr\":3,  \"fname\":\"OCQ_C.xpt\",\n",
    "     \"folders\":[\"2003\"]},\n",
    "    {\"label\":\"2005-2006\", \"sddsrvyr\":4,  \"fname\":\"OCQ_D.xpt\",\n",
    "     \"folders\":[\"2005\"]},\n",
    "    {\"label\":\"2007-2008\", \"sddsrvyr\":5,  \"fname\":\"OCQ_E.xpt\",\n",
    "     \"folders\":[\"2007\"]},\n",
    "    {\"label\":\"2009-2010\", \"sddsrvyr\":6,  \"fname\":\"OCQ_F.xpt\",\n",
    "     \"folders\":[\"2009\"]},\n",
    "    {\"label\":\"2011-2012\", \"sddsrvyr\":7,  \"fname\":\"OCQ_G.xpt\",\n",
    "     \"folders\":[\"2011\"]},\n",
    "    {\"label\":\"2013-2014\", \"sddsrvyr\":8,  \"fname\":\"OCQ_H.xpt\",\n",
    "     \"folders\":[\"2013\"]},\n",
    "    {\"label\":\"2015-2016\", \"sddsrvyr\":9,  \"fname\":\"OCQ_I.xpt\",\n",
    "     \"folders\":[\"2015\"]},\n",
    "    {\"label\":\"2017-2018\", \"sddsrvyr\":10, \"fname\":\"OCQ_J.xpt\",\n",
    "     \"folders\":[\"2017\", \"2017-2018\"]},\n",
    "\n",
    "    # Pre-pandemic combined release (your corrected path lives under 2017/)\n",
    "    {\"label\":\"2017–Mar 2020 (pre-pandemic)\", \"sddsrvyr\":11, \"fname\":\"P_OCQ.xpt\",\n",
    "     \"folders\":[\"2017\", \"2019\", \"2019-2020\", \"2017-2018\"]},\n",
    "\n",
    "    # 2021–2023 combined (your corrected path lives under 2021/)\n",
    "    {\"label\":\"Aug 2021–Aug 2023\", \"sddsrvyr\":12, \"fname\":\"OCQ_L.xpt\",\n",
    "     \"folders\":[\"2021\", \"2021-2022\", \"2021-2023\"]},\n",
    "    # If/when CDC posts 2023–2024 (M):\n",
    "    # {\"label\":\"2023-2024\", \"sddsrvyr\":13, \"fname\":\"OCQ_M.xpt\", \"folders\":[\"2023\",\"2023-2024\"]},\n",
    "]\n",
    "\n",
    "def _try_download(url: str, dest: Path, retries=3, backoff=1.6) -> bool:\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            with requests.get(url, stream=True, timeout=30) as r:\n",
    "                if r.status_code == 200:\n",
    "                    with open(dest, \"wb\") as f:\n",
    "                        for chunk in r.iter_content(1<<15):\n",
    "                            if chunk: f.write(chunk)\n",
    "                    return True\n",
    "                if r.status_code == 404:\n",
    "                    return False\n",
    "        except requests.RequestException:\n",
    "            pass\n",
    "        time.sleep(backoff**i)\n",
    "    return False\n",
    "\n",
    "def download_ocq_all(specs=OCQ_SPECS) -> list[tuple[Path,int]]:\n",
    "    paths = []\n",
    "    for s in specs:\n",
    "        fname_candidates = [s[\"fname\"]]\n",
    "        # Case variants (.xpt/.XPT)\n",
    "        if s[\"fname\"].endswith(\".xpt\"):\n",
    "            fname_candidates.append(s[\"fname\"][:-4] + \".XPT\")\n",
    "        else:\n",
    "            fname_candidates.append(s[\"fname\"][:-4] + \".xpt\")\n",
    "\n",
    "        got = False\n",
    "        for folder in s[\"folders\"]:\n",
    "            for fname in fname_candidates:\n",
    "                url = BASE.format(folder=folder, fname=fname)\n",
    "                local = OCQ_DIR / f\"{folder}_{fname}\"\n",
    "                if local.exists() and local.stat().st_size > 0:\n",
    "                #    print(f\"✓ Exists: {s['label']} @ {folder}/{fname}\")\n",
    "                    paths.append((local, s[\"sddsrvyr\"]))\n",
    "                    got = True\n",
    "                    break\n",
    "                print(f\"→ Fetch {s['label']}: {url}\")\n",
    "                ok = _try_download(url, local)\n",
    "                if ok:\n",
    "                #    print(f\"  ✓ Saved {local.name}\")\n",
    "                    paths.append((local, s[\"sddsrvyr\"]))\n",
    "                    got = True\n",
    "                    break\n",
    "            if got: break\n",
    "        if not got:\n",
    "            print(f\"⚠ Missing: {s['label']} (tried {s['folders']})\")\n",
    "    return paths\n",
    "\n",
    "# Download\n",
    "ocq_files = download_ocq_all()\n",
    "\n",
    "# Build\n",
    "def _infer_cycle_if_missing(df: pd.DataFrame, sddsrvyr: int) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    if \"SDDSRVYR\" not in df.columns or df[\"SDDSRVYR\"].isna().all():\n",
    "        df[\"SDDSRVYR\"] = sddsrvyr\n",
    "    else:\n",
    "        df[\"SDDSRVYR\"] = pd.to_numeric(df[\"SDDSRVYR\"], errors=\"coerce\")\n",
    "        df[\"SDDSRVYR\"] = df[\"SDDSRVYR\"].fillna(sddsrvyr)\n",
    "    return df\n",
    "\n",
    "def _recode_employment(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"EMPLOY\"] = np.nan\n",
    "    if \"OCD150\" in df.columns:\n",
    "        df.loc[df[\"OCD150\"] == 1, \"EMPLOY\"] = 1  # working\n",
    "        df.loc[df[\"OCD150\"] == 3, \"EMPLOY\"] = 2  # not working\n",
    "    if \"OCQ380\" in df.columns:\n",
    "        df.loc[df[\"OCQ380\"] == 5, \"EMPLOY\"] = 2  # unemployed/looking\n",
    "        df.loc[df[\"OCQ380\"] == 3, \"EMPLOY\"] = 3  # retired\n",
    "        df.loc[df[\"OCQ380\"].isin([4, 6]), \"EMPLOY\"] = 4  # disabled/other\n",
    "        df.loc[df[\"OCQ380\"].isin([1, 2, 7]), \"EMPLOY\"] = 5  # working/unknown\n",
    "    df[\"UNEMPLOYMENT\"] = (df[\"EMPLOY\"] == 2).astype(pd.Int64Dtype())\n",
    "    keep = [c for c in [\"SEQN\", \"SDDSRVYR\", \"EMPLOY\", \"UNEMPLOYMENT\"] if c in df.columns]\n",
    "    return df[keep]\n",
    "\n",
    "ocq_parts = []\n",
    "for p, cyc in ocq_files:\n",
    "    try:\n",
    "        df = read_any(p)\n",
    "        df = _infer_cycle_if_missing(df, cyc)\n",
    "        df = df.merge(age20[[\"SEQN\"]], on=\"SEQN\", how=\"inner\")  # adults (≥20)\n",
    "        ocq_parts.append(_recode_employment(df))\n",
    "    except Exception as e:\n",
    "        print(\"OCQ skip\", p.name, \"—\", e)\n",
    "\n",
    "ocq = (pd.concat(ocq_parts, ignore_index=True)\n",
    "        if ocq_parts else\n",
    "        pd.DataFrame(columns=[\"SEQN\",\"SDDSRVYR\",\"EMPLOY\",\"UNEMPLOYMENT\"]))\n",
    "ocq = ensure_seqn(ocq)\n",
    "\n",
    "print(\"OCQ shape:\", ocq.shape)\n",
    "\n",
    "# Optional: avoid 2017–2018 duplication if both J (10) and P (11) are present\n",
    "s10 = set(ocq.loc[ocq[\"SDDSRVYR\"]==10, \"SEQN\"])\n",
    "s11 = set(ocq.loc[ocq[\"SDDSRVYR\"]==11, \"SEQN\"])\n",
    "overlap = s10 & s11\n",
    "if overlap:\n",
    "    print(\"De-overlapping 2017–2018 vs pre-pandemic:\", len(overlap))\n",
    "    ocq = ocq[~((ocq[\"SEQN\"].isin(overlap)) & (ocq[\"SDDSRVYR\"]==10))].copy()\n",
    "\n",
    "# Audit\n",
    "vc = ocq[\"SDDSRVYR\"].value_counts(dropna=False).sort_index()\n",
    "print(\"\\nSDDSRVYR counts:\")\n",
    "for k, n in vc.items():\n",
    "    print(f\"{int(k):>2}  {int(n):>7} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d561c5c9-19d1-4158-96dd-cf612252e52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlap SEQNs (10 vs 11): 0\n"
     ]
    }
   ],
   "source": [
    "s10 = set(ocq.loc[ocq[\"SDDSRVYR\"]==10, \"SEQN\"])\n",
    "s11 = set(ocq.loc[ocq[\"SDDSRVYR\"]==11, \"SEQN\"])\n",
    "overlap = s10 & s11\n",
    "print(\"Overlap SEQNs (10 vs 11):\", len(overlap))\n",
    "\n",
    "# Prefer pre-pandemic records (11); drop cycle 10 for overlapping SEQNs\n",
    "if overlap:\n",
    "    ocq = ocq[~((ocq[\"SEQN\"].isin(overlap)) & (ocq[\"SDDSRVYR\"]==10))].copy()\n",
    "\n",
    "# Recheck counts\n",
    "# print(ocq[\"SDDSRVYR\"].value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "09cb866e-793f-4bc7-b2ad-b27191f94e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /Users/dengshuyue/Desktop/SDOH/analysis/output/ocq_1999_2023.parquet\n"
     ]
    }
   ],
   "source": [
    "# Sanity: no overlap expected (and you saw 0)\n",
    "s10 = set(ocq.loc[ocq[\"SDDSRVYR\"]==10, \"SEQN\"])\n",
    "s11 = set(ocq.loc[ocq[\"SDDSRVYR\"]==11, \"SEQN\"])\n",
    "assert len(s10 & s11) == 0, \"Found overlap between 10 and 11!\"\n",
    "\n",
    "# Tidy dtypes & save\n",
    "ocq[\"SDDSRVYR\"] = ocq[\"SDDSRVYR\"].astype(\"Int8\")\n",
    "ocq[\"UNEMPLOYMENT\"] = ocq[\"UNEMPLOYMENT\"].astype(\"Int8\")\n",
    "out_ocq = OUT / \"ocq_1999_2023.parquet\"\n",
    "ocq.to_parquet(out_ocq, index=False)\n",
    "print(\"Wrote:\", out_ocq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65319c8e-0bfc-4e9a-8d4e-c1d4a5580174",
   "metadata": {},
   "source": [
    "#### HOQ — Housing (subset) (99–18 main; 17-20, 21-23 added; adult filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0072996d-f3d8-4253-b359-df1ed79eb454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Fetch 1999-2000: https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/1999/DataFiles/HOQ.xpt\n",
      "  ✓ Saved 1999_HOQ.xpt\n",
      "→ Fetch 2001-2002: https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2001/DataFiles/HOQ_B.xpt\n",
      "  ✓ Saved 2001_HOQ_B.xpt\n",
      "→ Fetch 2003-2004: https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2003/DataFiles/HOQ_C.xpt\n",
      "  ✓ Saved 2003_HOQ_C.xpt\n",
      "→ Fetch 2005-2006: https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2005/DataFiles/HOQ_D.xpt\n",
      "  ✓ Saved 2005_HOQ_D.xpt\n",
      "→ Fetch 2007-2008: https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2007/DataFiles/HOQ_E.xpt\n",
      "  ✓ Saved 2007_HOQ_E.xpt\n",
      "→ Fetch 2009-2010: https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2009/DataFiles/HOQ_F.xpt\n",
      "  ✓ Saved 2009_HOQ_F.xpt\n",
      "→ Fetch 2011-2012: https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2011/DataFiles/HOQ_G.xpt\n",
      "  ✓ Saved 2011_HOQ_G.xpt\n",
      "→ Fetch 2013-2014: https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2013/DataFiles/HOQ_H.xpt\n",
      "  ✓ Saved 2013_HOQ_H.xpt\n",
      "→ Fetch 2015-2016: https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2015/DataFiles/HOQ_I.xpt\n",
      "  ✓ Saved 2015_HOQ_I.xpt\n",
      "→ Fetch 2017-2018: https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2017/DataFiles/HOQ_J.xpt\n",
      "  ✓ Saved 2017_HOQ_J.xpt\n",
      "→ Fetch 2017–Mar 2020 (pre-pandemic): https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2017/DataFiles/P_HOQ.xpt\n",
      "  ✗ Skip 2017–Mar 2020 (pre-pandemic) — 404 Client Error: Not Found for url: https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2017/DataFiles/P_HOQ.xpt\n",
      "→ Fetch Aug 2021–Aug 2023: https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2021/DataFiles/HOQ_L.xpt\n",
      "  ✓ Saved 2021_HOQ_L.xpt\n",
      "HOQ shape: (62890, 4)\n",
      "\n",
      "SDDSRVYR counts:\n",
      "SDDSRVYR\n",
      "1     4880\n",
      "2     5411\n",
      "3     5041\n",
      "4     4979\n",
      "5     5935\n",
      "6     6218\n",
      "7     5560\n",
      "8     5769\n",
      "9     5719\n",
      "10    5569\n",
      "12    7809\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Overlap SEQNs (10 vs 11): 0\n",
      "Wrote: /Users/dengshuyue/Desktop/SDOH/analysis/output/hoq_1999_2023.parquet\n"
     ]
    }
   ],
   "source": [
    "# --- HOQ (Housing): fetch 1999–2023 from CDC, keep HOD050(+HOD051) + HOQ065 (7/9 -> NA) ----\n",
    "from pathlib import Path\n",
    "import io, sys, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "BASE = \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public\"\n",
    "HOQ_DIR = (MOD / \"hoq\")\n",
    "HOQ_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Cycle specs (folder + filename on CDC + SDDSRVYR code)\n",
    "_HOQ_SPECS = [\n",
    "    # classic 2-year cycles\n",
    "    (1,  \"1999-2000\", \"1999\", \"HOQ.xpt\"),\n",
    "    (2,  \"2001-2002\", \"2001\", \"HOQ_B.xpt\"),\n",
    "    (3,  \"2003-2004\", \"2003\", \"HOQ_C.xpt\"),\n",
    "    (4,  \"2005-2006\", \"2005\", \"HOQ_D.xpt\"),\n",
    "    (5,  \"2007-2008\", \"2007\", \"HOQ_E.xpt\"),\n",
    "    (6,  \"2009-2010\", \"2009\", \"HOQ_F.xpt\"),\n",
    "    (7,  \"2011-2012\", \"2011\", \"HOQ_G.xpt\"),\n",
    "    (8,  \"2013-2014\", \"2013\", \"HOQ_H.xpt\"),\n",
    "    (9,  \"2015-2016\", \"2015\", \"HOQ_I.xpt\"),\n",
    "    (10, \"2017-2018\", \"2017\", \"HOQ_J.xpt\"),\n",
    "    # special combined (pre-pandemic) + post-pandemic combined\n",
    "    (11, \"2017–Mar 2020 (pre-pandemic)\", \"2017\", \"P_HOQ.xpt\"),\n",
    "    (12, \"Aug 2021–Aug 2023\", \"2021\", \"HOQ_L.xpt\"),\n",
    "]\n",
    "\n",
    "def _fetch_hoq_file(sddsrvyr: int, label: str, folder: str, fname: str) -> Path | None:\n",
    "    \"\"\"\n",
    "    Download an HOQ .xpt file to HOQ_DIR, return local path or None if failed.\n",
    "    \"\"\"\n",
    "    url = f\"{BASE}/{folder}/DataFiles/{fname}\"\n",
    "    # nice local name (keeps year folder + filename)\n",
    "    out = HOQ_DIR / f\"{folder}_{fname}\"\n",
    "    print(f\"→ Fetch {label}: {url}\")\n",
    "    try:\n",
    "        if not out.exists() or out.stat().st_size == 0:\n",
    "            r = requests.get(url, timeout=60)\n",
    "            r.raise_for_status()\n",
    "            out.write_bytes(r.content)\n",
    "            print(f\"  ✓ Saved {out.name}\")\n",
    "        else:\n",
    "            print(f\"  • Exists {out.name}\")\n",
    "        return out\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Skip {label} — {e}\")\n",
    "        return None\n",
    "\n",
    "# Download all HOQ cycles\n",
    "_downloaded = []\n",
    "for cyc, label, folder, fname in _HOQ_SPECS:\n",
    "    p = _fetch_hoq_file(cyc, label, folder, fname)\n",
    "    if p is not None:\n",
    "        _downloaded.append((cyc, p))\n",
    "\n",
    "# Collect candidate files (downloaded + any local extras you already have)\n",
    "hoq_files: list[Path] = []\n",
    "if HOQ_DIR.exists():\n",
    "    for pat in [\"HOQ*.xpt\", \"hoq*.sas7bdat\", \"hoq*.parquet\", \"hoq*.csv\", \"*HOQ*.xpt\", \"*_HOQ*.xpt\", \"P_HOQ*.xpt\"]:\n",
    "        hoq_files.extend(sorted(set(HOQ_DIR.glob(pat))))\n",
    "# ensure downloaded ones are present (order by SDDSRVYR)\n",
    "dl_set = {p for _, p in _downloaded}\n",
    "hoq_files = [p for _, p in sorted(_downloaded, key=lambda t: t[0])] + [p for p in hoq_files if p not in dl_set]\n",
    "\n",
    "# Helper: add SDDSRVYR if missing\n",
    "def _add_cycle_if_missing(df: pd.DataFrame, cyc: int) -> pd.DataFrame:\n",
    "    if \"SDDSRVYR\" not in df.columns:\n",
    "        df = df.copy()\n",
    "        df[\"SDDSRVYR\"] = cyc\n",
    "    return df\n",
    "\n",
    "def _recode_hoq(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    - Harmonize HOD050/HOD051 to HOD050\n",
    "    - Recode HOQ065: 7/9 -> NA\n",
    "    - Keep SEQN, SDDSRVYR, HOD050, HOQ065\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    # rooms var changed name to HOD051 in L cycle; prefer a unified HOD050\n",
    "    if \"HOD050\" not in df.columns and \"HOD051\" in df.columns:\n",
    "        df = df.rename(columns={\"HOD051\": \"HOD050\"})\n",
    "    # set invalids to NA for tenure\n",
    "    if \"HOQ065\" in df.columns:\n",
    "        df.loc[df[\"HOQ065\"].isin([7, 9]), \"HOQ065\"] = np.nan\n",
    "    keep = [c for c in [\"SEQN\", \"SDDSRVYR\", \"HOD050\", \"HOQ065\"] if c in df.columns]\n",
    "    return df[keep] if keep else pd.DataFrame(columns=[\"SEQN\",\"SDDSRVYR\",\"HOD050\",\"HOQ065\"])\n",
    "\n",
    "# Build parts\n",
    "hoq_parts = []\n",
    "for p in hoq_files:\n",
    "    try:\n",
    "        # infer SDDSRVYR from our spec if possible\n",
    "        cyc_guess = None\n",
    "        for cyc, label, folder, fname in _HOQ_SPECS:\n",
    "            if p.name.endswith(fname) and p.name.startswith(folder + \"_\"):\n",
    "                cyc_guess = cyc\n",
    "                break\n",
    "\n",
    "        df = read_any(p)  # your helper that reads XPT/SAS/Parquet/CSV\n",
    "        if cyc_guess is not None:\n",
    "            df = _add_cycle_if_missing(df, cyc_guess)\n",
    "        else:\n",
    "            df = _infer_cycle_if_missing(df, p)  # your existing suffix-based fallback\n",
    "\n",
    "        # adult filter (≥20) using DEMO-based age20 (expects a frame with SEQN)\n",
    "        df = df.merge(age20[[\"SEQN\"]], on=\"SEQN\", how=\"inner\")\n",
    "        part = _recode_hoq(df)\n",
    "        if not part.empty:\n",
    "            hoq_parts.append(part)\n",
    "    except Exception as e:\n",
    "        print(\"HOQ skip\", p.name, \"—\", e)\n",
    "\n",
    "hoq_all = pd.concat(hoq_parts, ignore_index=True) if hoq_parts else pd.DataFrame(columns=[\"SEQN\",\"SDDSRVYR\",\"HOD050\",\"HOQ065\"])\n",
    "hoq_all = ensure_seqn(hoq_all)  # your helper to clean/ensure SEQN integrity\n",
    "\n",
    "print(\"HOQ shape:\", hoq_all.shape)\n",
    "print(\"\\nSDDSRVYR counts:\")\n",
    "print(hoq_all[\"SDDSRVYR\"].value_counts().sort_index())\n",
    "\n",
    "# Optional: sanity check for possible overlap between 2017–2018 (10) and 2017–Mar 2020 (11)\n",
    "if {\"SDDSRVYR\"}.issubset(hoq_all.columns):\n",
    "    s10 = set(hoq_all.loc[hoq_all[\"SDDSRVYR\"]==10, \"SEQN\"])\n",
    "    s11 = set(hoq_all.loc[hoq_all[\"SDDSRVYR\"]==11, \"SEQN\"])\n",
    "    print(\"\\nOverlap SEQNs (10 vs 11):\", len(s10 & s11))\n",
    "\n",
    "# Optional: tidy dtypes & save\n",
    "hoq_all[\"SDDSRVYR\"] = hoq_all[\"SDDSRVYR\"].astype(\"Int8\", errors=\"ignore\")\n",
    "if \"HOQ065\" in hoq_all.columns:\n",
    "    hoq_all[\"HOQ065\"] = hoq_all[\"HOQ065\"].astype(\"Int8\", errors=\"ignore\")\n",
    "out_path = OUT / \"hoq_1999_2023.parquet\"\n",
    "hoq_all.to_parquet(out_path, index=False)\n",
    "print(\"Wrote:\", out_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542b1d99-27db-408f-ae6c-7bf3e9cbe8c7",
   "metadata": {},
   "source": [
    "#### HIQ/HIQS 99–18 main; 17-20, 21-23 added; — Health Insurance → INS (adult filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b92d0783-552c-4062-a46a-6d31d8e7d796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Exists: 1999-2000 @ 1999/HIQ.xpt\n",
      "✓ Exists: 2001-2002 @ 2001/HIQ_B.xpt\n",
      "✓ Exists: 2003-2004 @ 2003/HIQ_C.xpt\n",
      "✓ Exists: 2005-2006 @ 2005/HIQ_D.xpt\n",
      "✓ Exists: 2007-2008 @ 2007/HIQ_E.xpt\n",
      "✓ Exists: 2009-2010 @ 2009/HIQ_F.xpt\n",
      "✓ Exists: 2011-2012 @ 2011/HIQ_G.xpt\n",
      "✓ Exists: 2013-2014 @ 2013/HIQ_H.xpt\n",
      "✓ Exists: 2015-2016 @ 2015/HIQ_I.xpt\n",
      "✓ Exists: 2017-2018 @ 2017/HIQ_J.xpt\n",
      "✓ Exists: 2017–Mar 2020 (pre-pandemic) @ 2017/P_HIQ.xpt\n",
      "✓ Exists: Aug 2021–Aug 2023 @ 2021/HIQ_L.xpt\n",
      "INS shape: (72122, 3)\n",
      "\n",
      "SDDSRVYR counts (in HIQ/HIQS source):\n",
      "SDDSRVYR\n",
      "1     4880\n",
      "2     5411\n",
      "3     5041\n",
      "4     4979\n",
      "5     5935\n",
      "6     6218\n",
      "7     5560\n",
      "8     5769\n",
      "9     5719\n",
      "10    5569\n",
      "11    9232\n",
      "12    7809\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --- HIQ / HIQS (Insurance): fetch 1999–2023, build INS ----------------------\n",
    "from pathlib import Path\n",
    "import requests, time, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "HIQ_DIR = (MOD / \"hiq\")\n",
    "HIQ_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "BASE = \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/{folder}/DataFiles/{fname}\"\n",
    "\n",
    "# Cycles + folders + candidate filenames (HIQ & HIQS)\n",
    "HIQ_SPECS = [\n",
    "    # SDDSRVYR, label, folder, filename candidates (in priority order)\n",
    "    (1,  \"1999-2000\", \"1999\", [\"HIQ.xpt\", \"HIQS.xpt\"]),\n",
    "    (2,  \"2001-2002\", \"2001\", [\"HIQ_B.xpt\", \"HIQS_B.xpt\"]),\n",
    "    (3,  \"2003-2004\", \"2003\", [\"HIQ_C.xpt\", \"HIQS_C.xpt\"]),\n",
    "    (4,  \"2005-2006\", \"2005\", [\"HIQ_D.xpt\", \"HIQS_D.xpt\"]),\n",
    "    (5,  \"2007-2008\", \"2007\", [\"HIQ_E.xpt\", \"HIQS_E.xpt\"]),\n",
    "    (6,  \"2009-2010\", \"2009\", [\"HIQ_F.xpt\", \"HIQS_F.xpt\"]),\n",
    "    (7,  \"2011-2012\", \"2011\", [\"HIQ_G.xpt\", \"HIQS_G.xpt\"]),\n",
    "    (8,  \"2013-2014\", \"2013\", [\"HIQ_H.xpt\", \"HIQS_H.xpt\"]),\n",
    "    (9,  \"2015-2016\", \"2015\", [\"HIQ_I.xpt\", \"HIQS_I.xpt\"]),\n",
    "    (10, \"2017-2018\", \"2017\", [\"HIQ_J.xpt\", \"HIQS_J.xpt\"]),\n",
    "    # Pre-pandemic combined (under 2017 folder)\n",
    "    (11, \"2017–Mar 2020 (pre-pandemic)\", \"2017\", [\"P_HIQ.xpt\", \"P_HIQS.xpt\"]),\n",
    "    # 2021–2023 combined (under 2021 folder)\n",
    "    (12, \"Aug 2021–Aug 2023\", \"2021\", [\"HIQ_L.xpt\", \"HIQS_L.xpt\"]),\n",
    "]\n",
    "\n",
    "def _try_download(url: str, dest: Path, retries=3, backoff=1.6) -> bool:\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            with requests.get(url, stream=True, timeout=45) as r:\n",
    "                if r.status_code == 200:\n",
    "                    with open(dest, \"wb\") as f:\n",
    "                        for chunk in r.iter_content(1<<15):\n",
    "                            if chunk: f.write(chunk)\n",
    "                    return True\n",
    "                if r.status_code == 404:\n",
    "                    return False\n",
    "        except requests.RequestException:\n",
    "            pass\n",
    "        time.sleep(backoff**i)\n",
    "    return False\n",
    "\n",
    "def download_hiq_all(specs=HIQ_SPECS):\n",
    "    \"\"\"Return list of (local_path, sddsrvyr). Tries .xpt and .XPT, HIQ then HIQS.\"\"\"\n",
    "    paths = []\n",
    "    for cyc, label, folder, fnames in specs:\n",
    "        got = False\n",
    "        for fname in fnames:\n",
    "            # try both .xpt and .XPT\n",
    "            cand = [fname]\n",
    "            cand += [fname[:-4] + \".XPT\"] if fname.lower().endswith(\".xpt\") else [fname[:-4] + \".xpt\"]\n",
    "            for f in cand:\n",
    "                url = BASE.format(folder=folder, fname=f)\n",
    "                local = HIQ_DIR / f\"{folder}_{f}\"\n",
    "                if local.exists() and local.stat().st_size > 0:\n",
    "                    print(f\"✓ Exists: {label} @ {folder}/{f}\")\n",
    "                    paths.append((local, cyc))\n",
    "                    got = True\n",
    "                    break\n",
    "                print(f\"→ Fetch {label}: {url}\")\n",
    "                ok = _try_download(url, local)\n",
    "                if ok:\n",
    "                    print(f\"  ✓ Saved {local.name}\")\n",
    "                    paths.append((local, cyc))\n",
    "                    got = True\n",
    "                    break\n",
    "            if got:\n",
    "                break\n",
    "        if not got:\n",
    "            print(f\"⚠ Missing: {label} (tried {fnames})\")\n",
    "    return paths\n",
    "\n",
    "# 1) Download\n",
    "hiq_paths = download_hiq_all()\n",
    "\n",
    "# 2) Read + tag cycle + adult filter\n",
    "hiq_parts = []\n",
    "for p, cyc in hiq_paths:\n",
    "    try:\n",
    "        df = read_any(p)\n",
    "        df.columns = df.columns.str.upper()\n",
    "        if \"SDDSRVYR\" not in df.columns or df[\"SDDSRVYR\"].isna().all():\n",
    "            df[\"SDDSRVYR\"] = cyc\n",
    "        else:\n",
    "            df[\"SDDSRVYR\"] = pd.to_numeric(df[\"SDDSRVYR\"], errors=\"coerce\").fillna(cyc)\n",
    "        # adults (≥20)\n",
    "        df = df.merge(age20[[\"SEQN\"]], on=\"SEQN\", how=\"inner\")\n",
    "        # track source so we can dedupe HIQ vs HIQS within a cycle\n",
    "        df[\"_SRC\"] = p.name.upper()\n",
    "        hiq_parts.append(df)\n",
    "    except Exception as e:\n",
    "        print(\"HIQ skip\", p.name, \"—\", e)\n",
    "\n",
    "hiq_all = (pd.concat(hiq_parts, ignore_index=True)\n",
    "           if hiq_parts else pd.DataFrame())\n",
    "hiq_all.columns = hiq_all.columns.str.upper()\n",
    "\n",
    "# 3) Deduplicate if both HIQ & HIQS exist for same SEQN+cycle\n",
    "if not hiq_all.empty:\n",
    "    relevant = [c for c in [\n",
    "        \"HIQ031A\",\"HIQ031B\",\"HIQ031C\",\"HIQ031D\",\"HIQ031E\",\"HIQ031F\",\"HIQ031G\",\"HIQ031H\",\"HIQ031I\",\"HIQ011\",\n",
    "        \"HID030A\",\"HID030B\",\"HID030C\",\"HID030D\",\"HID010\"\n",
    "    ] if c in hiq_all.columns]\n",
    "    hiq_all[\"_NN\"] = hiq_all[relevant].notna().sum(axis=1) if relevant else 0\n",
    "    hiq_all[\"_PREF\"] = hiq_all[\"_SRC\"].str.contains(r\"\\bHIQ\", case=False, regex=True).astype(int)  # prefer HIQ over HIQS\n",
    "    hiq_all = (hiq_all\n",
    "               .sort_values([\"SEQN\",\"SDDSRVYR\",\"_PREF\",\"_NN\"], ascending=[True,True,False,False])\n",
    "               .drop_duplicates(subset=[\"SEQN\",\"SDDSRVYR\"], keep=\"first\")\n",
    "               .drop(columns=[\"_SRC\",\"_NN\",\"_PREF\"], errors=\"ignore\"))\n",
    "\n",
    "# 4) Build INS using your existing logic (safe column accessor)\n",
    "def s(df: pd.DataFrame, col: str, default_val=np.nan):\n",
    "    return df[col] if col in df.columns else pd.Series(default_val, index=df.index)\n",
    "\n",
    "ins = pd.DataFrame({\"SEQN\": s(hiq_all, \"SEQN\")})\n",
    "if \"SDDSRVYR\" in hiq_all:\n",
    "    ins[\"SDDSRVYR\"] = hiq_all[\"SDDSRVYR\"]\n",
    "ins[\"INS\"] = np.nan\n",
    "\n",
    "# Private\n",
    "cond_private = s(hiq_all,\"HIQ031A\").eq(14) | s(hiq_all,\"HID030A\").eq(1)\n",
    "ins.loc[cond_private, \"INS\"] = 1\n",
    "\n",
    "# Medicare\n",
    "cond_med = (s(hiq_all,\"HIQ031B\").eq(15) & ~s(hiq_all,\"HIQ031D\").eq(17) & ~s(hiq_all,\"HIQ031E\").eq(18)) | \\\n",
    "           (s(hiq_all,\"HID030B\").eq(1) & ~s(hiq_all,\"HID030C\").eq(1))\n",
    "ins.loc[cond_med, \"INS\"] = 2\n",
    "\n",
    "# Medicaid only or both → 3\n",
    "cond_mcaid_only = ((s(hiq_all,\"HIQ031D\").eq(17) | s(hiq_all,\"HIQ031E\").eq(18)) & ~s(hiq_all,\"HIQ031B\").eq(15)) | \\\n",
    "                  (~s(hiq_all,\"HID030B\").eq(1) & s(hiq_all,\"HID030C\").eq(1))\n",
    "cond_both = (s(hiq_all,\"HIQ031B\").eq(15) & s(hiq_all,\"HIQ031D\").eq(17)) | \\\n",
    "            (s(hiq_all,\"HID030B\").eq(1) & s(hiq_all,\"HID030C\").eq(1))\n",
    "ins.loc[cond_mcaid_only | cond_both, \"INS\"] = 3\n",
    "\n",
    "# Other\n",
    "other_cols = [c for c in [\"HIQ031C\",\"HIQ031F\",\"HIQ031G\",\"HIQ031H\",\"HIQ031I\",\"HID030D\"] if c in hiq_all.columns]\n",
    "cond_other = hiq_all[other_cols].eq(1).any(axis=1) if other_cols else pd.Series(False, index=hiq_all.index)\n",
    "ins.loc[cond_other, \"INS\"] = 5\n",
    "\n",
    "# None\n",
    "none_conds = []\n",
    "if \"HIQ011\" in hiq_all: none_conds.append(hiq_all[\"HIQ011\"].eq(2))\n",
    "if \"HID010\" in hiq_all: none_conds.append(hiq_all[\"HID010\"].eq(2))\n",
    "if none_conds:\n",
    "    from functools import reduce\n",
    "    ins.loc[reduce(lambda a,b: a|b, none_conds), \"INS\"] = 0\n",
    "\n",
    "# Finalize\n",
    "ins = ensure_seqn(ins)\n",
    "ins[\"INS\"] = pd.to_numeric(ins[\"INS\"], errors=\"coerce\").astype(pd.Int64Dtype())\n",
    "\n",
    "print(\"INS shape:\", ins.shape)\n",
    "print(\"\\nSDDSRVYR counts (in HIQ/HIQS source):\")\n",
    "print(ins[\"SDDSRVYR\"].value_counts(dropna=False).sort_index())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509b9230-00dc-4b48-93bb-8feaccc1de02",
   "metadata": {},
   "source": [
    "#### FSQ/FSQS — SNAP & FS; 99–18 main; 17-20, 21-23 added (adult filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b229a27a-8dc2-4416-a96a-1bb54e8f2318",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "28b8c379-3d89-4ec5-91fa-9dc0d11bde10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Exists: 1999-2000 @ 1999/FSQ.xpt\n",
      "✓ Exists: 2001-2002 @ 2001/FSQ_B.xpt\n",
      "✓ Exists: 2003-2004 @ 2003/FSQ_C.xpt\n",
      "✓ Exists: 2005-2006 @ 2005/FSQ_D.xpt\n",
      "✓ Exists: 2007-2008 @ 2007/FSQ_E.xpt\n",
      "✓ Exists: 2009-2010 @ 2009/FSQ_F.xpt\n",
      "✓ Exists: 2011-2012 @ 2011/FSQ_G.xpt\n",
      "✓ Exists: 2013-2014 @ 2013/FSQ_H.xpt\n",
      "✓ Exists: 2015-2016 @ 2015/FSQ_I.xpt\n",
      "✓ Exists: 2017-2018 @ 2017/FSQ_J.xpt\n",
      "✓ Exists: 2017–Mar 2020 (pre-pandemic) @ 2017/P_FSQ.xpt\n",
      "SNAP/FS shape: (64313, 13)\n",
      "By cycle (rows):\n",
      "SDDSRVYR\n",
      "1     4880\n",
      "2     5411\n",
      "3     5041\n",
      "4     4979\n",
      "5     5935\n",
      "6     6218\n",
      "7     5560\n",
      "8     5769\n",
      "9     5719\n",
      "10    5569\n",
      "11    9232\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Coverage (share non-missing):\n",
      "          FS_HH_pct  FS_ADULT_pct  FS_FINAL_pct  SNAP_pct\n",
      "SDDSRVYR                                                 \n",
      "1             0.125         0.115         0.125     1.000\n",
      "2             0.132         0.124         0.132     1.000\n",
      "3             0.133          <NA>         0.133     0.127\n",
      "4             0.139          <NA>         0.139     0.111\n",
      "5             0.160          <NA>         0.160     0.671\n",
      "6             0.203          <NA>         0.203     0.787\n",
      "7             0.202          <NA>         0.202     0.790\n",
      "8             0.193          <NA>         0.193     0.744\n",
      "9             0.248          <NA>         0.248     0.608\n",
      "10            0.234          <NA>         0.234     0.578\n",
      "11            0.233          <NA>         0.233     0.567\n",
      "\n",
      "Sources used for household FS by cycle:\n",
      "FS_SOURCE_HH  FSDHH  HHFDSEC\n",
      "SDDSRVYR                    \n",
      "1                 0     4745\n",
      "2                 0     5053\n",
      "3              4820        0\n",
      "4              4924        0\n",
      "5              5880        0\n",
      "6              6146        0\n",
      "7              5533        0\n",
      "8              5692        0\n",
      "9              5516        0\n",
      "10             5244        0\n",
      "11             8543        0\n"
     ]
    }
   ],
   "source": [
    "# --- FSQ/FSQS (SNAP & Food Security): fetch & build with HH + Adult (1999–2020) ---\n",
    "from pathlib import Path\n",
    "import requests, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "FSQ_DIR = (MOD / \"fsq\")\n",
    "FSQ_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "BASE = \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/{folder}/DataFiles/{fname}\"\n",
    "\n",
    "# Cycles: 1..10 (1999–2018), 11 = 2017–Mar 2020 (pre-pandemic).\n",
    "# FYI: FSQ_L (2021–2023) not posted publicly → will be skipped.\n",
    "FSQ_SPECS = [\n",
    "    (1,  \"1999-2000\", \"1999\", [\"FSQ.xpt\",   \"FSQS.xpt\"]),\n",
    "    (2,  \"2001-2002\", \"2001\", [\"FSQ_B.xpt\", \"FSQS_B.xpt\"]),\n",
    "    (3,  \"2003-2004\", \"2003\", [\"FSQ_C.xpt\", \"FSQS_C.xpt\"]),\n",
    "    (4,  \"2005-2006\", \"2005\", [\"FSQ_D.xpt\", \"FSQS_D.xpt\"]),\n",
    "    (5,  \"2007-2008\", \"2007\", [\"FSQ_E.xpt\", \"FSQS_E.xpt\"]),\n",
    "    (6,  \"2009-2010\", \"2009\", [\"FSQ_F.xpt\", \"FSQS_F.xpt\"]),\n",
    "    (7,  \"2011-2012\", \"2011\", [\"FSQ_G.xpt\", \"FSQS_G.xpt\"]),\n",
    "    (8,  \"2013-2014\", \"2013\", [\"FSQ_H.xpt\", \"FSQS_H.xpt\"]),\n",
    "    (9,  \"2015-2016\", \"2015\", [\"FSQ_I.xpt\", \"FSQS_I.xpt\"]),\n",
    "    (10, \"2017-2018\", \"2017\", [\"FSQ_J.xpt\", \"FSQS_J.xpt\"]),\n",
    "    (11, \"2017–Mar 2020 (pre-pandemic)\", \"2017\", [\"P_FSQ.xpt\", \"P_FSQS.xpt\"]),\n",
    "    # (12, \"Aug 2021–Aug 2023\", \"2021\", [\"FSQ_L.xpt\",\"FSQS_L.xpt\"]),  # not published\n",
    "]\n",
    "\n",
    "def _try_download(url: str, dest: Path, retries=3, backoff=1.6) -> bool:\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            with requests.get(url, stream=True, timeout=45) as r:\n",
    "                if r.status_code == 200:\n",
    "                    with open(dest, \"wb\") as f:\n",
    "                        for chunk in r.iter_content(1<<15):\n",
    "                            if chunk: f.write(chunk)\n",
    "                    return True\n",
    "                if r.status_code == 404:\n",
    "                    return False\n",
    "        except requests.RequestException:\n",
    "            pass\n",
    "        time.sleep(backoff**i)\n",
    "    return False\n",
    "\n",
    "def _normalize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = pd.Index([\n",
    "        (c.decode(\"utf-8\",\"ignore\") if isinstance(c,(bytes,bytearray)) else str(c)).upper()\n",
    "        for c in df.columns\n",
    "    ])\n",
    "    return df\n",
    "\n",
    "def _attach_cycle(df: pd.DataFrame, cyc: int) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    if \"SDDSRVYR\" in df.columns:\n",
    "        ser = pd.to_numeric(df[\"SDDSRVYR\"], errors=\"coerce\")\n",
    "        df[\"SDDSRVYR\"] = ser.fillna(cyc)\n",
    "    else:\n",
    "        df[\"SDDSRVYR\"] = cyc\n",
    "    return df\n",
    "\n",
    "def download_fsq_all(specs=FSQ_SPECS):\n",
    "    out = []\n",
    "    for cyc, label, folder, fnames in specs:\n",
    "        got = False\n",
    "        for fname in fnames:\n",
    "            cand = [fname, fname[:-4] + (\".XPT\" if fname.lower().endswith(\".xpt\") else \".xpt\")]\n",
    "            for f in cand:\n",
    "                url = BASE.format(folder=folder, fname=f)\n",
    "                local = FSQ_DIR / f\"{folder}_{f}\"\n",
    "                if local.exists() and local.stat().st_size > 0:\n",
    "                    print(f\"✓ Exists: {label} @ {folder}/{f}\")\n",
    "                    out.append((local, cyc)); got = True; break\n",
    "                print(f\"→ Fetch {label}: {url}\")\n",
    "                if _try_download(url, local):\n",
    "                    print(f\"  ✓ Saved {local.name}\")\n",
    "                    out.append((local, cyc)); got = True; break\n",
    "            if got: break\n",
    "        if not got:\n",
    "            print(f\"ℹ Skipped: {label} — no public FSQ/FSQS at expected URLs\")\n",
    "    return out\n",
    "\n",
    "# 1) Download & read + adult filter\n",
    "fsq_paths = download_fsq_all()\n",
    "\n",
    "fsq_parts = []\n",
    "for p, cyc in fsq_paths:\n",
    "    try:\n",
    "        df = read_any(p)\n",
    "        df = _normalize_columns(df)\n",
    "        df = _attach_cycle(df, cyc)\n",
    "        if \"SEQN\" not in df.columns:\n",
    "            print(\"FSQ skip\", p.name, \"— no SEQN\"); continue\n",
    "        if 'age20' in globals() and isinstance(age20, pd.DataFrame) and \"SEQN\" in age20.columns:\n",
    "            df = df.merge(age20[[\"SEQN\"]], on=\"SEQN\", how=\"inner\")\n",
    "        df[\"_SRC\"] = p.name.upper()  # to prefer FSQ over FSQS later\n",
    "        fsq_parts.append(df)\n",
    "    except Exception as e:\n",
    "        print(\"FSQ skip\", p.name, \"—\", e)\n",
    "\n",
    "fsq_all = pd.concat(fsq_parts, ignore_index=True) if fsq_parts else pd.DataFrame()\n",
    "fsq_all = _normalize_columns(fsq_all)\n",
    "\n",
    "if fsq_all.empty or not {\"SEQN\",\"SDDSRVYR\"}.issubset(fsq_all.columns):\n",
    "    raise RuntimeError(\"No FSQ data loaded or missing SEQN/SDDSRVYR. Check downloads.\")\n",
    "\n",
    "# 2) Dedupe FSQ vs FSQS within SEQN+cycle (prefer FSQ, then richer row)\n",
    "relevant = [c for c in [\n",
    "    \"FSDHH\",\"HHFDSEC\",\"ADFDSEC\",\"FSQ012\",\"FSQ165\",\"FSQ170\",\"FSQ171\",\n",
    "    \"FSD170N\",\"FSD180\",\"FSD190\",\"FSD200\"\n",
    "] if c in fsq_all.columns]\n",
    "fsq_all[\"_NN\"] = fsq_all[relevant].notna().sum(axis=1) if relevant else 0\n",
    "fsq_all[\"_PREF\"] = fsq_all[\"_SRC\"].str.contains(r\"\\bFSQ\", case=False, regex=True).astype(int)\n",
    "fsq_all = (fsq_all.sort_values([\"SEQN\",\"SDDSRVYR\",\"_PREF\",\"_NN\"], ascending=[True,True,False,False])\n",
    "                   .drop_duplicates(subset=[\"SEQN\",\"SDDSRVYR\"], keep=\"first\")\n",
    "                   .drop(columns=[\"_SRC\",\"_NN\"], errors=\"ignore\"))\n",
    "\n",
    "# 3) Build BOTH household & adult FS + SNAP\n",
    "def col_any(df: pd.DataFrame, names: list[str]):\n",
    "    for n in names:\n",
    "        if n in df.columns:\n",
    "            return df[n]\n",
    "    return pd.Series(np.nan, index=df.index)\n",
    "\n",
    "def num_any(df: pd.DataFrame, names: list[str]):\n",
    "    return pd.to_numeric(col_any(df, names), errors=\"coerce\")\n",
    "\n",
    "snap = fsq_all[[\"SEQN\",\"SDDSRVYR\"]].copy()\n",
    "\n",
    "# Raw 4-level categories\n",
    "snap[\"FSDHH\"]   = pd.to_numeric(fsq_all.get(\"FSDHH\"), errors=\"coerce\")  # household (2003+)\n",
    "snap[\"HHFDSEC\"] = num_any(fsq_all, [\"HHFDSEC\",\"HHFDSEC \",\"HHfdsec\"])\n",
    "snap[\"ADFDSEC\"] = num_any(fsq_all, [\"ADFDSEC\",\"ADFDSEC \",\"ADfdsec\"])\n",
    "\n",
    "# Household 4-level (harmonized):\n",
    "# - Prefer FSDHH when present\n",
    "# - In cycles 1–2, if FSDHH is missing but HHFDSEC exists, use HHFDSEC\n",
    "snap[\"FS_HH4\"] = snap[\"FSDHH\"]\n",
    "mask_early = snap[\"SDDSRVYR\"].isin([1,2]) & snap[\"FS_HH4\"].isna() & snap[\"HHFDSEC\"].notna()\n",
    "snap.loc[mask_early, \"FS_HH4\"] = snap.loc[mask_early, \"HHFDSEC\"]\n",
    "\n",
    "# Adult 4-level\n",
    "snap[\"FS_ADULT4\"] = snap[\"ADFDSEC\"]\n",
    "\n",
    "# Binary mappings (1,2 → 0; 3,4 → 1)\n",
    "map_bin = {1:0, 2:0, 3:1, 4:1}\n",
    "snap[\"FS_HH\"]    = snap[\"FS_HH4\"].map(map_bin).astype(\"Int64\")\n",
    "snap[\"FS_ADULT\"] = snap[\"FS_ADULT4\"].map(map_bin).astype(\"Int64\")\n",
    "\n",
    "# Final FS: prefer household; else adult\n",
    "snap[\"FS_FINAL\"] = snap[\"FS_HH\"].where(snap[\"FS_HH\"].notna(), snap[\"FS_ADULT\"]).astype(\"Int64\")\n",
    "\n",
    "# Provenance flags\n",
    "snap[\"FS_SOURCE_HH\"] = pd.Series(pd.NA, index=snap.index, dtype=\"string\")\n",
    "snap.loc[snap[\"FSDHH\"].notna(), \"FS_SOURCE_HH\"] = \"FSDHH\"\n",
    "snap.loc[snap[\"FSDHH\"].isna() & snap[\"HHFDSEC\"].notna() & snap[\"SDDSRVYR\"].isin([1,2]), \"FS_SOURCE_HH\"] = \"HHFDSEC\"\n",
    "\n",
    "snap[\"FS_SOURCE_FINAL\"] = pd.Series(pd.NA, index=snap.index, dtype=\"string\")\n",
    "snap.loc[snap[\"FS_FINAL\"].notna() & snap[\"FS_HH\"].notna(), \"FS_SOURCE_FINAL\"] = \"household\"\n",
    "snap.loc[snap[\"FS_FINAL\"].notna() & snap[\"FS_HH\"].isna() & snap[\"FS_ADULT\"].notna(), \"FS_SOURCE_FINAL\"] = \"adult\"\n",
    "\n",
    "# --- SNAP (ENHANCED: adds 1999–2002 variables FSD180/FSD190 alongside existing ones) ---\n",
    "snap[\"SNAP\"] = pd.Series(pd.NA, index=snap.index, dtype=\"Int64\")\n",
    "\n",
    "# Pull numeric helpers (some files have trailing spaces; we guard for that)\n",
    "n_FSD170N = num_any(fsq_all, [\"FSD170N\",\"FSD170N \"])     # count authorized persons → ≥1 implies SNAP-related authorization\n",
    "n_FSD190  = num_any(fsq_all, [\"FSD190\",\"FSD190 \"])       # months authorized in last 12\n",
    "cur_FSD200 = (col_any(fsq_all, [\"FSD200\"]) == 1)         # currently authorized\n",
    "\n",
    "# YES evidence (set SNAP=1)\n",
    "yes_mask = (\n",
    "    (col_any(fsq_all, [\"FSQ012\"]) == 1) |\n",
    "    (col_any(fsq_all, [\"FSQ171\"]) == 1) |\n",
    "    (col_any(fsq_all, [\"FSQ170\"]) == 1) |\n",
    "    (n_FSD170N.ge(1)) |\n",
    "    (n_FSD190.ge(1)) |\n",
    "    (col_any(fsq_all, [\"FSD180\"]) == 1) |   # 1999–2002: authorized in last 12 months\n",
    "    (cur_FSD200)                             # currently authorized\n",
    ")\n",
    "snap.loc[yes_mask, \"SNAP\"] = 1\n",
    "\n",
    "# NO evidence (set SNAP=0 only where not already 1)\n",
    "# Be conservative: require explicit \"no\" PLUS no months PLUS not current PLUS no authorized persons.\n",
    "no_mask = (\n",
    "    (col_any(fsq_all, [\"FSQ012\"]) == 2) |\n",
    "    (col_any(fsq_all, [\"FSQ171\"]) == 2) |\n",
    "    ((col_any(fsq_all, [\"FSQ170\"]) == 2) & n_FSD170N.fillna(0).lt(1)) |\n",
    "    ((col_any(fsq_all, [\"FSD180\"]) == 2) & n_FSD190.fillna(0).lt(1) & (~cur_FSD200) & n_FSD170N.fillna(0).lt(1))\n",
    ")\n",
    "snap.loc[snap[\"SNAP\"].isna() & no_mask, \"SNAP\"] = 0\n",
    "\n",
    "snap[\"SNAP\"] = pd.to_numeric(snap[\"SNAP\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# Types, keys, dedupe\n",
    "snap = ensure_seqn(snap)  # ensures SEQN Int64 and sane ordering\n",
    "for c in [\"FSDHH\",\"HHFDSEC\",\"ADFDSEC\",\"FS_HH4\",\"FS_ADULT4\"]:\n",
    "    if c in snap.columns:\n",
    "        snap[c] = pd.to_numeric(snap[c], errors=\"coerce\").astype(\"Int64\")\n",
    "snap = snap.drop_duplicates([\"SEQN\",\"SDDSRVYR\"])\n",
    "\n",
    "# --- Diagnostics\n",
    "print(\"SNAP/FS shape:\", snap.shape)\n",
    "print(\"By cycle (rows):\")\n",
    "print(snap[\"SDDSRVYR\"].value_counts(dropna=False).sort_index())\n",
    "print(\"\\nCoverage (share non-missing):\")\n",
    "with pd.option_context(\"display.float_format\", \"{:.3f}\".format):\n",
    "    print((snap.groupby(\"SDDSRVYR\")[[\"FS_HH\",\"FS_ADULT\",\"FS_FINAL\",\"SNAP\"]]\n",
    "               .mean().rename(columns=lambda c: f\"{c}_pct\")))\n",
    "print(\"\\nSources used for household FS by cycle:\")\n",
    "print(snap.groupby([\"SDDSRVYR\",\"FS_SOURCE_HH\"])[\"SEQN\"].size().unstack(fill_value=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5c6ee423-d62f-4eff-80d0-22f78c138ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNAP/FS shape: (64313, 13)\n",
      "Rows by cycle:\n",
      " SDDSRVYR\n",
      "1     4880\n",
      "2     5411\n",
      "3     5041\n",
      "4     4979\n",
      "5     5935\n",
      "6     6218\n",
      "7     5560\n",
      "8     5769\n",
      "9     5719\n",
      "10    5569\n",
      "11    9232\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Final COVERAGE (non-missing share):\n",
      "           FS_HH_cov  FS_ADULT_cov  FS_FINAL_cov  SNAP_cov\n",
      "SDDSRVYR                                                 \n",
      "1             0.972         0.972         0.972     0.098\n",
      "2             0.934         0.934         0.934     0.085\n",
      "3             0.956         0.000         0.956     0.981\n",
      "4             0.989         0.000         0.989     0.988\n",
      "5             0.991         0.000         0.991     0.232\n",
      "6             0.988         0.000         0.988     0.249\n",
      "7             0.995         0.000         0.995     0.297\n",
      "8             0.987         0.000         0.987     0.296\n",
      "9             0.965         0.000         0.965     0.408\n",
      "10            0.942         0.000         0.942     0.382\n",
      "11            0.925         0.000         0.925     0.396 \n",
      "\n",
      "Final PREVALENCE (share==1):\n",
      "           FS_HH_prev FS_ADULT_prev  FS_FINAL_prev  SNAP_prev\n",
      "SDDSRVYR                                                    \n",
      "1              0.125         0.115          0.125      1.000\n",
      "2              0.132         0.124          0.132      1.000\n",
      "3              0.133          <NA>          0.133      0.127\n",
      "4              0.139          <NA>          0.139      0.111\n",
      "5              0.160          <NA>          0.160      0.671\n",
      "6              0.203          <NA>          0.203      0.787\n",
      "7              0.202          <NA>          0.202      0.790\n",
      "8              0.193          <NA>          0.193      0.744\n",
      "9              0.248          <NA>          0.248      0.608\n",
      "10             0.234          <NA>          0.234      0.578\n",
      "11             0.233          <NA>          0.233      0.567 \n",
      "\n",
      "Household FS source by cycle:\n",
      "FS_SOURCE_HH  FSDHH  HHFDSEC\n",
      "SDDSRVYR                    \n",
      "1                 0     4745\n",
      "2                 0     5053\n",
      "3              4820        0\n",
      "4              4924        0\n",
      "5              5880        0\n",
      "6              6146        0\n",
      "7              5533        0\n",
      "8              5692        0\n",
      "9              5516        0\n",
      "10             5244        0\n",
      "11             8543        0 \n",
      "\n",
      "SNAP sources — coverage by cycle:\n",
      "           FSQ012_cov  FSQ170_cov  FSQ171_cov  FSQ165_cov  FSD170N_cov  \\\n",
      "SDDSRVYR                                                                \n",
      "1              0.000       0.000       0.000       0.000        0.098   \n",
      "2              0.000       0.000       0.000       0.000        0.085   \n",
      "3              0.000       0.988       0.000       0.000        0.981   \n",
      "4              0.000       0.989       0.000       0.000        0.988   \n",
      "5              0.000       0.000       0.232       0.992        0.000   \n",
      "6              0.000       0.000       0.249       0.994        0.000   \n",
      "7              0.000       0.000       0.297       0.995        0.000   \n",
      "8              0.297       0.000       0.000       0.988        0.000   \n",
      "9              0.408       0.000       0.000       0.966        0.000   \n",
      "10             0.385       0.000       0.000       0.945        0.000   \n",
      "11             0.398       0.000       0.000       0.928        0.000   \n",
      "\n",
      "          FSD180_cov  FSD190_cov  FSD200_cov  \n",
      "SDDSRVYR                                      \n",
      "1              0.098       0.039       0.039  \n",
      "2              0.085       0.049       0.049  \n",
      "3              0.981       0.075       0.075  \n",
      "4              0.988       0.068       0.068  \n",
      "5              0.000       0.000       0.000  \n",
      "6              0.000       0.000       0.000  \n",
      "7              0.000       0.000       0.000  \n",
      "8              0.000       0.000       0.000  \n",
      "9              0.000       0.000       0.000  \n",
      "10             0.000       0.000       0.000  \n",
      "11             0.000       0.000       0.000   \n",
      "\n",
      "SNAP sources — positive/evidence shares by cycle:\n",
      "           FSQ012_yes  FSQ170_yes  FSQ171_yes  FSD180_yes  FSD200_yes  \\\n",
      "SDDSRVYR                                                               \n",
      "1              0.000       0.000       0.000       0.057       0.033   \n",
      "2              0.000       0.000       0.000       0.056       0.041   \n",
      "3              0.000       0.125       0.000       0.076       0.066   \n",
      "4              0.000       0.110       0.000       0.069       0.054   \n",
      "5              0.000       0.000       0.155       0.000       0.000   \n",
      "6              0.000       0.000       0.196       0.000       0.000   \n",
      "7              0.000       0.000       0.234       0.000       0.000   \n",
      "8              0.220       0.000       0.000       0.000       0.000   \n",
      "9              0.248       0.000       0.000       0.000       0.000   \n",
      "10             0.221       0.000       0.000       0.000       0.000   \n",
      "11             0.225       0.000       0.000       0.000       0.000   \n",
      "\n",
      "          FSD170N_ge1  FSD190_ge1  FSQ165_no  \n",
      "SDDSRVYR                                      \n",
      "1               0.098       0.039      0.000  \n",
      "2               0.085       0.049      0.000  \n",
      "3               0.125       0.075      0.000  \n",
      "4               0.109       0.068      0.000  \n",
      "5               0.000       0.000      0.759  \n",
      "6               0.000       0.000      0.739  \n",
      "7               0.000       0.000      0.695  \n",
      "8               0.000       0.000      0.689  \n",
      "9               0.000       0.000      0.552  \n",
      "10              0.000       0.000      0.552  \n",
      "11              0.000       0.000      0.523  \n"
     ]
    }
   ],
   "source": [
    "# --- Concise diagnostics: coverage vs prevalence ------------------------------\n",
    "print(\"SNAP/FS shape:\", snap.shape)\n",
    "print(\"Rows by cycle:\\n\", snap[\"SDDSRVYR\"].value_counts().sort_index(), \"\\n\")\n",
    "\n",
    "# Coverage (non-missing share) and Prevalence (share==1) for final outputs\n",
    "final_cols = [\"FS_HH\",\"FS_ADULT\",\"FS_FINAL\",\"SNAP\"]\n",
    "cov = snap.groupby(\"SDDSRVYR\")[final_cols].agg(lambda s: s.notna().mean())\n",
    "prev = snap.groupby(\"SDDSRVYR\")[final_cols].agg(lambda s: (s==1).mean())\n",
    "with pd.option_context(\"display.float_format\",\"{:.3f}\".format):\n",
    "    print(\"Final COVERAGE (non-missing share):\\n\", cov.rename(columns=lambda c: c+\"_cov\"), \"\\n\")\n",
    "    print(\"Final PREVALENCE (share==1):\\n\", prev.rename(columns=lambda c: c+\"_prev\"), \"\\n\")\n",
    "\n",
    "# Household FS source usage\n",
    "print(\"Household FS source by cycle:\")\n",
    "print(snap.groupby([\"SDDSRVYR\",\"FS_SOURCE_HH\"])[\"SEQN\"].size().unstack(fill_value=0), \"\\n\")\n",
    "\n",
    "# SNAP source columns — coverage + positive shares\n",
    "src = [\"FSQ012\",\"FSQ170\",\"FSQ171\",\"FSQ165\",\"FSD170N\",\"FSD180\",\"FSD190\",\"FSD200\"]\n",
    "g = fsq_all.groupby(\"SDDSRVYR\")\n",
    "cov_src = pd.concat(\n",
    "    [g[c].apply(lambda s: s.notna().mean()).rename(c+\"_cov\") for c in src if c in fsq_all.columns],\n",
    "    axis=1\n",
    ")\n",
    "pos_src = pd.concat([\n",
    "    (g[\"FSQ012\"].apply(lambda s: (s==1).mean()) if \"FSQ012\" in fsq_all.columns else pd.Series(dtype=float)).rename(\"FSQ012_yes\"),\n",
    "    (g[\"FSQ170\"].apply(lambda s: (s==1).mean()) if \"FSQ170\" in fsq_all.columns else pd.Series(dtype=float)).rename(\"FSQ170_yes\"),\n",
    "    (g[\"FSQ171\"].apply(lambda s: (s==1).mean()) if \"FSQ171\" in fsq_all.columns else pd.Series(dtype=float)).rename(\"FSQ171_yes\"),\n",
    "    (g[\"FSD180\"].apply(lambda s: (s==1).mean()) if \"FSD180\" in fsq_all.columns else pd.Series(dtype=float)).rename(\"FSD180_yes\"),\n",
    "    (g[\"FSD200\"].apply(lambda s: (s==1).mean()) if \"FSD200\" in fsq_all.columns else pd.Series(dtype=float)).rename(\"FSD200_yes\"),\n",
    "    (g[\"FSD170N\"].apply(lambda s: pd.to_numeric(s, errors=\"coerce\").ge(1).mean()) if \"FSD170N\" in fsq_all.columns else pd.Series(dtype=float)).rename(\"FSD170N_ge1\"),\n",
    "    (g[\"FSD190\"].apply(lambda s: pd.to_numeric(s, errors=\"coerce\").ge(1).mean()) if \"FSD190\" in fsq_all.columns else pd.Series(dtype=float)).rename(\"FSD190_ge1\"),\n",
    "    (g[\"FSQ165\"].apply(lambda s: (s==2).mean()) if \"FSQ165\" in fsq_all.columns else pd.Series(dtype=float)).rename(\"FSQ165_no\"),\n",
    "], axis=1)\n",
    "with pd.option_context(\"display.float_format\",\"{:.3f}\".format):\n",
    "    print(\"SNAP sources — coverage by cycle:\\n\", cov_src, \"\\n\")\n",
    "    print(\"SNAP sources — positive/evidence shares by cycle:\\n\", pos_src)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8253dc-30c1-4541-a580-f17c1aa8214e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442fb019-c4b1-4abe-bd6e-2d1c41fced39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31942e9-1c39-463d-90f0-072df2eff7e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0347290f-cb4a-4a94-bd2e-f3a8eebc5696",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcfe441-9471-4f0c-8f9f-3b74ae5f41d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "d9e0534e-3977-4301-9034-543eada19b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Exists: 1999-2000 @ 1999/FSQ.xpt\n",
      "✓ Exists: 2001-2002 @ 2001/FSQ_B.xpt\n",
      "✓ Exists: 2003-2004 @ 2003/FSQ_C.xpt\n",
      "✓ Exists: 2005-2006 @ 2005/FSQ_D.xpt\n",
      "✓ Exists: 2007-2008 @ 2007/FSQ_E.xpt\n",
      "✓ Exists: 2009-2010 @ 2009/FSQ_F.xpt\n",
      "✓ Exists: 2011-2012 @ 2011/FSQ_G.xpt\n",
      "✓ Exists: 2013-2014 @ 2013/FSQ_H.xpt\n",
      "✓ Exists: 2015-2016 @ 2015/FSQ_I.xpt\n",
      "✓ Exists: 2017-2018 @ 2017/FSQ_J.xpt\n",
      "✓ Exists: 2017–Mar 2020 (pre-pandemic) @ 2017/P_FSQ.xpt\n",
      "SNAP/FS shape: (64313, 14)\n",
      "By cycle (rows):\n",
      "SDDSRVYR\n",
      "1     4880\n",
      "2     5411\n",
      "3     5041\n",
      "4     4979\n",
      "5     5935\n",
      "6     6218\n",
      "7     5560\n",
      "8     5769\n",
      "9     5719\n",
      "10    5569\n",
      "11    9232\n",
      "Name: count, dtype: int64\n",
      "\n",
      "FS/SNAP coverage (share non-missing) by cycle:\n",
      "          FS_HH_cov  FS_ADULT_cov  FS_FINAL_cov  SNAP_cov\n",
      "SDDSRVYR                                                 \n",
      "1             0.972         0.972         0.972     0.097\n",
      "2             0.934         0.934         0.934     0.085\n",
      "3             0.956         0.000         0.956     0.981\n",
      "4             0.989         0.000         0.989     0.988\n",
      "5             0.991         0.000         0.991     0.991\n",
      "6             0.988         0.000         0.988     0.988\n",
      "7             0.995         0.000         0.995     0.992\n",
      "8             0.987         0.000         0.987     0.985\n",
      "9             0.965         0.000         0.965     0.959\n",
      "10            0.942         0.000         0.942     0.934\n",
      "11            0.925         0.000         0.925     0.919\n",
      "\n",
      "FS source by cycle:\n",
      "FS_SOURCE_HH  FSDHH  HHFDSEC\n",
      "SDDSRVYR                    \n",
      "1                 0     4745\n",
      "2                 0     5053\n",
      "3              4820        0\n",
      "4              4924        0\n",
      "5              5880        0\n",
      "6              6146        0\n",
      "7              5533        0\n",
      "8              5692        0\n",
      "9              5516        0\n",
      "10             5244        0\n",
      "11             8543        0\n"
     ]
    }
   ],
   "source": [
    "# --- FSQ/FSQS (SNAP & Food Security): fetch & build with HH + Adult (1999–2020) ---\n",
    "from pathlib import Path\n",
    "import requests, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "FSQ_DIR = (MOD / \"fsq\"); FSQ_DIR.mkdir(parents=True, exist_ok=True)\n",
    "BASE = \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/{folder}/DataFiles/{fname}\"\n",
    "\n",
    "FSQ_SPECS = [\n",
    "    (1,  \"1999-2000\", \"1999\", [\"FSQ.xpt\",   \"FSQS.xpt\"]),\n",
    "    (2,  \"2001-2002\", \"2001\", [\"FSQ_B.xpt\", \"FSQS_B.xpt\"]),\n",
    "    (3,  \"2003-2004\", \"2003\", [\"FSQ_C.xpt\", \"FSQS_C.xpt\"]),\n",
    "    (4,  \"2005-2006\", \"2005\", [\"FSQ_D.xpt\", \"FSQS_D.xpt\"]),\n",
    "    (5,  \"2007-2008\", \"2007\", [\"FSQ_E.xpt\", \"FSQS_E.xpt\"]),\n",
    "    (6,  \"2009-2010\", \"2009\", [\"FSQ_F.xpt\", \"FSQS_F.xpt\"]),\n",
    "    (7,  \"2011-2012\", \"2011\", [\"FSQ_G.xpt\", \"FSQS_G.xpt\"]),\n",
    "    (8,  \"2013-2014\", \"2013\", [\"FSQ_H.xpt\", \"FSQS_H.xpt\"]),\n",
    "    (9,  \"2015-2016\", \"2015\", [\"FSQ_I.xpt\", \"FSQS_I.xpt\"]),\n",
    "    (10, \"2017-2018\", \"2017\", [\"FSQ_J.xpt\", \"FSQS_J.xpt\"]),\n",
    "    (11, \"2017–Mar 2020 (pre-pandemic)\", \"2017\", [\"P_FSQ.xpt\", \"P_FSQS.xpt\"]),\n",
    "    # (12, \"Aug 2021–Aug 2023\", \"2021\", [\"FSQ_L.xpt\",\"FSQS_L.xpt\"]),  # not public\n",
    "]\n",
    "\n",
    "def _try_download(url: str, dest: Path, retries=3, backoff=1.6) -> bool:\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            with requests.get(url, stream=True, timeout=45) as r:\n",
    "                if r.status_code == 200:\n",
    "                    with open(dest, \"wb\") as f:\n",
    "                        for chunk in r.iter_content(1<<15):\n",
    "                            if chunk: f.write(chunk)\n",
    "                    return True\n",
    "                if r.status_code == 404:\n",
    "                    return False\n",
    "        except requests.RequestException:\n",
    "            pass\n",
    "        time.sleep(backoff**i)\n",
    "    return False\n",
    "\n",
    "def _normalize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = pd.Index([\n",
    "        (c.decode(\"utf-8\",\"ignore\") if isinstance(c,(bytes,bytearray)) else str(c)).upper()\n",
    "        for c in df.columns\n",
    "    ])\n",
    "    return df\n",
    "\n",
    "def _attach_cycle(df: pd.DataFrame, cyc: int) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    if \"SDDSRVYR\" in df.columns:\n",
    "        ser = pd.to_numeric(df[\"SDDSRVYR\"], errors=\"coerce\")\n",
    "        df[\"SDDSRVYR\"] = ser.fillna(cyc)\n",
    "    else:\n",
    "        df[\"SDDSRVYR\"] = cyc\n",
    "    return df\n",
    "\n",
    "def download_fsq_all(specs=FSQ_SPECS):\n",
    "    out = []\n",
    "    for cyc, label, folder, fnames in specs:\n",
    "        got = False\n",
    "        for fname in fnames:\n",
    "            cand = [fname, fname[:-4] + (\".XPT\" if fname.lower().endswith(\".xpt\") else \".xpt\")]\n",
    "            for f in cand:\n",
    "                url = BASE.format(folder=folder, fname=f)\n",
    "                local = FSQ_DIR / f\"{folder}_{f}\"\n",
    "                if local.exists() and local.stat().st_size > 0:\n",
    "                    print(f\"✓ Exists: {label} @ {folder}/{f}\")\n",
    "                    out.append((local, cyc)); got = True; break\n",
    "                print(f\"→ Fetch {label}: {url}\")\n",
    "                if _try_download(url, local):\n",
    "                    print(f\"  ✓ Saved {local.name}\")\n",
    "                    out.append((local, cyc)); got = True; break\n",
    "            if got: break\n",
    "        if not got:\n",
    "            print(f\"ℹ Skipped: {label} — no public FSQ/FSQS at expected URLs\")\n",
    "    return out\n",
    "\n",
    "# 1) Download & read + adult filter\n",
    "fsq_paths = download_fsq_all()\n",
    "\n",
    "fsq_parts = []\n",
    "for p, cyc in fsq_paths:\n",
    "    try:\n",
    "        df = read_any(p)\n",
    "        df = _normalize_columns(df)\n",
    "        df = _attach_cycle(df, cyc)\n",
    "        if \"SEQN\" not in df.columns:\n",
    "            print(\"FSQ skip\", p.name, \"— no SEQN\"); continue\n",
    "        if 'age20' in globals() and isinstance(age20, pd.DataFrame) and \"SEQN\" in age20.columns:\n",
    "            df = df.merge(age20[[\"SEQN\"]], on=\"SEQN\", how=\"inner\")\n",
    "        df[\"_SRC\"] = p.name.upper()\n",
    "        fsq_parts.append(df)\n",
    "    except Exception as e:\n",
    "        print(\"FSQ skip\", p.name, \"—\", e)\n",
    "\n",
    "fsq_all = pd.concat(fsq_parts, ignore_index=True) if fsq_parts else pd.DataFrame()\n",
    "fsq_all = _normalize_columns(fsq_all)\n",
    "if fsq_all.empty or not {\"SEQN\",\"SDDSRVYR\"}.issubset(fsq_all.columns):\n",
    "    raise RuntimeError(\"No FSQ data loaded or missing SEQN/SDDSRVYR.\")\n",
    "\n",
    "# 2) Dedupe FSQ vs FSQS by SEQN×cycle (prefer FSQ, then more-filled)\n",
    "relevant = [c for c in [\n",
    "    \"FSDHH\",\"HHFDSEC\",\"ADFDSEC\",\"FSQ012\",\"FSQ165\",\"FSQ170\",\"FSQ171\",\n",
    "    \"FSD170N\",\"FSD180\",\"FSD190\",\"FSD200\"\n",
    "] if c in fsq_all.columns]\n",
    "fsq_all[\"_NN\"]   = fsq_all[relevant].notna().sum(axis=1) if relevant else 0\n",
    "fsq_all[\"_PREF\"] = fsq_all[\"_SRC\"].str.contains(r\"\\bFSQ\", case=False, regex=True).astype(int)\n",
    "fsq_all = (fsq_all.sort_values([\"SEQN\",\"SDDSRVYR\",\"_PREF\",\"_NN\"], ascending=[True,True,False,False])\n",
    "                   .drop_duplicates([\"SEQN\",\"SDDSRVYR\"], keep=\"first\")\n",
    "                   .drop(columns=[\"_SRC\",\"_NN\"], errors=\"ignore\"))\n",
    "\n",
    "# 3) Build BOTH household & adult FS (4-level + binary + provenance) and SNAP + SNAP_SOURCE\n",
    "def col_any(df: pd.DataFrame, names: list[str]):\n",
    "    for n in names:\n",
    "        if n in df.columns:\n",
    "            return df[n]\n",
    "    return pd.Series(np.nan, index=df.index)\n",
    "\n",
    "def num_any(df: pd.DataFrame, names: list[str]):\n",
    "    return pd.to_numeric(col_any(df, names), errors=\"coerce\")\n",
    "\n",
    "snap = fsq_all[[\"SEQN\",\"SDDSRVYR\"]].copy()\n",
    "\n",
    "# Raw 4-level categories (keep as extras)\n",
    "snap[\"FSDHH\"]   = pd.to_numeric(fsq_all.get(\"FSDHH\"), errors=\"coerce\")      # household (2003+)\n",
    "snap[\"HHFDSEC\"] = num_any(fsq_all, [\"HHFDSEC\",\"HHFDSEC \",\"HHfdsec\"])        # early cycles\n",
    "snap[\"ADFDSEC\"] = num_any(fsq_all, [\"ADFDSEC\",\"ADFDSEC \",\"ADfdsec\"])        # early cycles\n",
    "\n",
    "# Harmonized 4-level\n",
    "snap[\"FS_HH4\"] = snap[\"FSDHH\"]\n",
    "mask_early = snap[\"SDDSRVYR\"].isin([1,2]) & snap[\"FS_HH4\"].isna() & snap[\"HHFDSEC\"].notna()\n",
    "snap.loc[mask_early, \"FS_HH4\"] = snap.loc[mask_early, \"HHFDSEC\"]\n",
    "snap[\"FS_ADULT4\"] = snap[\"ADFDSEC\"]\n",
    "\n",
    "# Binary mappings\n",
    "map_bin = {1:0, 2:0, 3:1, 4:1}\n",
    "snap[\"FS_HH\"]    = snap[\"FS_HH4\"].map(map_bin).astype(\"Int64\")\n",
    "snap[\"FS_ADULT\"] = snap[\"FS_ADULT4\"].map(map_bin).astype(\"Int64\")\n",
    "snap[\"FS_FINAL\"] = snap[\"FS_HH\"].where(snap[\"FS_HH\"].notna(), snap[\"FS_ADULT\"]).astype(\"Int64\")\n",
    "\n",
    "# Provenance\n",
    "snap[\"FS_SOURCE_HH\"] = pd.Series(pd.NA, index=snap.index, dtype=\"string\")\n",
    "snap.loc[snap[\"FSDHH\"].notna(), \"FS_SOURCE_HH\"] = \"FSDHH\"\n",
    "snap.loc[snap[\"FSDHH\"].isna() & snap[\"HHFDSEC\"].notna() & snap[\"SDDSRVYR\"].isin([1,2]), \"FS_SOURCE_HH\"] = \"HHFDSEC\"\n",
    "\n",
    "snap[\"FS_SOURCE_FINAL\"] = pd.Series(pd.NA, index=snap.index, dtype=\"string\")\n",
    "snap.loc[snap[\"FS_FINAL\"].notna() & snap[\"FS_HH\"].notna(), \"FS_SOURCE_FINAL\"] = \"household\"\n",
    "snap.loc[snap[\"FS_FINAL\"].notna() & snap[\"FS_HH\"].isna() & snap[\"FS_ADULT\"].notna(), \"FS_SOURCE_FINAL\"] = \"adult\"\n",
    "\n",
    "# --- SNAP with priority FSD180 for 1999–2002 + SNAP_SOURCE --------------------\n",
    "snap[\"SNAP\"] = pd.Series(pd.NA, index=snap.index, dtype=\"Int64\")\n",
    "snap[\"SNAP_SOURCE\"] = pd.Series(pd.NA, index=snap.index, dtype=\"string\")\n",
    "\n",
    "q012 = col_any(fsq_all, [\"FSQ012\"])\n",
    "q171 = col_any(fsq_all, [\"FSQ171\"])\n",
    "q170 = col_any(fsq_all, [\"FSQ170\"])\n",
    "q165 = col_any(fsq_all, [\"FSQ165\"])\n",
    "\n",
    "n170_raw = col_any(fsq_all, [\"FSD170N\",\"FSD170N \"])\n",
    "n190_raw = col_any(fsq_all, [\"FSD190\",\"FSD190 \"])\n",
    "s180     = col_any(fsq_all, [\"FSD180\"])\n",
    "s200     = col_any(fsq_all, [\"FSD200\"])\n",
    "\n",
    "# treat 77/99 as missing for counts\n",
    "n170 = pd.to_numeric(n170_raw, errors=\"coerce\").where(lambda x: x.between(1,30), np.nan)\n",
    "n190 = pd.to_numeric(n190_raw, errors=\"coerce\").where(lambda x: x.between(1,12), np.nan)\n",
    "\n",
    "early = snap[\"SDDSRVYR\"].isin([1,2])\n",
    "late  = ~early\n",
    "\n",
    "# Early cycles: prioritize FSD180\n",
    "m = early & (s180 == 1)\n",
    "snap.loc[m, \"SNAP\"] = 1; snap.loc[m, \"SNAP_SOURCE\"] = \"FSD180\"\n",
    "m = early & (s180 == 2)\n",
    "snap.loc[m & snap[\"SNAP\"].isna(), \"SNAP\"] = 0; snap.loc[m & snap[\"SNAP\"].isna(), \"SNAP_SOURCE\"] = \"FSD180\"\n",
    "\n",
    "m = early & snap[\"SNAP\"].isna() & ((s200 == 1) | n190.ge(1) | n170.ge(1))\n",
    "snap.loc[m, \"SNAP\"] = 1; snap.loc[m, \"SNAP_SOURCE\"] = \"FSD200/FSD190/FSD170N\"\n",
    "\n",
    "m = early & snap[\"SNAP\"].isna() & ((s200 == 2) & n190.fillna(0).lt(1) & n170.fillna(0).lt(1))\n",
    "snap.loc[m, \"SNAP\"] = 0; snap.loc[m, \"SNAP_SOURCE\"] = \"FSD200(no)+no months/people\"\n",
    "\n",
    "# Later cycles: FSQ items, with fallbacks\n",
    "m = late & (q012 == 1)\n",
    "snap.loc[m, \"SNAP\"] = 1; snap.loc[m, \"SNAP_SOURCE\"] = \"FSQ012\"\n",
    "m = late & (q012 == 2)\n",
    "snap.loc[m & snap[\"SNAP\"].isna(), \"SNAP\"] = 0; snap.loc[m & snap[\"SNAP\"].isna(), \"SNAP_SOURCE\"] = \"FSQ012\"\n",
    "\n",
    "m = late & snap[\"SNAP\"].isna() & (q171 == 1)\n",
    "snap.loc[m, \"SNAP\"] = 1; snap.loc[m, \"SNAP_SOURCE\"] = \"FSQ171\"\n",
    "m = late & snap[\"SNAP\"].isna() & (q171 == 2)\n",
    "snap.loc[m, \"SNAP\"] = 0; snap.loc[m, \"SNAP_SOURCE\"] = \"FSQ171\"\n",
    "\n",
    "m = late & snap[\"SNAP\"].isna() & (q170 == 1)\n",
    "snap.loc[m, \"SNAP\"] = 1; snap.loc[m, \"SNAP_SOURCE\"] = \"FSQ170\"\n",
    "m = late & snap[\"SNAP\"].isna() & (q170 == 2) & n170.fillna(0).lt(1)\n",
    "snap.loc[m, \"SNAP\"] = 0; snap.loc[m, \"SNAP_SOURCE\"] = \"FSQ170(no)+FSD170N<1\"\n",
    "\n",
    "m = late & snap[\"SNAP\"].isna() & ((n170.ge(1)) | (n190.ge(1)) | (s200 == 1))\n",
    "snap.loc[m, \"SNAP\"] = 1; snap.loc[m, \"SNAP_SOURCE\"] = \"FSD170N/190/200\"\n",
    "\n",
    "m = late & snap[\"SNAP\"].isna() & (q165 == 2)\n",
    "snap.loc[m, \"SNAP\"] = 0; snap.loc[m, \"SNAP_SOURCE\"] = \"FSQ165(no)\"\n",
    "\n",
    "snap[\"SNAP\"] = pd.to_numeric(snap[\"SNAP\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# Types & dedupe\n",
    "snap = ensure_seqn(snap)\n",
    "for c in [\"FSDHH\",\"HHFDSEC\",\"ADFDSEC\",\"FS_HH4\",\"FS_ADULT4\"]:\n",
    "    if c in snap.columns:\n",
    "        snap[c] = pd.to_numeric(snap[c], errors=\"coerce\").astype(\"Int64\")\n",
    "snap = snap.drop_duplicates([\"SEQN\",\"SDDSRVYR\"])\n",
    "\n",
    "# --- Diagnostics (minimal)\n",
    "print(\"SNAP/FS shape:\", snap.shape)\n",
    "print(\"By cycle (rows):\")\n",
    "print(snap[\"SDDSRVYR\"].value_counts(dropna=False).sort_index())\n",
    "print(\"\\nFS/SNAP coverage (share non-missing) by cycle:\")\n",
    "with pd.option_context(\"display.float_format\", \"{:.3f}\".format):\n",
    "    print((snap.groupby(\"SDDSRVYR\")[[\"FS_HH\",\"FS_ADULT\",\"FS_FINAL\",\"SNAP\"]]\n",
    "             .agg(lambda s: s.notna().mean())).rename(columns=lambda c: c+\"_cov\"))\n",
    "print(\"\\nFS source by cycle:\")\n",
    "print(snap.groupby([\"SDDSRVYR\",\"FS_SOURCE_HH\"])[\"SEQN\"].size().unstack(fill_value=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "2a84d4eb-8adf-4d0e-983f-b1baf8879c44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEQN</th>\n",
       "      <th>SDDSRVYR</th>\n",
       "      <th>FSDHH</th>\n",
       "      <th>HHFDSEC</th>\n",
       "      <th>ADFDSEC</th>\n",
       "      <th>FS_HH4</th>\n",
       "      <th>FS_ADULT4</th>\n",
       "      <th>FS_HH</th>\n",
       "      <th>FS_ADULT</th>\n",
       "      <th>FS_FINAL</th>\n",
       "      <th>FS_SOURCE_HH</th>\n",
       "      <th>FS_SOURCE_FINAL</th>\n",
       "      <th>SNAP</th>\n",
       "      <th>SNAP_SOURCE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>HHFDSEC</td>\n",
       "      <td>household</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>HHFDSEC</td>\n",
       "      <td>household</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>HHFDSEC</td>\n",
       "      <td>household</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>192</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>HHFDSEC</td>\n",
       "      <td>household</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>193</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>HHFDSEC</td>\n",
       "      <td>household</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>194</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>HHFDSEC</td>\n",
       "      <td>household</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>195</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>HHFDSEC</td>\n",
       "      <td>household</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>197</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>HHFDSEC</td>\n",
       "      <td>household</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    SEQN  SDDSRVYR  FSDHH  HHFDSEC  ADFDSEC  FS_HH4  FS_ADULT4  FS_HH  \\\n",
       "0      2         1   <NA>        1        1       1          1      0   \n",
       "1      5         1   <NA>        1        1       1          1      0   \n",
       "2      7         1   <NA>     <NA>     <NA>    <NA>       <NA>   <NA>   \n",
       "3     10         1   <NA>     <NA>     <NA>    <NA>       <NA>   <NA>   \n",
       "4     12         1   <NA>        1        1       1          1      0   \n",
       "..   ...       ...    ...      ...      ...     ...        ...    ...   \n",
       "95   192         1   <NA>        2        2       2          2      0   \n",
       "96   193         1   <NA>        1        1       1          1      0   \n",
       "97   194         1   <NA>        1        1       1          1      0   \n",
       "98   195         1   <NA>        1        1       1          1      0   \n",
       "99   197         1   <NA>        1        1       1          1      0   \n",
       "\n",
       "    FS_ADULT  FS_FINAL FS_SOURCE_HH FS_SOURCE_FINAL  SNAP SNAP_SOURCE  \n",
       "0          0         0      HHFDSEC       household  <NA>        <NA>  \n",
       "1          0         0      HHFDSEC       household  <NA>        <NA>  \n",
       "2       <NA>      <NA>         <NA>            <NA>  <NA>        <NA>  \n",
       "3       <NA>      <NA>         <NA>            <NA>  <NA>        <NA>  \n",
       "4          0         0      HHFDSEC       household  <NA>        <NA>  \n",
       "..       ...       ...          ...             ...   ...         ...  \n",
       "95         0         0      HHFDSEC       household  <NA>        <NA>  \n",
       "96         0         0      HHFDSEC       household  <NA>        <NA>  \n",
       "97         0         0      HHFDSEC       household  <NA>        <NA>  \n",
       "98         0         0      HHFDSEC       household  <NA>        <NA>  \n",
       "99         0         0      HHFDSEC       household  <NA>        <NA>  \n",
       "\n",
       "[100 rows x 14 columns]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snap.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f2fa64-6024-44ba-8d65-4d22f6c4af24",
   "metadata": {},
   "source": [
    "##### \n",
    "- check snap missingness "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "55b45494-8fe1-45c7-a30e-6029879ef123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             n  nonmiss  missing  nonmiss_pct  prev_all  prev_nonmiss\n",
      "SDDSRVYR                                                             \n",
      "1         4880      475     4405        0.097     0.589         0.589\n",
      "2         5411      460     4951        0.085     0.663         0.663\n",
      "3         5041     4946       95        0.981     0.127         0.127\n",
      "4         4979     4918       61        0.988     0.111         0.111\n",
      "5         5935     5880       55        0.991     0.157         0.157\n",
      "6         6218     6144       74        0.988     0.198         0.198\n",
      "7         5560     5514       46        0.992     0.236         0.236\n",
      "8         5769     5685       84        0.985     0.224         0.224\n",
      "9         5719     5487      232        0.959     0.259         0.259\n",
      "10        5569     5201      368        0.934     0.237         0.237\n",
      "11        9232     8483      749        0.919     0.245         0.245\n"
     ]
    }
   ],
   "source": [
    "# SNAP missingness by cycle (counts + %), and prevalence\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "snap_missing = (\n",
    "    snap.groupby(\"SDDSRVYR\")\n",
    "        .agg(\n",
    "            n=(\"SEQN\",\"size\"),\n",
    "            nonmiss=(\"SNAP\", lambda s: s.notna().sum()),\n",
    "            missing=(\"SNAP\", lambda s: s.isna().sum()),\n",
    "            nonmiss_pct=(\"SNAP\", lambda s: s.notna().mean()),\n",
    "            prev_all=(\"SNAP\", lambda s: s.eq(1).mean()),  # prevalence over all rows (incl. missing)\n",
    "            prev_nonmiss=(\"SNAP\", lambda s: s[s.notna()].eq(1).mean() if s.notna().any() else np.nan)\n",
    "        )\n",
    ")\n",
    "\n",
    "with pd.option_context(\"display.float_format\",\"{:.3f}\".format):\n",
    "    print(snap_missing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0a1a37-5a83-4a0e-be78-32bb330e2d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "3615ee2f-808d-4308-b9fe-b0a340ea7fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Quick audit — cycles seen & coverage if you want a pulse check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "de4967b5-8a02-4793-806a-bb81b906430b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDOH cycles: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8), np.int64(9), np.int64(10), np.int64(12), np.int64(66)]\n",
      "SDOH | base=128,809 | addon(seqn)=128,809 | inner keeps=128,809\n",
      "OCQ cycles: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8), np.int64(9), np.int64(10), np.int64(11), np.int64(12)]\n",
      " OCQ | base=128,809 | addon(seqn)=72,122 | inner keeps=72,122\n",
      "HOQ cycles: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8), np.int64(9), np.int64(10), np.int64(12)]\n",
      " HOQ | base=128,809 | addon(seqn)=62,890 | inner keeps=62,890\n",
      "HIQ cycles: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8), np.int64(9), np.int64(10), np.int64(11), np.int64(12)]\n",
      " HIQ | base=128,809 | addon(seqn)=72,122 | inner keeps=72,122\n",
      "FSQ cycles: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8), np.int64(9), np.int64(10), np.int64(11)]\n",
      " FSQ | base=128,809 | addon(seqn)=64,313 | inner keeps=64,313\n"
     ]
    }
   ],
   "source": [
    "# === Audit helpers ============================================================\n",
    "def _as_int64(s): \n",
    "    return pd.to_numeric(s, errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "def _cycles(df: pd.DataFrame, name: str):\n",
    "    if df is None or df.empty: print(f\"{name} cycles: (empty)\"); return\n",
    "    if \"SDDSRVYR\" in df.columns and df[\"SDDSRVYR\"].notna().any():\n",
    "        cyc = pd.to_numeric(df[\"SDDSRVYR\"], errors=\"coerce\").dropna().astype(int).unique()\n",
    "        print(f\"{name} cycles:\", sorted(cyc))\n",
    "    else:\n",
    "        print(f\"{name} cycles: (none)\")\n",
    "\n",
    "def _coverage(base: pd.DataFrame, addon: pd.DataFrame, name: str):\n",
    "    if addon is None or addon.empty: print(f\"{name:>4} | (addon empty)\"); return\n",
    "    b = base.copy(); a = addon.copy()\n",
    "    if \"SEQN\" in b: b[\"SEQN\"] = _as_int64(b[\"SEQN\"])\n",
    "    if \"SEQN\" in a: a[\"SEQN\"] = _as_int64(a[\"SEQN\"])\n",
    "    base_u  = b[\"SEQN\"].dropna().nunique()\n",
    "    add_u   = a[\"SEQN\"].dropna().nunique()\n",
    "    inner_u = b.merge(a[[\"SEQN\"]].drop_duplicates(), on=\"SEQN\", how=\"inner\")[\"SEQN\"].nunique()\n",
    "    print(f\"{name:>4} | base={base_u:,} | addon(seqn)={add_u:,} | inner keeps={inner_u:,}\")\n",
    "\n",
    "# pick base to compare to\n",
    "_base_for_audit = base if \"base\" in globals() else (master if \"master\" in globals() else sdoh)\n",
    "\n",
    "for (n, df_) in [(\"SDOH\", sdoh), (\"OCQ\", ocq), (\"HOQ\", hoq_all), (\"HIQ\", ins), (\"FSQ\", snap)]:\n",
    "    _cycles(df_, n)\n",
    "    _coverage(_base_for_audit, df_, n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930cac6d-10c4-4093-a087-8d8fb7b9e031",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b7c5b2-00f3-477c-96ad-5d3b0a1a9d11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cab9b3d0-e13f-417e-b409-baa2b26e84ca",
   "metadata": {},
   "source": [
    "## Merge these modules onto your base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "57ae3439-c082-4700-9441-9062dc73d24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged shape: (128809, 82)\n"
     ]
    }
   ],
   "source": [
    "# === Merge SDOH + modules (left-join with upsert) =============================\n",
    "master = ensure_seqn(base)\n",
    "if master[\"SEQN\"].dtype != \"Int64\":\n",
    "    master[\"SEQN\"] = pd.to_numeric(master[\"SEQN\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# SDOH first\n",
    "sdoh_cols = [\"SEQN\",\"PIR\",\"PIR_CAT\",\"INDFMINC\",\"EDU\",\"EDU_CAT\",\"RACE_ETH\",\"MARITAL\",\"MARITAL_CAT\"]\n",
    "sdoh_piece = sdoh[[c for c in sdoh_cols if c in sdoh.columns]].drop_duplicates(\"SEQN\")\n",
    "\n",
    "def _left_upsert(m: pd.DataFrame, piece: pd.DataFrame, tag: str):\n",
    "    if piece is None or piece.empty:\n",
    "        print(f\"ℹ️ {tag}: empty — skipped\"); \n",
    "        return m\n",
    "    p = ensure_seqn(piece)\n",
    "    if p[\"SEQN\"].duplicated().any():\n",
    "        dups = p.loc[p[\"SEQN\"].duplicated(), \"SEQN\"].unique()[:5]\n",
    "        raise RuntimeError(f\"{tag}: duplicate SEQN detected (e.g., {dups.tolist()} …).\")\n",
    "    overlap = (set(p.columns) & set(m.columns)) - {\"SEQN\"}\n",
    "    tmp = m.merge(p, on=\"SEQN\", how=\"left\", validate=\"one_to_one\", suffixes=(\"\", f\"_{tag.lower()}\"))\n",
    "    for c in sorted(overlap):\n",
    "        tmp[c] = tmp[c].fillna(tmp[f\"{c}_{tag.lower()}\"])\n",
    "        tmp.drop(columns=[f\"{c}_{tag.lower()}\"], inplace=True)\n",
    "    return tmp\n",
    "\n",
    "if not sdoh_piece.empty:\n",
    "    master = _left_upsert(master, sdoh_piece, \"SDOH\")\n",
    "else:\n",
    "    print(\"ℹ️ SDOH: empty — skipped\")\n",
    "\n",
    "# Modules (including FSQ built in Cell A)\n",
    "#pieces = {\n",
    "#    \"OCQ\": ocq[[c for c in [\"SEQN\",\"EMPLOY\",\"UNEMPLOYMENT\"] if c in ocq.columns]],\n",
    "#    \"HOQ\": hoq_all[[c for c in [\"SEQN\",\"HOD050\",\"HOQ065\"] if c in hoq_all.columns]],\n",
    "#    \"HIQ\": ins[[c for c in [\"SEQN\",\"INS\"] if c in ins.columns]],\n",
    "#    \"FSQ\": snap[[c for c in [\"SEQN\",\"SNAP\",\"FSDHH\",\"FS\", \"FS_HH\", \"FS_ADULT\", \"FS_FINAL\"] if c in snap.columns]],\n",
    "#}\n",
    "\n",
    "# --- Modules (incl. FSQ with extras; alias FS := FS_FINAL if FS missing) -----\n",
    "# Make a safe copy so we can add the FS alias without side effects\n",
    "snap_use = snap.copy()\n",
    "if \"FS\" not in snap_use.columns and \"FS_FINAL\" in snap_use.columns:\n",
    "    snap_use[\"FS\"] = snap_use[\"FS_FINAL\"]\n",
    "\n",
    "fs_cols_base  = [\"SEQN\", \"SNAP\", \"FSDHH\", \"FS\", \"FS_HH\", \"FS_ADULT\", \"FS_FINAL\"]\n",
    "fs_cols_extra = [\"HHFDSEC\", \"ADFDSEC\", \"FS_HH4\", \"FS_ADULT4\",\n",
    "                 \"FS_SOURCE_HH\", \"FS_SOURCE_FINAL\", \"SNAP_SOURCE\"]\n",
    "fs_keep = [c for c in (fs_cols_base + fs_cols_extra) if c in snap_use.columns]\n",
    "\n",
    "pieces = {\n",
    "    \"OCQ\": ocq[[c for c in [\"SEQN\",\"EMPLOY\",\"UNEMPLOYMENT\"] if c in ocq.columns]],\n",
    "    \"HOQ\": hoq_all[[c for c in [\"SEQN\",\"HOD050\",\"HOQ065\"] if c in hoq_all.columns]],\n",
    "    \"HIQ\": ins[[c for c in [\"SEQN\",\"INS\"] if c in ins.columns]],\n",
    "    \"FSQ\": snap_use[fs_keep],\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "for name, piece in pieces.items():\n",
    "    master = _left_upsert(master, piece, name)\n",
    "\n",
    "print(\"Merged shape:\", master.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52574e9-292e-410c-8645-3bc9287fa3af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "f8cc54b6-57e3-406a-a13a-84e33dabc201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename \n",
    "demo_mt_cov_dp_sdoh = master\n",
    "del master  # optional: drop old handle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3382196c-c8cc-44f9-b535-092a1d5808d5",
   "metadata": {},
   "source": [
    "#### save final merged table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "ad72dfe1-eb40-45ee-a099-d97787bfb484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: /Users/dengshuyue/Desktop/SDOH/analysis/output\n"
     ]
    }
   ],
   "source": [
    "# Save final merged table\n",
    "(out := OUT).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "demo_mt_cov_dp_sdoh.to_parquet(out / \"demo_mt_cov_dp_sdoh.parquet\", index=False)\n",
    "# demo_mt_cv_dp_sdoh.to_csv(out / \"demo_mt_cov_dp_sdoh.csv.gz\", index=False, compression=\"gzip\")\n",
    "\n",
    "print(\"Saved to:\", out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8853ba87-c7c4-4f80-a49b-fd353d4b38db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456d1638-2e49-4d64-9fc3-64ca43364e65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "932e51ef-2c16-4153-b9e8-042f420af0a8",
   "metadata": {},
   "source": [
    "#### check FS_HH and FS_FINAL  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "8cb89fc6-9607-4136-84b7-27d6f3fc3f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick check \n",
    "# demo_mt_cov_dp_sdoh[\"FS_HH\"].head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "bc07b103-67aa-4025-8f76-0b0a8fa7974f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Coverage by cycle (which cycles actually have FS data?)\n",
    "# demo_mt_cov_dp_sdoh.groupby(\"SDDSRVYR\")[\"FS_FINAL\"].apply(lambda s: s.notna().mean()).round(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260fcb35-6a7d-450f-9206-a147e38f32f8",
   "metadata": {},
   "source": [
    "#### check: list all column name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "ab42caf9-4904-4a7e-9ec7-a0e618afb9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo_mt_cov_dp_sdoh.columns.tolist()\n",
    "# print(\"\\n\".join(demo_mt_cov_dp_sdoh.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "354b3575-7982-492f-bdca-852b7b607e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate SEQN×SDDSRVYR rows: 0\n",
      "Missing CORE cols: []\n",
      "Missing OPTIONAL cols (ok to be missing): []\n",
      "\n",
      "Coverage & prevalence by cycle:\n",
      "           FS_HH_cov  FS_FINAL_cov  SNAP_cov  FS_HH_prev_nonmiss  \\\n",
      "SDDSRVYR                                                          \n",
      "1.0           0.476         0.476     0.048               0.125   \n",
      "2.0           0.458         0.458     0.042               0.132   \n",
      "3.0           0.476         0.476     0.489               0.133   \n",
      "4.0           0.476         0.476     0.475               0.139   \n",
      "5.0           0.579         0.579     0.579               0.160   \n",
      "6.0           0.583         0.583     0.583               0.203   \n",
      "7.0           0.567         0.567     0.565               0.202   \n",
      "8.0           0.559         0.559     0.559               0.193   \n",
      "9.0           0.553         0.553     0.550               0.248   \n",
      "10.0          0.567         0.567     0.562               0.234   \n",
      "12.0          0.000         0.000     0.000                 NaN   \n",
      "66.0          0.549         0.549     0.545               0.233   \n",
      "\n",
      "          FS_FINAL_prev_nonmiss  SNAP_prev_nonmiss  FS_HH_prev_all  \\\n",
      "SDDSRVYR                                                             \n",
      "1.0                       0.125              0.589           0.060   \n",
      "2.0                       0.132              0.663           0.061   \n",
      "3.0                       0.133              0.127           0.063   \n",
      "4.0                       0.139              0.111           0.066   \n",
      "5.0                       0.160              0.157           0.093   \n",
      "6.0                       0.203              0.198           0.119   \n",
      "7.0                       0.202              0.236           0.114   \n",
      "8.0                       0.193              0.224           0.108   \n",
      "9.0                       0.248              0.259           0.137   \n",
      "10.0                      0.234              0.237           0.132   \n",
      "12.0                        NaN                NaN           0.000   \n",
      "66.0                      0.233              0.245           0.128   \n",
      "\n",
      "          FS_FINAL_prev_all  SNAP_prev_all  \n",
      "SDDSRVYR                                    \n",
      "1.0                   0.060          0.028  \n",
      "2.0                   0.061          0.028  \n",
      "3.0                   0.063          0.062  \n",
      "4.0                   0.066          0.053  \n",
      "5.0                   0.093          0.091  \n",
      "6.0                   0.119          0.116  \n",
      "7.0                   0.114          0.134  \n",
      "8.0                   0.108          0.125  \n",
      "9.0                   0.137          0.142  \n",
      "10.0                  0.132          0.133  \n",
      "12.0                  0.000          0.000  \n",
      "66.0                  0.128          0.133  \n",
      "\n",
      "FS_FINAL logic OK: True  | Mismatch rows: 0\n",
      "\n",
      "Rows: 128809  | Unique keys: 128809\n"
     ]
    }
   ],
   "source": [
    "# --- Quick checks for demo_mt_cov_dp_sdoh (tailored to your columns) ----------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = demo_mt_cov_dp_sdoh\n",
    "\n",
    "# 1) Unique key?\n",
    "dups = df.duplicated([\"SEQN\",\"SDDSRVYR\"]).sum()\n",
    "print(\"Duplicate SEQN×SDDSRVYR rows:\", dups)\n",
    "\n",
    "# 2) Required vs optional SDOH fields (based on what you said you have)\n",
    "required_core = [\n",
    "    # OCQ\n",
    "    \"EMPLOY\",\"UNEMPLOYMENT\",\n",
    "    # HOQ\n",
    "    \"HOD050\",\"HOQ065\",\n",
    "    # HIQ\n",
    "    \"INS\",\n",
    "    # FSQ (core you said you have)\n",
    "    \"FSDHH\",\"FS_HH\",\"FS_ADULT\",\"FS_FINAL\",\"SNAP\",\n",
    "]\n",
    "optional_extras = [\n",
    "    # FS extras (only if you merged them earlier)\n",
    "    \"HHFDSEC\",\"ADFDSEC\",\"FS_HH4\",\"FS_ADULT4\",\"FS_SOURCE_HH\",\"FS_SOURCE_FINAL\",\"SNAP_SOURCE\",\n",
    "]\n",
    "\n",
    "missing_core   = [c for c in required_core   if c not in df.columns]\n",
    "missing_extra  = [c for c in optional_extras if c not in df.columns]\n",
    "print(\"Missing CORE cols:\", missing_core)\n",
    "print(\"Missing OPTIONAL cols (ok to be missing):\", missing_extra)\n",
    "\n",
    "# 3) Coverage + prevalence snapshot for FS and SNAP\n",
    "cov_prev = (\n",
    "    df.groupby(\"SDDSRVYR\")\n",
    "      .agg(\n",
    "          FS_HH_cov    = (\"FS_HH\",    lambda s: s.notna().mean()),\n",
    "          FS_FINAL_cov = (\"FS_FINAL\", lambda s: s.notna().mean()),\n",
    "          SNAP_cov     = (\"SNAP\",     lambda s: s.notna().mean()),\n",
    "          # prevalence among non-missing\n",
    "          FS_HH_prev_nonmiss    = (\"FS_HH\",    lambda s: s[s.notna()].eq(1).mean() if s.notna().any() else np.nan),\n",
    "          FS_FINAL_prev_nonmiss = (\"FS_FINAL\", lambda s: s[s.notna()].eq(1).mean() if s.notna().any() else np.nan),\n",
    "          SNAP_prev_nonmiss     = (\"SNAP\",     lambda s: s[s.notna()].eq(1).mean() if s.notna().any() else np.nan),\n",
    "          # overall prevalence (treat missing as 0)\n",
    "          FS_HH_prev_all    = (\"FS_HH\",    lambda s: s.fillna(0).eq(1).mean()),\n",
    "          FS_FINAL_prev_all = (\"FS_FINAL\", lambda s: s.fillna(0).eq(1).mean()),\n",
    "          SNAP_prev_all     = (\"SNAP\",     lambda s: s.fillna(0).eq(1).mean()),\n",
    "      )\n",
    "      .round(3)\n",
    ")\n",
    "print(\"\\nCoverage & prevalence by cycle:\\n\", cov_prev)\n",
    "\n",
    "# 4) FS_FINAL logic check (FS_FINAL should equal FS_HH if present, else FS_ADULT)\n",
    "expected_fs = df[\"FS_HH\"].where(df[\"FS_HH\"].notna(), df[\"FS_ADULT\"])\n",
    "logic_ok = (df[\"FS_FINAL\"].fillna(-1) == expected_fs.fillna(-1)).all()\n",
    "mismatches = (df[\"FS_FINAL\"].fillna(-1) != expected_fs.fillna(-1))\n",
    "print(\"\\nFS_FINAL logic OK:\", logic_ok, \" | Mismatch rows:\", int(mismatches.sum()))\n",
    "if mismatches.any():\n",
    "    print(df.loc[mismatches, [\"SEQN\",\"SDDSRVYR\",\"FS_HH\",\"FS_ADULT\",\"FS_FINAL\"]].head())\n",
    "\n",
    "# 5) One row per participant-cycle?\n",
    "print(\"\\nRows:\", len(df), \" | Unique keys:\", df[[\"SEQN\",\"SDDSRVYR\"]].drop_duplicates().shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "a41af8f6-cc74-4eb1-8eae-664a66b98544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added cols: ['HHFDSEC', 'ADFDSEC', 'FS_HH4', 'FS_ADULT4', 'FS_SOURCE_HH', 'FS_SOURCE_FINAL', 'SNAP_SOURCE']\n",
      "Rows unchanged: 128809\n",
      "HHFDSEC            0.076\n",
      "ADFDSEC            0.076\n",
      "FS_HH4             0.482\n",
      "FS_ADULT4          0.076\n",
      "FS_SOURCE_HH       0.482\n",
      "FS_SOURCE_FINAL    0.482\n",
      "SNAP_SOURCE        0.380\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "added = [c for c in [\"HHFDSEC\",\"ADFDSEC\",\"FS_HH4\",\"FS_ADULT4\",\"FS_SOURCE_HH\",\"FS_SOURCE_FINAL\",\"SNAP_SOURCE\"]\n",
    "         if c in demo_mt_cv_dp_sdoh.columns]\n",
    "print(\"Added cols:\", added)\n",
    "print(\"Rows unchanged:\", len(demo_mt_cv_dp_sdoh))\n",
    "\n",
    "# Optional: coverage of the added fields\n",
    "print(demo_mt_cov_dp_sdoh[added].notna().mean().round(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaead903-b28e-47b0-ae89-705bef7648b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3178b8c-ee6d-4eca-b918-fc13abb6c12d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba32523f-3d87-4446-b229-8f73b7e9fc56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99f6272-b957-4185-8eff-1c9d2a4a3faf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9cf05551-4cf6-4e75-a112-776d750adb66",
   "metadata": {},
   "source": [
    "#### check my column name with previous coworker's name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6481c8c5-7a4c-4701-8db9-efb3e8c7e546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo_mt_cov_dp_sdoh.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "9005ed5d-63cf-4717-8edd-dde68ed2ff67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Demo vs Ref85 (column names) ===\n",
      "Ref85 count: 85\n",
      "Your count:  74\n",
      "Exact matches (case-sensitive): 15\n",
      "\n",
      "Missing in your df:\n",
      "['ALCG2', 'DR12DRST', 'Death_alz', 'Death_cancer', 'Death_cerev', 'Death_diabe', 'Death_heart', 'Death_infl', 'Death_inj', 'Death_kid', 'Death_oth2', 'Death_other', 'Death_resp', 'FS', 'HEI2015_TOTAL_SCORE', 'HYPERTEN', 'INDFMPIR', 'RACE', 'RIDAGEYR', 'SMK', 'SMK_AVG', 'WTDR2D', 'WTDRD1', 'ageend', 'agesq', 'agestart', 'angina', 'angina_rx', 'bmi', 'bmic', 'cancer', 'chol_rx', 'dbp', 'death_cmd', 'death_cmdk', 'death_cmdkh', 'death_cvd', 'death_multi', 'death_other1', 'dm_rx', 'dm_self', 'employ', 'hba1c', 'hdl', 'hei2015_sd', 'i_FCS', 'i_FCS_sd', 'i_HSR', 'i_HSR_sd', 'i_Optup', 'i_Optup_sd', 'i_nutri', 'i_nutri_sd', 'include', 'ins', 'ldl', 'lung_disease', 'marriage', 'met_hr', 'perE_alco', 'pir', 'py', 'sbp', 'sdmvpsu', 'sdmvstra', 'tchol', 'tg', 'unemployment', 'wt', 'wt10']\n",
      "\n",
      "Extras in your df:\n",
      "['AGE_YR', 'ALCOHOL_CAT', 'BMI', 'BMI_CLAS', 'BMXHT', 'BMXWT', 'CANCER', 'CENSORED', 'CIDI_12M_MDE', 'CIDI_SCORE_RAW', 'CIGS_PER_DAY', 'DBP', 'DEP_HARMONIZED', 'DEP_IMP', 'DEP_SOURCE', 'DMDHHSIZ', 'DPQ_CAT', 'DRINKS_PER_DAY', 'EDU_CAT', 'EMPLOY', 'EVENT', 'FEMALE', 'FORMER_SMOKER', 'FS_ADULT', 'FS_FINAL', 'FS_HH', 'FU_YRS_EXM', 'FU_YRS_INT', 'HDL', 'HIGH_CHOL', 'HTN', 'IMP', 'INDFMINC', 'INS', 'IS_ADULT', 'IS_POST2018', 'LDL', 'LTPA', 'MARITAL', 'MARITAL_CAT', 'METSCORE', 'MORTALITY_COVERED', 'PACK_YEARS', 'PHQ9', 'PHQ9_GE10', 'PIR', 'PIR_CAT', 'RACE_ETH', 'RIAGENDR', 'SBP', 'SDMVPSU', 'SDMVSTRA', 'SMK_STATUS', 'TCHOL', 'TG', 'UCOD_LABEL', 'UNEMPLOYMENT', 'WTMEC2YR', 'WTSCI2YR']\n",
      "\n",
      "Proposed renames (old → new):\n",
      "  SDMVPSU → sdmvpsu\n",
      "  SDMVSTRA → sdmvstra\n",
      "  BMI → bmi\n",
      "  CANCER → cancer\n",
      "  SBP → sbp\n",
      "  DBP → dbp\n",
      "  TCHOL → tchol\n",
      "  HDL → hdl\n",
      "  LDL → ldl\n",
      "  TG → tg\n",
      "  PIR → pir\n",
      "  EMPLOY → employ\n",
      "  UNEMPLOYMENT → unemployment\n",
      "  INS → ins\n",
      "  HTN → HYPERTEN\n",
      "\n",
      "Still missing after proposed renames:\n",
      "['ALCG2', 'DR12DRST', 'Death_alz', 'Death_cancer', 'Death_cerev', 'Death_diabe', 'Death_heart', 'Death_infl', 'Death_inj', 'Death_kid', 'Death_oth2', 'Death_other', 'Death_resp', 'FS', 'HEI2015_TOTAL_SCORE', 'INDFMPIR', 'RACE', 'RIDAGEYR', 'SMK', 'SMK_AVG', 'WTDR2D', 'WTDRD1', 'ageend', 'agesq', 'agestart', 'angina', 'angina_rx', 'bmic', 'chol_rx', 'death_cmd', 'death_cmdk', 'death_cmdkh', 'death_cvd', 'death_multi', 'death_other1', 'dm_rx', 'dm_self', 'hba1c', 'hei2015_sd', 'i_FCS', 'i_FCS_sd', 'i_HSR', 'i_HSR_sd', 'i_Optup', 'i_Optup_sd', 'i_nutri', 'i_nutri_sd', 'include', 'lung_disease', 'marriage', 'met_hr', 'perE_alco', 'py', 'wt', 'wt10']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 85-column reference (from your CSV, after dropping DIABE, ins2, unemployment2,\n",
    "# and the lowercase duplicates: death_heart, death_cancer, death_resp, death_cerev, death_diabe)\n",
    "REF85 = [\n",
    " 'SEQN','marriage','SDDSRVYR','HOD050','HOQ065','employ','unemployment','ELIGSTAT','MORTSTAT',\n",
    " 'UCOD_LEADING','DIABETES','HYPERTEN','PERMTH_INT','PERMTH_EXM','Death_heart','Death_cancer',\n",
    " 'Death_resp','Death_cerev','Death_diabe','Death_other','death_cvd','death_cmd','SNAP','FSDHH','FS',\n",
    " 'ins','RIDAGEYR','SEX','RACE','EDU','INDFMPIR','SMK_AVG','SMK','ALCG2','HEI2015_TOTAL_SCORE',\n",
    " 'WTDRD1','WTDR2D','DR12DRST','i_FCS','i_Optup','i_HSR','i_nutri','sdmvpsu','sdmvstra','met_hr',\n",
    " 'perE_alco','dm_self','tchol','hdl','ldl','tg','bmi','CVD','dm_rx','chol_rx','angina_rx','lung_disease',\n",
    " 'angina','hba1c','sbp','dbp','cancer','wt10','wt','i_FCS_sd','i_Optup_sd','i_nutri_sd','i_HSR_sd',\n",
    " 'hei2015_sd','Death_inj','Death_alz','Death_infl','Death_kid','death_other1','Death_oth2','death_cmdk',\n",
    " 'death_cmdkh','death_multi','agesq','py','agestart','ageend','pir','bmic','include'\n",
    "]\n",
    "\n",
    "df = demo_mt_cov_dp_sdoh.copy()  # your DataFrame\n",
    "\n",
    "# Case-insensitive helpers\n",
    "ref_upper = {c.upper(): c for c in REF85}\n",
    "df_upper  = {c.upper(): c for c in df.columns}\n",
    "\n",
    "# Exact, missing, extras (case-sensitive)\n",
    "exact = sorted(set(REF85) & set(df.columns))\n",
    "missing = sorted(set(REF85) - set(df.columns))\n",
    "extras = sorted(set(df.columns) - set(REF85))\n",
    "\n",
    "# Build an OLD->NEW rename map:\n",
    "# 1) case-only differences: if df has SBP but ref expects 'sbp', suggest SBP->sbp\n",
    "rename_map = {}\n",
    "for up, df_name in df_upper.items():\n",
    "    if up in ref_upper:\n",
    "        ref_name = ref_upper[up]\n",
    "        if df_name != ref_name:\n",
    "            rename_map[df_name] = ref_name\n",
    "\n",
    "# 2) common synonym fixes (ONLY if present in df and missing in ref)\n",
    "synonyms = {\n",
    "    # examples: tweak as you find more\n",
    "    'HTN': 'HYPERTEN',\n",
    "    'TOTAL_CHOL' : 'tchol',\n",
    "    'HDL_CHOL'   : 'hdl',\n",
    "    'LDL_CHOL'   : 'ldl',\n",
    "    'TRIG'       : 'tg',\n",
    "    'BMI'        : 'bmi',\n",
    "    'SBP'        : 'sbp',\n",
    "    'DBP'        : 'dbp',\n",
    "    'INS': 'ins',\n",
    "    'EMPLOYMENT_STATUS': 'employ',\n",
    "    'UNEMPLOYED': 'unemployment',\n",
    "    'WT': 'wt',\n",
    "    'WT10': 'wt10',\n",
    "    'MET_HR': 'met_hr',\n",
    "}\n",
    "for old, new in synonyms.items():\n",
    "    if old in df.columns and new in REF85 and new not in df.columns:\n",
    "        rename_map[old] = new\n",
    "\n",
    "# Preview post-rename set\n",
    "preview_cols = [rename_map.get(c, c) for c in df.columns]\n",
    "missing_after = sorted(set(REF85) - set(preview_cols))\n",
    "\n",
    "print(\"=== Demo vs Ref85 (column names) ===\")\n",
    "print(f\"Ref85 count: {len(REF85)}\")\n",
    "print(f\"Your count:  {len(df.columns)}\")\n",
    "print(f\"Exact matches (case-sensitive): {len(exact)}\")\n",
    "print(\"\\nMissing in your df:\")\n",
    "print(missing)\n",
    "print(\"\\nExtras in your df:\")\n",
    "print(extras)\n",
    "print(\"\\nProposed renames (old → new):\")\n",
    "for k, v in rename_map.items():\n",
    "    print(f\"  {k} → {v}\")\n",
    "print(\"\\nStill missing after proposed renames:\")\n",
    "print(missing_after)\n",
    "\n",
    "# When ready to apply:\n",
    "# df = df.rename(columns=rename_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be640130-ff59-451d-89e8-5d387ce9bf2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12a8d3bc-b954-4c63-b58c-53728ebcba0e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
