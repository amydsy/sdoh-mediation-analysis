{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23e4e0a1",
   "metadata": {},
   "source": [
    "<h1> 00 — merge demo, mort, sdoh</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24edfeb",
   "metadata": {},
   "source": [
    "<h2>Shared environment and helper functions used across notebooks.</h2>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "88aa7d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap loaded.\n",
      "ROOT: /Users/dengshuyue/Desktop/SDOH/analysis\n",
      "Data dir exists: True\n",
      "Output dir exists: True\n",
      "Mortality cycles: ['1999-2000', '2001-2002', '2003-2004', '2005-2006', '2007-2008', '2009-2010', '2011-2012', '2013-2014', '2015-2016', '2017-2018']\n",
      "Non-mortality cycles: ['2017-March 2020 (pre-pandemic)', 'August 2021–August 2023']\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os, sys, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------------\n",
    "# Root & existing folders (NO mkdir here)\n",
    "# -------------------------\n",
    "ROOT = Path(os.environ.get(\"SDOH_ROOT\", \"/Users/dengshuyue/Desktop/SDOH/analysis\"))\n",
    "\n",
    "CODE   = ROOT / \"code\"\n",
    "DATA   = ROOT / \"data\"\n",
    "OUT    = ROOT / \"output\"\n",
    "\n",
    "# Data subfolders that you already have\n",
    "NH_DEIT      = DATA / \"nhanes_deit\"\n",
    "NH_BY_MOD    = DATA / \"nhanes_by_module\"\n",
    "FPED_DIR     = DATA / \"fped\"\n",
    "FNDDDS_DIR   = DATA / \"fndds\"\n",
    "BPQ_DIR      = DATA / \"bpq\"\n",
    "HEALTH_ACC   = DATA / \"health_access\"\n",
    "HH_SIZE_DIR  = DATA / \"household_size\"\n",
    "TMP_NORM_XPT = DATA / \"tmp_norm_xpt\"\n",
    "LESS_IMP     = DATA / \"less_important\"\n",
    "\n",
    "# Common files already present\n",
    "FILES = {\n",
    "    \"demoall_csv\":          DATA / \"demoall.csv\",\n",
    "    \"demoall_pkl\":          DATA / \"demoall.pkl\",\n",
    "    \"hei9918_sas7bdat\":     DATA / \"hei9918.sas7bdat\",\n",
    "    \"sodh_diet_mort_sas\":   DATA / \"sodh_diet_mort.sas7bdat\",\n",
    "    \"sodh_diet_mort_pkl\":   DATA / \"SODH_diet_mort.pkl\",\n",
    "    # multiple CSV variants exist; we’ll glob when needed\n",
    "}\n",
    "\n",
    "# Output files/folders (already exist in your tree)\n",
    "OUT_FILES = {\n",
    "    \"demo_summary_csv\":     OUT / \"demo_summary.csv\",\n",
    "    \"demo_summary_r_csv\":   OUT / \"demo_summary_r.csv\",\n",
    "    \"ahei_combined_csv\":    DATA / \"ahei_combined.csv\",  # lives under data/\n",
    "}\n",
    "TABLES_FIGS = OUT  # you keep tables directly in output/\n",
    "\n",
    "# -------------------------\n",
    "# NHANES cycles\n",
    "# - Keep it simple: Cox is in R; use explicit lists\n",
    "# -------------------------\n",
    "CYCLES_MORTALITY = [\n",
    "    \"1999-2000\",\"2001-2002\",\"2003-2004\",\"2005-2006\",\"2007-2008\",\n",
    "    \"2009-2010\",\"2011-2012\",\"2013-2014\",\"2015-2016\",\"2017-2018\",\n",
    "]\n",
    "CYCLES_NONMORT = [\n",
    "    \"2017-March 2020 (pre-pandemic)\",  # P_DEMO.xpt style\n",
    "    \"August 2021–August 2023\",         # DEMO_L.xpt\n",
    "]\n",
    "CYCLES_ALL = CYCLES_MORTALITY + CYCLES_NONMORT\n",
    "\n",
    "# Optional suffix/prefix hints (use only if you need to load DEMO files by pattern)\n",
    "CYCLE_SUFFIX = {\n",
    "    \"1999-2000\": \"\",\n",
    "    \"2001-2002\": \"_B\", \"2003-2004\": \"_C\", \"2005-2006\": \"_D\",\n",
    "    \"2007-2008\": \"_E\", \"2009-2010\": \"_F\", \"2011-2012\": \"_G\",\n",
    "    \"2013-2014\": \"_H\", \"2015-2016\": \"_I\", \"2017-2018\": \"_J\",\n",
    "    \"2017-March 2020 (pre-pandemic)\": \"P_\",  # e.g., P_DEMO.xpt\n",
    "    \"August 2021–August 2023\": \"_L\",         # e.g., DEMO_L.xpt\n",
    "}\n",
    "def cycle_suffix(label: str) -> str:\n",
    "    return CYCLE_SUFFIX.get(label, \"\")\n",
    "\n",
    "# -------------------------\n",
    "# Small helpers\n",
    "# -------------------------\n",
    "def z(x):\n",
    "    x = pd.Series(x, dtype=\"float64\")\n",
    "    return (x - x.mean(skipna=True)) / x.std(skipna=True)\n",
    "\n",
    "def combine_wtmec(w, n_cycles: int):\n",
    "    \"\"\"\n",
    "    When stacking cycles, divide 2-year MEC weights by the number of\n",
    "    2-year cycles actually included in the stack you’re building.\n",
    "    \"\"\"\n",
    "    return w / float(n_cycles)\n",
    "\n",
    "def list_existing(paths):\n",
    "    \"\"\"Quickly check which paths exist (debug helper).\"\"\"\n",
    "    return {k: (p if p.exists() else None) for k, p in paths.items()}\n",
    "\n",
    "# -------------------------\n",
    "# Display prefs\n",
    "# -------------------------\n",
    "pd.options.display.max_rows = 60\n",
    "pd.options.display.max_columns = 120\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"Bootstrap loaded.\")\n",
    "print(\"ROOT:\", ROOT)\n",
    "print(\"Data dir exists:\", DATA.exists())\n",
    "print(\"Output dir exists:\", OUT.exists())\n",
    "print(\"Mortality cycles:\", CYCLES_MORTALITY)\n",
    "print(\"Non-mortality cycles:\", CYCLES_NONMORT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b53c4c7-ff92-45cb-b330-d34c7102b204",
   "metadata": {},
   "source": [
    "<h2>preview sas code and data </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68500809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidates: [PosixPath('/Users/dengshuyue/Desktop/SDOH/analysis/data/household_size/DEMO_I.xpt'), PosixPath('/Users/dengshuyue/Desktop/SDOH/analysis/data/nhanes_deit/DEMO_I.xpt')]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEQN</th>\n",
       "      <th>SDDSRVYR</th>\n",
       "      <th>RIDSTATR</th>\n",
       "      <th>RIAGENDR</th>\n",
       "      <th>RIDAGEYR</th>\n",
       "      <th>RIDAGEMN</th>\n",
       "      <th>RIDRETH1</th>\n",
       "      <th>RIDRETH3</th>\n",
       "      <th>RIDEXMON</th>\n",
       "      <th>RIDEXAGM</th>\n",
       "      <th>DMQMILIZ</th>\n",
       "      <th>DMQADFC</th>\n",
       "      <th>DMDBORN4</th>\n",
       "      <th>DMDCITZN</th>\n",
       "      <th>DMDYRSUS</th>\n",
       "      <th>DMDEDUC3</th>\n",
       "      <th>DMDEDUC2</th>\n",
       "      <th>DMDMARTL</th>\n",
       "      <th>RIDEXPRG</th>\n",
       "      <th>SIALANG</th>\n",
       "      <th>SIAPROXY</th>\n",
       "      <th>SIAINTRP</th>\n",
       "      <th>FIALANG</th>\n",
       "      <th>FIAPROXY</th>\n",
       "      <th>FIAINTRP</th>\n",
       "      <th>MIALANG</th>\n",
       "      <th>MIAPROXY</th>\n",
       "      <th>MIAINTRP</th>\n",
       "      <th>AIALANGA</th>\n",
       "      <th>DMDHHSIZ</th>\n",
       "      <th>DMDFMSIZ</th>\n",
       "      <th>DMDHHSZA</th>\n",
       "      <th>DMDHHSZB</th>\n",
       "      <th>DMDHHSZE</th>\n",
       "      <th>DMDHRGND</th>\n",
       "      <th>DMDHRAGE</th>\n",
       "      <th>DMDHRBR4</th>\n",
       "      <th>DMDHREDU</th>\n",
       "      <th>DMDHRMAR</th>\n",
       "      <th>DMDHSEDU</th>\n",
       "      <th>WTINT2YR</th>\n",
       "      <th>WTMEC2YR</th>\n",
       "      <th>SDMVPSU</th>\n",
       "      <th>SDMVSTRA</th>\n",
       "      <th>INDHHIN2</th>\n",
       "      <th>INDFMIN2</th>\n",
       "      <th>INDFMPIR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83732.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.397605e-79</td>\n",
       "      <td>5.397605e-79</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>134671.370419</td>\n",
       "      <td>135629.507405</td>\n",
       "      <td>1.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>83733.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.397605e-79</td>\n",
       "      <td>5.397605e-79</td>\n",
       "      <td>5.397605e-79</td>\n",
       "      <td>1.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24328.560239</td>\n",
       "      <td>25282.425927</td>\n",
       "      <td>1.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>83734.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.397605e-79</td>\n",
       "      <td>5.397605e-79</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12400.008522</td>\n",
       "      <td>12575.838818</td>\n",
       "      <td>1.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>83735.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.397605e-79</td>\n",
       "      <td>5.397605e-79</td>\n",
       "      <td>5.397605e-79</td>\n",
       "      <td>2.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>102717.995647</td>\n",
       "      <td>102078.634508</td>\n",
       "      <td>1.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>83736.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.397605e-79</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>5.397605e-79</td>\n",
       "      <td>2.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17627.674984</td>\n",
       "      <td>18234.736219</td>\n",
       "      <td>2.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      SEQN  SDDSRVYR  RIDSTATR  RIAGENDR  RIDAGEYR  RIDAGEMN  RIDRETH1  \\\n",
       "0  83732.0       9.0       2.0       1.0      62.0       NaN       3.0   \n",
       "1  83733.0       9.0       2.0       1.0      53.0       NaN       3.0   \n",
       "2  83734.0       9.0       2.0       1.0      78.0       NaN       3.0   \n",
       "3  83735.0       9.0       2.0       2.0      56.0       NaN       3.0   \n",
       "4  83736.0       9.0       2.0       2.0      42.0       NaN       4.0   \n",
       "\n",
       "   RIDRETH3  RIDEXMON  RIDEXAGM  DMQMILIZ  DMQADFC  DMDBORN4  DMDCITZN  \\\n",
       "0       3.0       1.0       NaN       2.0      NaN       1.0       1.0   \n",
       "1       3.0       1.0       NaN       2.0      NaN       2.0       2.0   \n",
       "2       3.0       2.0       NaN       1.0      2.0       1.0       1.0   \n",
       "3       3.0       2.0       NaN       2.0      NaN       1.0       1.0   \n",
       "4       4.0       2.0       NaN       2.0      NaN       1.0       1.0   \n",
       "\n",
       "   DMDYRSUS  DMDEDUC3  DMDEDUC2  DMDMARTL  RIDEXPRG  SIALANG  SIAPROXY  \\\n",
       "0       NaN       NaN       5.0       1.0       NaN      1.0       2.0   \n",
       "1       7.0       NaN       3.0       3.0       NaN      1.0       2.0   \n",
       "2       NaN       NaN       3.0       1.0       NaN      1.0       2.0   \n",
       "3       NaN       NaN       5.0       6.0       NaN      1.0       2.0   \n",
       "4       NaN       NaN       4.0       3.0       1.0      1.0       2.0   \n",
       "\n",
       "   SIAINTRP  FIALANG  FIAPROXY  FIAINTRP  MIALANG  MIAPROXY  MIAINTRP  \\\n",
       "0       2.0      1.0       2.0       2.0      1.0       2.0       2.0   \n",
       "1       2.0      1.0       2.0       2.0      1.0       2.0       2.0   \n",
       "2       2.0      1.0       2.0       2.0      1.0       2.0       2.0   \n",
       "3       2.0      1.0       2.0       2.0      1.0       2.0       2.0   \n",
       "4       2.0      1.0       2.0       2.0      1.0       2.0       2.0   \n",
       "\n",
       "   AIALANGA  DMDHHSIZ  DMDFMSIZ      DMDHHSZA      DMDHHSZB      DMDHHSZE  \\\n",
       "0       1.0       2.0       2.0  5.397605e-79  5.397605e-79  1.000000e+00   \n",
       "1       1.0       1.0       1.0  5.397605e-79  5.397605e-79  5.397605e-79   \n",
       "2       NaN       2.0       2.0  5.397605e-79  5.397605e-79  2.000000e+00   \n",
       "3       1.0       1.0       1.0  5.397605e-79  5.397605e-79  5.397605e-79   \n",
       "4       1.0       5.0       5.0  5.397605e-79  2.000000e+00  5.397605e-79   \n",
       "\n",
       "   DMDHRGND  DMDHRAGE  DMDHRBR4  DMDHREDU  DMDHRMAR  DMDHSEDU       WTINT2YR  \\\n",
       "0       1.0      62.0       1.0       5.0       1.0       3.0  134671.370419   \n",
       "1       1.0      53.0       2.0       3.0       3.0       NaN   24328.560239   \n",
       "2       2.0      79.0       1.0       3.0       1.0       3.0   12400.008522   \n",
       "3       2.0      56.0       1.0       5.0       6.0       NaN  102717.995647   \n",
       "4       2.0      42.0       1.0       4.0       3.0       NaN   17627.674984   \n",
       "\n",
       "        WTMEC2YR  SDMVPSU  SDMVSTRA  INDHHIN2  INDFMIN2  INDFMPIR  \n",
       "0  135629.507405      1.0     125.0      10.0      10.0      4.39  \n",
       "1   25282.425927      1.0     125.0       4.0       4.0      1.32  \n",
       "2   12575.838818      1.0     131.0       5.0       5.0      1.51  \n",
       "3  102078.634508      1.0     131.0      10.0      10.0      5.00  \n",
       "4   18234.736219      2.0     126.0       7.0       7.0      1.23  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SEQN', 'SDDSRVYR', 'RIDSTATR', 'RIAGENDR', 'RIDAGEYR', 'RIDAGEMN', 'RIDRETH1', 'RIDRETH3', 'RIDEXMON', 'RIDEXAGM', 'DMQMILIZ', 'DMQADFC', 'DMDBORN4', 'DMDCITZN', 'DMDYRSUS', 'DMDEDUC3', 'DMDEDUC2', 'DMDMARTL', 'RIDEXPRG', 'SIALANG', 'SIAPROXY', 'SIAINTRP', 'FIALANG', 'FIAPROXY', 'FIAINTRP', 'MIALANG', 'MIAPROXY', 'MIAINTRP', 'AIALANGA', 'DMDHHSIZ', 'DMDFMSIZ', 'DMDHHSZA', 'DMDHHSZB', 'DMDHHSZE', 'DMDHRGND', 'DMDHRAGE', 'DMDHRBR4', 'DMDHREDU', 'DMDHRMAR', 'DMDHSEDU', 'WTINT2YR', 'WTMEC2YR', 'SDMVPSU', 'SDMVSTRA', 'INDHHIN2', 'INDFMIN2', 'INDFMPIR']\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "BASE = Path(\"/Users/dengshuyue/Desktop/SDOH/analysis/data\")\n",
    "\n",
    "# Find DEMO for 2015–2016 (\"I\")\n",
    "cands = list(BASE.rglob(\"DEMO_I.*\"))\n",
    "print(\"Candidates:\", cands)\n",
    "\n",
    "if not cands:\n",
    "    # fallback: list any DEMO files you have\n",
    "    print(\"Any DEMO files I can see:\")\n",
    "    print(list(BASE.rglob(\"DEMO*.*\")))\n",
    "else:\n",
    "    p = cands[0]\n",
    "    if p.suffix.lower() == \".xpt\":\n",
    "        df = pd.read_sas(p, format=\"xport\")  # NHANES XPT format\n",
    "    elif p.suffix.lower() == \".sas7bdat\":\n",
    "        import pyreadstat\n",
    "        df, _ = pyreadstat.read_sas7bdat(p)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown extension: {p.suffix}\")\n",
    "\n",
    "    pd.set_option(\"display.max_columns\", None)\n",
    "    display(df.head())\n",
    "    print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef01a718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SEQN', 'DAYS', 'DR12IFDC', 'WTDRD1', 'DR12DRST', 'SDDSRVYR', 'RIAGENDR', 'RIDAGEYR', 'RIDRETH1', 'DMDEDUC3', 'DMDEDUC2', 'INDFMPIR', 'DMDHREDU', 'WTINT2YR', 'WTMEC2YR', 'SDMVPSU', 'SDMVSTRA', 'DR12IKC2', 'CYCLE', 'WTDR2D', 'DR12FS', 'DMDHREDZ', 'age', 'race', 'edu', 'pedu', 'Incm', 'incm2', 'include', 'Weight16a', 'cycles', 'sex', 'age1', 'age2', 'age3', 'race2', 'race3', 'race4', 'weekend', '_NAME_', '_LABEL_', 'DRDAY1', 'DRDAY2', 'tkal1', 'tkal2', 'Tcal', 'DR12DAY', 'kcal1', 'kcal2', 'kcal3', 'kcal4', 'kcal12', 'wt1', 'wt2', 'wt3', 'wt4', 'wt12', 'pcte1', 'pcte2', 'pcte3', 'pcte4', 'pcte12', 'pctg1', 'pctg2', 'pctg3', 'pctg4', 'pctg12', 'kcals2', 'kcals5', 'kcals6', 'kcals9', 'kcals13', 'kcals14', 'kcals17', 'kcals20', 'kcals21', 'kcals22', 'kcals23', 'kcals25', 'kcals28', 'kcals29', 'kcals33', 'kcals36', 'kcals39', 'kcals41', 'kcals3', 'kcals37', 'kcals38', 'kcals40', 'kcals42', 'kcals1', 'kcals10', 'kcals16', 'kcals24', 'kcals15', 'kcals18', 'kcals7', 'kcals8', 'kcals19', 'kcals46', 'kcals44', 'kcals45', 'kcals12', 'kcals27', 'kcals34', 'kcals30', 'kcals26', 'kcals43', 'kcals214', 'kcals215', 'kcals32', 'kcals31', 'wts2', 'wts5', 'wts6', 'wts9', 'wts13', 'wts14', 'wts17', 'wts20', 'wts21', 'wts22', 'wts23', 'wts25', 'wts28', 'wts29', 'wts33', 'wts36', 'wts39', 'wts41', 'wts3', 'wts37', 'wts38', 'wts40', 'wts42', 'wts1', 'wts10', 'wts16', 'wts24', 'wts15', 'wts18', 'wts7', 'wts8', 'wts19', 'wts46', 'wts44', 'wts45', 'wts12', 'wts27', 'wts34', 'wts30', 'wts26', 'wts43', 'wts214', 'wts215', 'wts32', 'wts31', 'pctes2', 'pctes5', 'pctes6', 'pctes9', 'pctes13', 'pctes14', 'pctes17', 'pctes20', 'pctes21', 'pctes22', 'pctes23', 'pctes25', 'pctes28', 'pctes29', 'pctes33', 'pctes36', 'pctes39', 'pctes41', 'pctes3', 'pctes37', 'pctes38', 'pctes40', 'pctes42', 'pctes1', 'pctes10', 'pctes16', 'pctes24', 'pctes15', 'pctes18', 'pctes7', 'pctes8', 'pctes19', 'pctes46', 'pctes44', 'pctes45', 'pctes12', 'pctes27', 'pctes34', 'pctes30', 'pctes26', 'pctes43', 'pctes214', 'pctes215', 'pctes32', 'pctes31', 'pctgs2', 'pctgs5', 'pctgs6', 'pctgs9', 'pctgs13', 'pctgs14', 'pctgs17', 'pctgs20', 'pctgs21', 'pctgs22', 'pctgs23', 'pctgs25', 'pctgs28', 'pctgs29', 'pctgs33', 'pctgs36', 'pctgs39', 'pctgs41', 'pctgs3', 'pctgs37', 'pctgs38', 'pctgs40', 'pctgs42', 'pctgs1', 'pctgs10', 'pctgs16', 'pctgs24', 'pctgs15', 'pctgs18', 'pctgs7', 'pctgs8', 'pctgs19', 'pctgs46', 'pctgs44', 'pctgs45', 'pctgs12', 'pctgs27', 'pctgs34', 'pctgs30', 'pctgs26', 'pctgs43', 'pctgs214', 'pctgs215', 'pctgs32', 'pctgs31', 'kcals4', 'kcals11', 'kcals35', 'kcals47', 'kcals48', 'kcals49', 'kcals50', 'kcals51', 'pctgs4', 'pctgs11', 'pctgs35', 'pctgs47', 'pctgs48', 'pctgs49', 'pctgs50', 'pctgs51', 'pctes4', 'pctes11', 'pctes35', 'pctes47', 'pctes48', 'pctes49', 'pctes50', 'pctes51', 'wts4', 'wts11', 'wts35', 'wts47', 'wts48', 'wts49', 'wts50', 'wts51', 'Pctes1a', 'Pctes6a', 'Pctgs1a', 'Pctgs6a', 'Pctgs10b', 'Pctgs112', 'Kcals1a', 'Kcals6a', 'Kcals10b', 'Kcals112', 'Kcals10a', 'Pctes10a', 'wts10a', 'Pctgs10a', 'Pctes10b', 'Pctes112', 'Kcals2a', 'Pctes2a', 'wts2a', 'Pctgs2a', 'Kcals13a', 'Pctes13a', 'wts13a', 'Pctgs13a', 'Pctes23a', 'Kcals23a', 'Pctgs23a', 'Kcals38a', 'Pctes38a', 'wts38a', 'Pctgs38a', 'Pctes25a', 'Pctgs25a', 'Kcals25a', 'Pctes24a', 'Pctgs24a', 'Kcals24a', 'Pctes30a', 'Pctes30b', 'Pctgs30a', 'Pctgs30b', 'Kcals30a', 'Kcals30b', 'Pctes37b', 'pctes44a']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Path to your .sas7bdat file\n",
    "file_path = \"/Users/dengshuyue/Desktop/SDOH/analysis/data/less_important/gg.sas7bdat\"\n",
    "\n",
    "# Load the dataset using pandas (requires pyreadstat)\n",
    "df = pd.read_sas(file_path, format=\"sas7bdat\")\n",
    "\n",
    "# Preview the data\n",
    "pd.set_option('display.max_columns', None)\n",
    "df.head()\n",
    "\n",
    "# List all column names\n",
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a1ea600",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "libname data \"C:\\Users\\lwang18\\Box\\Projects\\5_UPF_Mortality\\data\";\n",
      "*%let home=C:\\Users\\LWANG18\\Box\\Projects\\5_UPF_Mortality\\results_revision ; \n",
      "%let home= C:\\Users\\lwang18\\Box\\Projects\\Kroger project\\Data for analysis\\Scores;\n",
      " %let path= C:\\Users\\lwang18\\Box\\Projects\\Kroger project\\Data for analysis\\Scores;\n",
      "\n",
      "%let home= C:\\Users\\lwang18\\OneDrive - Tufts\\Desktop\\Projects\\Food Insecurity,;\n",
      "libname out \"C:\\Users\\lwang18\\OneDrive - Tufts\\Desktop\\Projects\\Food Insecurity,\";\n",
      "\n",
      "libname NHANES \"C:\\Users\\LWANG18\\Box\\NHANES_Lu\" ;\n",
      "\n",
      "/** main analysis **/\n",
      "\n",
      "%macro cox(data, dvar, evars, covars, death , out);\n",
      "ods select all ; \n",
      "ODS OUTPUT PARAMETERESTIMATES=r0; \n",
      "proc surveyphreg data=&data;\n",
      "\tstrata sdmvstra;\n",
      "\tcluster sdmvpsu;\n",
      "\tweight wt;\n",
      "\tclass    sex (ref=\"1\") race edu(ref=\"1\") smk  pir(ref=\"3\") SNAP(ref=\"0\") FS ins2(ref=\"1\") ins i_FCS_sdq hei2015q(ref=\"3\") marriage  hoq065/param=ref;\n",
      "\tmodel py*&death(0)= &dvar &covars /rl ties=breslow;\n",
      "run ;\n",
      "data r0 ; set r0 ; \n",
      "HRCL=compress(round(hazardratio,0.01)||\"(\"||round(HRLOWerCL,.01)||\",\" ||round(HRupperCL,0.01)||\")\")  ; \n",
      "outcome=\"&death.\" ;\n",
      "predictor=\"&dvar.\";\n",
      "explainvar=\"null\" ;\n",
      "model=\"&out.\"; \n",
      "run; \n",
      "proc append data=r0  base=&out force; run;\n",
      "\n",
      "%let i=1 ;\n",
      "%do %while(%scan(&evars,&i) ne ) ;\n",
      "%let evar=%scan(&evars,&i) ;\n",
      "\n",
      "ods select all ; \n",
      "ODS OUTPUT PARAMETERESTIMATES=r1; \n",
      "proc surveyphreg data=&data;\n",
      "\tstrata sdmvstra;\n",
      "\tcluster sdmvpsu;\n",
      "\tweight wt;\n",
      "\tclass    sex (ref=\"1\") race edu(ref=\"1\") smk  pir(ref=\"3\") SNAP(ref=\"0\") FS ins2(ref=\"1\") ins i_FCS_sdq hei2015q(ref=\"3\") marriage hoq065(ref=\"1\")/param=ref;\n",
      "\tmodel py*&death(0)= &dvar &evar &covars /rl ties=breslow;\n",
      "run ;\n",
      "\n",
      "data resc&death.&i ; \n",
      "set r1; \n",
      "HRCL=compress(round(hazardratio,0.01)||\"(\"||round(HRLOWerCL,.01)||\",\" ||round(HRupperCL,0.01)||\")\")  ; \n",
      "outcome=\"&death.\" ;\n",
      "predictor=\"&dvar.\";\n",
      "explainvar=\"&evar.\" ;\n",
      "model=\"&out.\"; \n",
      "run; \n",
      "proc append data=resc&death.&i  base=&out force; run;\n",
      " \n",
      "%let i=%eval(&i+1) ;\n",
      "%end ;\n",
      "%mend ; \n",
      "\n",
      " \n",
      "%macro cox1(data, dvar, evars, covars, death , out, label);\n",
      "OPTION SPOOL ; \n",
      "ods select all ; \n",
      "ODS OUTPUT PARAMETERESTIMATES=r1; \n",
      "proc surveyphreg data=&data;\n",
      "\tstrata sdmvstra;\n",
      "\tcluster sdmvpsu;\n",
      "\tweight wt;\n",
      "\tclass    sex (ref=\"1\") race edu(ref=\"1\") smk  pir(ref=\"3\") SNAP(ref=\"0\") FS ins ins2(ref=\"1\") i_FCS_sdq hei2015q(ref=\"3\") /param=ref;\n",
      "\tmodel py*&death(0)= &dvar &evars &covars /rl ties=breslow;\n",
      "run ;\n",
      "\n",
      "data resc&death ; \n",
      "set r1; \n",
      "HRCL=compress(round(hazardratio,0.01)||\"(\"||round(HRLOWerCL,.01)||\",\" ||round(HRupperCL,0.01)||\")\")  ; \n",
      "outcome=\"&death.\" ;\n",
      "predictor=\"&dvar.\";\n",
      "explainvar=&label ; \n",
      "model=\"&out.\"; \n",
      "run; \n",
      "proc append data=resc&death  base=&out force; run;\n",
      "%mend ; \n",
      "\n",
      "*%cox(score_mort1,pir, fs  i_FCS_sdq SNAP ins ins2, ridageyr sex race   , mortstat, out_model0 );\n",
      "%cox(score_mort,pir,  hei2015q i_FCS_sdq fs SNAP ins ins2 marriage hoq065 unemployment unemployment2  ,  ridageyr sex race  edu , mortstat, out_model1 );\n",
      "%cox1(score_mort,pir, i_FCS_sdq fs , ridageyr sex race  edu , mortstat, out_model1 , \"comb1\");\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq SNAP ins, ridageyr sex race  edu , mortstat, out_model1 , \"all1\");\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq SNAP ins2, ridageyr sex race  edu , mortstat, out_model1 , \"all2\");\n",
      "\n",
      "/**age, sex, race, edu, ins as the base model*/\n",
      "%cox(score_mort,pir, hei2015q i_FCS_sdq fs  SNAP marriage hoq065 unemployment unemployment2,  ridageyr sex race  edu ins , mortstat, out_model11 );  \n",
      "%cox1(score_mort,pir, fs i_FCS_sdq, ridageyr sex race  edu ins, mortstat, out_model11 , \"comb1\");\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq SNAP ins, ridageyr sex race  edu ins, mortstat, out_model11 , \"all1\");\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq SNAP ins2, ridageyr sex race  edu ins, mortstat, out_model11 , \"all2\");\n",
      "\n",
      "/**age, sex, race, edu, lifestyles as the base model*/\n",
      "%cox(score_mort,pir, hei2015q i_FCS_sdq fs  SNAP ins ins2 marriage hoq065 unemployment unemployment2, ridageyr sex race  edu smk\tmet_hr perE_alco , mortstat, out_model2 );\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq, ridageyr sex race  edu smk\tmet_hr perE_alco, mortstat, out_model2 , \"comb1\");\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq SNAP ins, ridageyr sex race  edu smk\tmet_hr perE_alco, mortstat, out_model2 , \"all1\");\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq SNAP ins2, ridageyr sex race  edu smk\tmet_hr perE_alco, mortstat, out_model2 , \"all2\");\n",
      "\n",
      "/**age, sex, race, edu, lifestyles, insurance as the base model*/\n",
      "%cox(score_mort,pir, hei2015q i_FCS_sdq fs  SNAP marriage hoq065 unemployment unemployment2, ridageyr sex race  edu smk\tmet_hr perE_alco ins , mortstat, out_model21 );\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq, ridageyr sex race  edu smk\tmet_hr perE_alco ins, mortstat, out_model21 , \"comb1\");\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq SNAP ins, ridageyr sex race  edu smk\tmet_hr perE_alco ins, mortstat, out_model21 , \"all1\");\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq SNAP ins2, ridageyr sex race  edu smk\tmet_hr perE_alco ins, mortstat, out_model21 , \"all2\");\n",
      "\n",
      "\n",
      "/**age, sex, race, edu, lifestyles, baseline health as the base model*/\n",
      "%cox(score_mort,pir, hei2015q i_FCS_sdq fs  SNAP ins ins2 marriage hoq065 unemployment unemployment2, ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl , mortstat, out_model3 );\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq, ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl , mortstat, out_model3 , \"comb1\");\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq SNAP ins, ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl , mortstat, out_model3 , \"all1\");\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq SNAP ins2, ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl , mortstat, out_model3 , \"all2\");\n",
      "\n",
      "\n",
      "/**age, sex, race, edu, lifestyles, baseline health, insurance, as the base model*/\n",
      "%cox(score_mort,pir, hei2015q i_FCS_sdq fs  SNAP marriage hoq065 unemployment unemployment2, ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl ins, mortstat , out_model31 );\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq, ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl ins, mortstat, out_model31 , \"comb1\");\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq SNAP ins, ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl ins, mortstat, out_model31 , \"all1\");\n",
      "%cox1(score_mort,pir, fs i_FCS_sdq SNAP ins2, ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl ins, mortstat, out_model31 , \"all2\");\n",
      "\n",
      "\n",
      "data out.out_models_pir ; set out_model1 out_model11 out_model2 out_model21 out_model3 out_model31; run; \n",
      "\n",
      "\n",
      "\n",
      "PROC expORT data= WORK.out_models_pir \n",
      "            outFILE= \"&home\\allmodelspir.xlsx\" \n",
      "            DBMS=xlsx REPLACE;\n",
      " *    GETNAMES=YES;\n",
      "RUN;\n",
      "\n",
      "/***Predictor=FS **/\n",
      "\n",
      "*%cox(score_mort1,pir, fs  i_FCS_sdq SNAP ins ins2, ridageyr sex race   , mortstat, out_model0 );\n",
      "%cox(score_mort,fs, hei2015q i_FCS_sdq pir SNAP ins ins2 marriage hoq065 unemployment unemployment2,  ridageyr sex race  edu , mortstat, out_model1a );\n",
      "\n",
      "%cox(score_mort,fs, hei2015q i_FCS_sdq, ridageyr sex race edu ins marriage unemployment2 hoq065 pir snap , mortstat, out_model11a );  \n",
      "\n",
      "%cox(score_mort,fs, hei2015q i_FCS_sdq pir SNAP ins ins2 marriage hoq065 unemployment unemployment2, ridageyr sex race edu smk met_hr perE_alco , mortstat, out_model2a );  \n",
      "\n",
      "%cox(score_mort,fs, hei2015q i_FCS_sdq, ridageyr sex race edu smk met_hr perE_alco ins marriage unemployment2 hoq065 pir snap , mortstat, out_model21a);  \n",
      "/**age, sex, race, edu, lifestyles, baseline health, insurance, as the base model*/\n",
      "\n",
      "%cox(score_mort,fs, hei2015q i_FCS_sdq pir  SNAP ins ins2 marriage hoq065 unemployment unemployment2, ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl , mortstat , out_model3a );\n",
      "\n",
      "%cox1(score_mort,fs, hei2015q pir  SNAP ins marriage hoq065 unemployment , ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl , mortstat, out_model4 , \"all1\");\n",
      "\n",
      "\n",
      "data out_models_fs ; set out_model1a out_model11a out_model2a out_model21a  out_model3a out_model4; run; \n",
      "\n",
      "\n",
      "PROC expORT data= WORK.out_models_fs \n",
      "            outFILE= \"&home\\allmodelsfs.xlsx\" \n",
      "            DBMS=xlsx REPLACE;\n",
      " *    GETNAMES=YES;\n",
      "RUN;\n",
      "\n",
      "\n",
      "%cox(score_mort, hei2015q, fs pir SNAP ins ins2 marriage hoq065 unemployment unemployment2,  ridageyr sex race  edu , mortstat, out_model1b );\n",
      "\n",
      "%cox(score_mort, hei2015q, fs pir SNAP ins ins2 marriage hoq065 unemployment unemployment2, ridageyr sex race edu smk met_hr perE_alco , mortstat, out_model2b );  \n",
      "\n",
      "%cox(score_mort, hei2015q, fs pir  SNAP ins ins2 marriage hoq065 unemployment unemployment2, ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl , mortstat , out_model3b );\n",
      "\n",
      "%cox1(score_mort, hei2015q, fs pir  SNAP ins  , ridageyr sex race  edu smk\tmet_hr perE_alco , mortstat, out_model2b , \"all1\");\n",
      "\n",
      "\n",
      "%cox1(score_mort, hei2015q, fs pir  SNAP ins marriage hoq065 unemployment , ridageyr sex race  edu smk\tmet_hr perE_alco , mortstat, out_model2b , \"all2\");\n",
      "\n",
      "%cox1(score_mort, hei2015q, fs pir  SNAP ins  , ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl , mortstat, out_model3b , \"all1\");\n",
      "\n",
      "%cox1(score_mort, hei2015q, fs pir  SNAP ins marriage hoq065 unemployment , ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl , mortstat, out_model3b , \"all2\");\n",
      "\n",
      "%cox1(score_mort, hei2015q, fs pir  SNAP ins marriage hoq065 unemployment , ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl , mortstat, out_model3b , \"all3\");\n",
      "\n",
      "\n",
      "data data.out_models_hei ; set out_model1b out_model2b out_model3b; run; \n",
      "\n",
      "\n",
      "PROC expORT data= WORK.out_models_hei \n",
      "            outFILE= \"&home\\allmodelshei.xlsx\" \n",
      "            DBMS=xlsx REPLACE;\n",
      " *    GETNAMES=YES;\n",
      "RUN;\n",
      "\n",
      "\n",
      "%cox(score_mort, SNAP, hei2015q fs pir ins ins2 marriage hoq065 unemployment unemployment2,  ridageyr sex race  edu , mortstat, out_model1c );\n",
      "\n",
      "%cox(score_mort, SNAP, hei2015q fs pir ins ins2 marriage hoq065 unemployment unemployment2, ridageyr sex race edu smk met_hr perE_alco , mortstat, out_model2c );  \n",
      "\n",
      "%cox(score_mort, SNAP, hei2015q fs pir  ins ins2 marriage hoq065 unemployment unemployment2, ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl , mortstat , out_model3c );\n",
      "\n",
      "%cox1(score_mort, SNAP, hei2015q fs pir  ins  , ridageyr sex race  edu smk\tmet_hr perE_alco , mortstat, out_model2c , \"all1\");\n",
      "\n",
      "%cox1(score_mort, SNAP, hei2015q fs pir  ins marriage hoq065 unemployment , ridageyr sex race  edu smk\tmet_hr perE_alco , mortstat, out_model2c , \"all2\");\n",
      "\n",
      "%cox1(score_mort, SNAP, hei2015q fs pir  ins  , ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl , mortstat, out_model3c , \"all1\");\n",
      "\n",
      "%cox1(score_mort, SNAP, hei2015q fs pir  ins marriage hoq065 unemployment , ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl , mortstat, out_model3c , \"all2\");\n",
      "\n",
      "%cox1(score_mort, SNAP, hei2015q fs pir  ins marriage hoq065 unemployment2 , ridageyr sex race  edu smk\tmet_hr perE_alco dm_self CVD dm_rx\tchol_rx\t\n",
      "angina_rx Cancer lung_disease\tangina bmi\thba1c sbp\tdbp  hdl\tldl , mortstat, out_model3c , \"all3\");\n",
      "\n",
      "\n",
      "data data.out_models_SNAP ; set out_model1c out_model2c out_model3c; run; \n",
      "\n",
      "\n",
      "PROC expORT data= data.out_models_snap \n",
      "            outFILE= \"&home\\allmodelssnap.xlsx\" \n",
      "            DBMS=xlsx REPLACE;\n",
      " *    GETNAMES=YES;\n",
      "RUN;\n",
      "\n",
      "\n",
      "\n",
      "/*PROC expORT data= WORK.out_model0*/\n",
      "/*            outFILE= \"&home\\model0.xlsx\" */\n",
      "/*            DBMS=xlsx REPLACE;*/\n",
      "/* *    GETNAMES=YES;*/\n",
      "/*RUN;*/\n",
      "/*PROC expORT data= WORK.out_model1*/\n",
      "/*            outFILE= \"&home\\model1.xlsx\" */\n",
      "/*            DBMS=xlsx REPLACE;*/\n",
      "/* *    GETNAMES=YES;*/\n",
      "/*RUN;*/\n",
      "/*PROC expORT data= WORK.out_model2*/\n",
      "/*            outFILE= \"&home\\model2.xlsx\" */\n",
      "/*            DBMS=xlsx REPLACE;*/\n",
      "/* *    GETNAMES=YES;*/\n",
      "/*RUN;*/\n",
      "/**/\n",
      "/*PROC expORT data= WORK.out_model3*/\n",
      "/*            outFILE= \"&home\\model3.xlsx\" */\n",
      "/*            DBMS=xlsx REPLACE;*/\n",
      "/* *    GETNAMES=YES;*/\n",
      "/*RUN;*/\n",
      "\n",
      "\n",
      "******************************************************************************************************;\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to your .sas script file (SAS code, not dataset)\n",
    "file_path = \"/Users/dengshuyue/Desktop/SDOH/analysis/code/code_lu/Analysis1_COX_allcause.sas\"\n",
    "\n",
    "# Load the SAS script as plain text\n",
    "with open(file_path, \"r\") as file:\n",
    "    sas_code = file.read()\n",
    "\n",
    "# Optionally preview the first 30 lines\n",
    "sas_code_lines = sas_code.splitlines()\n",
    "for line in sas_code_lines[:300]:\n",
    "    print(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7908d2b4-cccd-4a07-bcae-4a6bed5e87b5",
   "metadata": {},
   "source": [
    "<h2>Step 1: NHANES demographic data (fetch and merge)</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdf967b3-dd39-49f5-b7e1-d1cdfc7f9026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total rows: 128809\n",
      "✅ Unique SEQN: 128809\n",
      "Columns: ['SEQN', 'RIDAGEYR', 'SDDSRVYR', 'CYCLE', 'MARRIAGE', 'MARRIAGE3']\n",
      "\n",
      "2021–2023 MARRIAGE3 counts:\n",
      " MARRIAGE3\n",
      "<NA>    4150\n",
      "1       4136\n",
      "2       2022\n",
      "3       1625\n",
      "Name: count, dtype: Int64\n"
     ]
    }
   ],
   "source": [
    "# %% Step 1: Paths and cycle mapping (matches your project)\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "BASE = Path(\"/Users/dengshuyue/Desktop/SDOH/analysis\")\n",
    "DATA: Path = BASE / \"data\"\n",
    "(DATA / \"nhanes_by_module\" / \"DEMO\").mkdir(parents=True, exist_ok=True)  # where we’ll save downloads\n",
    "\n",
    "# --- Candidate upstream URLs for DEMO files (multiple fallbacks per cycle) ---\n",
    "# Includes the “classic” /Nhanes/<cycle>/ path and the /Nchs/Data/Nhanes/Public/<year>/DataFiles/ path.\n",
    "def nhanes_url_candidates():\n",
    "    return {\n",
    "        \"1999-2000\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/1999/DataFiles/DEMO.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/1999-2000/DEMO.XPT\",\n",
    "        ],\n",
    "        \"2001-2002\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2001/DataFiles/DEMO_B.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2001-2002/DEMO_B.XPT\",\n",
    "        ],\n",
    "        \"2003-2004\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2003/DataFiles/DEMO_C.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2003-2004/DEMO_C.XPT\",\n",
    "        ],\n",
    "        \"2005-2006\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2005/DataFiles/DEMO_D.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2005-2006/DEMO_D.XPT\",\n",
    "        ],\n",
    "        \"2007-2008\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2007/DataFiles/DEMO_E.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2007-2008/DEMO_E.XPT\",\n",
    "        ],\n",
    "        \"2009-2010\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2009/DataFiles/DEMO_F.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2009-2010/DEMO_F.XPT\",\n",
    "        ],\n",
    "        \"2011-2012\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2011/DataFiles/DEMO_G.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2011-2012/DEMO_G.XPT\",\n",
    "        ],\n",
    "        \"2013-2014\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2013/DataFiles/DEMO_H.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2013-2014/DEMO_H.XPT\",\n",
    "        ],\n",
    "        \"2015-2016\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2015/DataFiles/DEMO_I.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2015-2016/DEMO_I.XPT\",\n",
    "        ],\n",
    "        \"2017-2018\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2017/DataFiles/DEMO_J.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2017-2018/DEMO_J.XPT\",\n",
    "        ],\n",
    "        # Special combined pre-pandemic release\n",
    "        \"2017-March 2020 (pre-pandemic)\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2017/DataFiles/P_DEMO.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2017-2020/P_DEMO.XPT\",\n",
    "        ],\n",
    "        # August 2021–August 2023 (DEMO_L; include Q as fallback just in case)\n",
    "        \"August 2021–August 2023\": [\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2021/DataFiles/DEMO_L.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2021-2023/DEMO_L.XPT\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2021/DataFiles/DEMO_Q.xpt\",\n",
    "            \"https://wwwn.cdc.gov/Nchs/Nhanes/2021-2023/DEMO_Q.XPT\",\n",
    "        ],\n",
    "    }\n",
    "\n",
    "NHANES_URLS = nhanes_url_candidates()\n",
    "\n",
    "# Build local filename candidates per cycle (try these names before downloading)\n",
    "def candidates_from_urls(urls):\n",
    "    out, seen = [], set()\n",
    "    for url in urls:\n",
    "        fname = Path(url).name\n",
    "        for v in (fname, fname.upper(), fname.lower(), fname.capitalize()):\n",
    "            if v not in seen:\n",
    "                seen.add(v)\n",
    "                out.append(v)\n",
    "    return out\n",
    "\n",
    "LOCAL_CANDIDATES = {cycle: candidates_from_urls(urls)\n",
    "                    for cycle, urls in NHANES_URLS.items()}\n",
    "\n",
    "# %% Step 2: helpers (local search + download + reader)\n",
    "def find_first_under_data(patterns):\n",
    "    \"\"\"Search recursively under DATA for any of the provided filenames (case-sensitive per pattern list).\"\"\"\n",
    "    for pattern in patterns:\n",
    "        hits = list(DATA.rglob(pattern))\n",
    "        if hits:\n",
    "            return hits[0]\n",
    "    return None\n",
    "\n",
    "# define fuction to fetch from CDC web\n",
    "def download_to(path: Path, url: str, timeout=90):\n",
    "    \"\"\"Download URL -> path with a basic retry; return local path.\"\"\"\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    headers = {\"User-Agent\": \"nhanes-fetch/1.0 (+https://cdc.gov)\"}\n",
    "    last_err = None\n",
    "    for attempt in range(2):  # two tries per URL\n",
    "        try:\n",
    "            resp = requests.get(url, headers=headers, timeout=timeout, stream=True)\n",
    "            resp.raise_for_status()\n",
    "            tmp = path.with_suffix(path.suffix + \".downloading\")\n",
    "            with open(tmp, \"wb\") as f:\n",
    "                for chunk in resp.iter_content(chunk_size=1 << 15):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "            tmp.rename(path)\n",
    "            return path\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "    raise last_err\n",
    "\n",
    "def ensure_demo_file(cycle_label: str) -> Path:\n",
    "    \"\"\"Return a local DEMO file path for the cycle (search first, then download from known URLs).\"\"\"\n",
    "    local = find_first_under_data(LOCAL_CANDIDATES[cycle_label])\n",
    "    if local:\n",
    "        return local\n",
    "    for url in NHANES_URLS[cycle_label]:\n",
    "        out = DATA / \"nhanes_by_module\" / \"DEMO\" / Path(url).name\n",
    "        try:\n",
    "            print(f\"⬇️  Downloading {cycle_label} from {url}\")\n",
    "            return download_to(out, url)\n",
    "        except Exception as e:\n",
    "            print(f\"   ⚠️ Download failed from {url}: {e}\")\n",
    "    raise FileNotFoundError(f\"No DEMO file found or downloaded for {cycle_label}\")\n",
    "\n",
    "def read_demo_file(p: Path) -> pd.DataFrame:\n",
    "    \"\"\"Read DEMO file (.xpt preferred, .sas7bdat fallback).\"\"\"\n",
    "    if p.suffix.lower() == \".xpt\":\n",
    "        # Prefer pyreadstat if available (faster/labels), else pandas\n",
    "        try:\n",
    "            import pyreadstat\n",
    "            df, _ = pyreadstat.read_xport(p)\n",
    "        except Exception:\n",
    "            df = pd.read_sas(p, format=\"xport\")\n",
    "    elif p.suffix.lower() == \".sas7bdat\":\n",
    "        try:\n",
    "            import pyreadstat\n",
    "            df, _ = pyreadstat.read_sas7bdat(p)\n",
    "        except Exception:\n",
    "            df = pd.read_sas(p, format=\"sas7bdat\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {p.suffix}\")\n",
    "    return df\n",
    "\n",
    "# Which cycles to include in this DEMO build (OK to include through 2023 here)\n",
    "DEMO_CYCLES = [\n",
    "    \"1999-2000\",\"2001-2002\",\"2003-2004\",\"2005-2006\",\"2007-2008\",\n",
    "    \"2009-2010\",\"2011-2012\",\"2013-2014\",\"2015-2016\",\"2017-2018\",\n",
    "    \"2017-March 2020 (pre-pandemic)\",\"August 2021–August 2023\",\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- helpers to recode marital status ---\n",
    "def recode_L_to_4(s):\n",
    "    # DMDMARTL codes: 1=Married, 2=Never, 3=Widowed, 4=Divorced, 5=Separated, 6=Living with partner, 77/99=DK/Ref\n",
    "    s = pd.to_numeric(s, errors=\"coerce\")\n",
    "    out = pd.Series(pd.NA, index=s.index, dtype=\"Int64\")\n",
    "    out = out.where(~s.isin([1, 6]), 1)   # married/partner\n",
    "    out = out.where(~s.isin([3, 4]), 2)   # widowed/divorced\n",
    "    out = out.where(~(s == 2), 3)         # never married\n",
    "    out = out.where(~(s == 5), 4)         # separated\n",
    "    out = out.where(~s.isin([77, 99]), pd.NA)\n",
    "    return out\n",
    "\n",
    "def recode_L_to_3(s):\n",
    "    # Collapse L into 3 categories to match Z:\n",
    "    # 1 = married/partner; 2 = previously married (widowed/divorced/separated); 3 = never\n",
    "    s = pd.to_numeric(s, errors=\"coerce\")\n",
    "    out = pd.Series(pd.NA, index=s.index, dtype=\"Int64\")\n",
    "    out = out.where(~s.isin([1, 6]), 1)\n",
    "    out = out.where(~s.isin([3, 4, 5]), 2)\n",
    "    out = out.where(~(s == 2), 3)\n",
    "    out = out.where(~s.isin([77, 99]), pd.NA)\n",
    "    return out\n",
    "\n",
    "def recode_Z_to_3(s):\n",
    "    # DMDMARTZ codes (2021–2023): 1=married/partner, 2=previously married, 3=never, 77/99=DK/Ref\n",
    "    s = pd.to_numeric(s, errors=\"coerce\")\n",
    "    out = pd.Series(pd.NA, index=s.index, dtype=\"Int64\")\n",
    "    out = out.where(~(s == 1), 1)\n",
    "    out = out.where(~(s == 2), 2)\n",
    "    out = out.where(~(s == 3), 3)\n",
    "    out = out.where(~s.isin([77, 99]), pd.NA)\n",
    "    return out\n",
    "\n",
    "def compute_marriage_cols(df_upper: pd.DataFrame):\n",
    "    \"\"\"Return (MARRIAGE, MARRIAGE3) given an uppercase-column DataFrame.\"\"\"\n",
    "    hasL = \"DMDMARTL\" in df_upper.columns\n",
    "    hasZ = \"DMDMARTZ\" in df_upper.columns\n",
    "    M4 = pd.Series(pd.NA, index=df_upper.index, dtype=\"Int64\")\n",
    "    M3 = pd.Series(pd.NA, index=df_upper.index, dtype=\"Int64\")\n",
    "    if hasL:\n",
    "        M4 = recode_L_to_4(df_upper[\"DMDMARTL\"])\n",
    "        M3 = recode_L_to_3(df_upper[\"DMDMARTL\"])\n",
    "    elif hasZ:\n",
    "        # Can't reconstruct a 4-cat from Z; leave M4 as NA\n",
    "        M3 = recode_Z_to_3(df_upper[\"DMDMARTZ\"])\n",
    "    return M4, M3\n",
    "\n",
    "# %% Step 3: Load (local-or-download), tag cycle, recode marriage per cycle, and stack\n",
    "demo_dfs, missing = [], []\n",
    "for cycle in DEMO_CYCLES:\n",
    "    try:\n",
    "        p = ensure_demo_file(cycle)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"⚠️ {e}\")\n",
    "        missing.append(cycle)\n",
    "        continue\n",
    "\n",
    "    df = read_demo_file(p)\n",
    "    df.columns = [c.upper() for c in df.columns]\n",
    "    df[\"CYCLE\"] = cycle\n",
    "\n",
    "    # Recode marital status for this cycle using whichever column exists\n",
    "    M4, M3 = compute_marriage_cols(df)\n",
    "    df[\"MARRIAGE\"]  = M4          # 4-category where available; NA for 2021–2023\n",
    "    df[\"MARRIAGE3\"] = M3          # 3-category available for ALL cycles\n",
    "\n",
    "    # Keep only relevant columns that exist\n",
    "    keep = [c for c in [\"SEQN\",\"RIDAGEYR\",\"SDDSRVYR\",\"CYCLE\",\"MARRIAGE\",\"MARRIAGE3\"] if c in df.columns]\n",
    "    demo_dfs.append(df[keep])\n",
    "\n",
    "if missing:\n",
    "    print(\"⚠️ Missing cycles (not found and not downloaded):\", missing)\n",
    "if not demo_dfs:\n",
    "    raise FileNotFoundError(\"No DEMO files were found or downloaded under your data/ directory.\")\n",
    "\n",
    "# %% Step 4: Combine\n",
    "demo9923 = pd.concat(demo_dfs, ignore_index=True).copy()\n",
    "\n",
    "# %% Step 5/6 merged: we already kept just the needed columns; print summary\n",
    "print(f\"✅ Total rows: {demo9923.shape[0]}\")\n",
    "print(f\"✅ Unique SEQN: {demo9923['SEQN'].nunique()}\")\n",
    "print(\"Columns:\", demo9923.columns.tolist())\n",
    "\n",
    "# Optional quick check: 2021–2023 now has MARRIAGE3 populated\n",
    "vc = demo9923.loc[demo9923[\"CYCLE\"]==\"August 2021–August 2023\",\"MARRIAGE3\"].value_counts(dropna=False)\n",
    "print(\"\\n2021–2023 MARRIAGE3 counts:\\n\", vc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "27920ff6-03a2-443f-a2a1-cc45131e2e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved DEMO with survey design → /Users/dengshuyue/Desktop/SDOH/analysis/interim/demo_9923.parquet\n",
      "✅ Total rows: 128809\n",
      "✅ Unique SEQN: 128809\n",
      "Columns: ['SEQN', 'RIDAGEYR', 'SDDSRVYR', 'SDMVPSU', 'SDMVSTRA', 'WTMEC2YR', 'CYCLE', 'MARRIAGE', 'MARRIAGE3']\n",
      "SDDSRVYR NA %: 0.0 %\n",
      "SDMVPSU NA %: 0.0 %\n",
      "SDMVSTRA NA %: 0.0 %\n",
      "WTMEC2YR NA %: 12.08 %\n",
      "\n",
      "2021–2023 MARRIAGE3 counts:\n",
      " MARRIAGE3\n",
      "<NA>    4150\n",
      "1       4136\n",
      "2       2022\n",
      "3       1625\n",
      "Name: count, dtype: Int64\n"
     ]
    }
   ],
   "source": [
    "# %% Step 3: Load (local-or-download), tag cycle, add survey design fields, recode marriage per cycle, and stack\n",
    "import re\n",
    "\n",
    "def pick_weight_col(df):\n",
    "    \"\"\"Pick a MEC 2-year weight column (prefer WTMEC2YR; regex; else WTINT2YR fallback).\"\"\"\n",
    "    colsU = {c.upper(): c for c in df.columns}\n",
    "    # 1) exact preferred\n",
    "    if \"WTMEC2YR\" in colsU:\n",
    "        return colsU[\"WTMEC2YR\"], \"WTMEC2YR\"\n",
    "    # 2) regex like \"WT...MEC...2YR\"\n",
    "    for cU, c in colsU.items():\n",
    "        if re.match(r\"^WT.*MEC.*2YR$\", cU):\n",
    "            return c, \"regex_WTMEC2YR\"\n",
    "    # 3) interview weight as last resort\n",
    "    if \"WTINT2YR\" in colsU:\n",
    "        return colsU[\"WTINT2YR\"], \"WTINT2YR_fallback\"\n",
    "    return None, None\n",
    "\n",
    "demo_dfs, missing = [], []\n",
    "for cycle in DEMO_CYCLES:\n",
    "    try:\n",
    "        p = ensure_demo_file(cycle)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"⚠️ {e}\")\n",
    "        missing.append(cycle)\n",
    "        continue\n",
    "\n",
    "    df = read_demo_file(p)\n",
    "    df.columns = [c.upper() for c in df.columns]\n",
    "    df[\"CYCLE\"] = cycle\n",
    "\n",
    "    # --- survey design fields ---\n",
    "    # Normalize weight column to WTMEC2YR if possible\n",
    "    wcol, wsrc = pick_weight_col(df)\n",
    "    if wcol is not None and \"WTMEC2YR\" not in df.columns:\n",
    "        df[\"WTMEC2YR\"] = df[wcol]\n",
    "\n",
    "    # --- marital status recodes (unchanged) ---\n",
    "    M4, M3 = compute_marriage_cols(df)\n",
    "    df[\"MARRIAGE\"]  = M4\n",
    "    df[\"MARRIAGE3\"] = M3\n",
    "\n",
    "    # Keep only relevant columns that exist\n",
    "    keep = [c for c in [\n",
    "        \"SEQN\",\"RIDAGEYR\",\"SDDSRVYR\",\"SDMVPSU\",\"SDMVSTRA\",\"WTMEC2YR\",\n",
    "        \"CYCLE\",\"MARRIAGE\",\"MARRIAGE3\"\n",
    "    ] if c in df.columns]\n",
    "    demo_dfs.append(df[keep])\n",
    "\n",
    "if missing:\n",
    "    print(\"⚠️ Missing cycles (not found and not downloaded):\", missing)\n",
    "if not demo_dfs:\n",
    "    raise FileNotFoundError(\"No DEMO files were found or downloaded under your data/ directory.\")\n",
    "\n",
    "# %% Step 4: Combine and coerce types\n",
    "demo9923 = pd.concat(demo_dfs, ignore_index=True).copy()\n",
    "for c in [\"SEQN\",\"RIDAGEYR\",\"SDDSRVYR\",\"SDMVPSU\",\"SDMVSTRA\",\"WTMEC2YR\"]:\n",
    "    if c in demo9923.columns:\n",
    "        demo9923[c] = pd.to_numeric(demo9923[c], errors=\"coerce\")\n",
    "\n",
    "demo9923 = demo9923.dropna(subset=[\"SEQN\"]).drop_duplicates(subset=[\"SEQN\"])\n",
    "\n",
    "# %% Step 5: Save tidy stacked DEMO with survey design fields\n",
    "INTERIM = DATA.parent / \"interim\"\n",
    "INTERIM.mkdir(parents=True, exist_ok=True)\n",
    "demo_out = INTERIM / \"demo_9923.parquet\"\n",
    "demo9923.to_parquet(demo_out, index=False)\n",
    "print(f\"✅ Saved DEMO with survey design → {demo_out}\")\n",
    "\n",
    "# %% Step 6: Quick summaries\n",
    "print(f\"✅ Total rows: {demo9923.shape[0]}\")\n",
    "print(f\"✅ Unique SEQN: {demo9923['SEQN'].nunique()}\")\n",
    "print(\"Columns:\", demo9923.columns.tolist())\n",
    "\n",
    "# Missingness report for design fields\n",
    "for k in [\"SDDSRVYR\",\"SDMVPSU\",\"SDMVSTRA\",\"WTMEC2YR\"]:\n",
    "    if k in demo9923.columns:\n",
    "        print(f\"{k} NA %:\", round(demo9923[k].isna().mean()*100, 2), \"%\")\n",
    "    else:\n",
    "        print(f\"{k} missing entirely\")\n",
    "\n",
    "# Optional: verify 2021–2023 marriage3 populated\n",
    "if \"MARRIAGE3\" in demo9923.columns:\n",
    "    vc = demo9923.loc[demo9923[\"CYCLE\"]==\"August 2021–August 2023\",\"MARRIAGE3\"].value_counts(dropna=False)\n",
    "    print(\"\\n2021–2023 MARRIAGE3 counts:\\n\", vc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562ed918-6c96-4412-acba-827f652d9fc6",
   "metadata": {},
   "source": [
    "<h3>save and check</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "473fea1a-e695-4b8a-877e-2329915dc8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /Users/dengshuyue/Desktop/SDOH/analysis/data/demo9923.pkl\n",
      "Saved: /Users/dengshuyue/Desktop/SDOH/analysis/data/demo9923.csv\n"
     ]
    }
   ],
   "source": [
    "# Simple saves without pyarrow (plain CSV)\n",
    "out_pkl = DATA / \"demo9923.pkl\"   # fastest round-trip in Python\n",
    "out_csv = DATA / \"demo9923.csv\"   # plain CSV (max compatibility)\n",
    "\n",
    "demo9923.to_pickle(out_pkl)\n",
    "demo9923.to_csv(out_csv, index=False)  # no compression\n",
    "\n",
    "print(\"Saved:\", out_pkl)\n",
    "print(\"Saved:\", out_csv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af7ba73c-db7a-4696-b68a-14fdea35b5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Counts by CYCLE:\n",
      "CYCLE\n",
      "2017-March 2020 (pre-pandemic)    15560\n",
      "August 2021–August 2023           11933\n",
      "2001-2002                         11039\n",
      "2009-2010                         10537\n",
      "2005-2006                         10348\n",
      "2013-2014                         10175\n",
      "2007-2008                         10149\n",
      "2003-2004                         10122\n",
      "2015-2016                          9971\n",
      "1999-2000                          9965\n",
      "2011-2012                          9756\n",
      "2017-2018                          9254\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts by SDDSRVYR (raw numeric code, may be NaN for newer releases):\n",
      "SDDSRVYR\n",
      "1.0      9965\n",
      "2.0     11039\n",
      "3.0     10122\n",
      "4.0     10348\n",
      "5.0     10149\n",
      "6.0     10537\n",
      "7.0      9756\n",
      "8.0     10175\n",
      "9.0      9971\n",
      "10.0     9254\n",
      "12.0    11933\n",
      "66.0    15560\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCounts by CYCLE:\")\n",
    "print(demo9923[\"CYCLE\"].value_counts(dropna=False))\n",
    "\n",
    "print(\"\\nCounts by SDDSRVYR (raw numeric code, may be NaN for newer releases):\")\n",
    "print(demo9923[\"SDDSRVYR\"].value_counts(dropna=False).sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d124b00c-1b5e-4047-80dc-1f19a12132c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "min             1.0\n",
       "max        142310.0\n",
       "nunique    128809.0\n",
       "Name: SEQN, dtype: float64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo9923.tail(16)\n",
    "demo9923[\"SEQN\"].agg([\"min\", \"max\", \"nunique\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fceb15f-f051-4f4d-bb66-0936de39864f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50aff7dc-77ed-4d06-be26-be021a58156a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a9ad7e-d9e0-4488-a042-b17f884dbb95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72df42b6-ac05-4e45-9520-070a01e153fe",
   "metadata": {},
   "source": [
    "<h2> Step 2: merge demo with mortality </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5dd8e708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mortality file: /Users/dengshuyue/Desktop/SDOH/analysis/data/less_important/mortality9918.sas7bdat\n",
      "\n",
      "📊 NHANES cycles in mortality-linked data:\n",
      "SDDSRVYR\n",
      "1999–2000    4973\n",
      "2001–2002    5586\n",
      "2003–2004    5293\n",
      "2005–2006    5332\n",
      "2007–2008    5989\n",
      "2009–2010    6346\n",
      "2011–2012    5603\n",
      "2013–2014    5913\n",
      "2015–2016    5720\n",
      "2017–2018    5498\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total records: 56253\n",
      "\n",
      "⏱️  Survival summary:\n",
      "  N (unique SEQN): 56253\n",
      "  Events (%): 14.87\n",
      "  TIME_Y (min / median / max): 0.0 9.416666666666666 20.75\n"
     ]
    }
   ],
   "source": [
    "# %% Mortality: locate, read, prep, and merge with DEMO (uses demo9923)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Paths -------------------------------------------------------\n",
    "ROOT = Path(\"/Users/dengshuyue/Desktop/SDOH/analysis\")\n",
    "DATA = ROOT / \"data\"\n",
    "\n",
    "# --- Locate mortality file (search recursively under data/) -----\n",
    "MORT_PATTERNS = [\n",
    "    \"*mortality*.sas7bdat\", \"*mort*.sas7bdat\",\n",
    "    \"*mortality*.xpt\",      \"*mort*.xpt\",\n",
    "    \"*mortality*.csv\",      \"*mort*.csv\"\n",
    "]\n",
    "\n",
    "def find_first(base: Path, patterns):\n",
    "    for pat in patterns:\n",
    "        hits = list(base.rglob(pat))\n",
    "        if hits:\n",
    "            return hits[0]\n",
    "    return None\n",
    "\n",
    "mort_path = find_first(DATA, MORT_PATTERNS)\n",
    "if mort_path is None:\n",
    "    raise FileNotFoundError(\n",
    "        f\"No mortality file found under {DATA}. \"\n",
    "        \"Place a file like 'NHANES_1999_2019_LMF_public.csv' or 'mortality9918.sas7bdat' there.\"\n",
    "    )\n",
    "print(\"Using mortality file:\", mort_path)\n",
    "\n",
    "# --- Load mortality (handles .xpt / .sas7bdat / .csv) ----------\n",
    "suffix = mort_path.suffix.lower()\n",
    "if suffix == \".xpt\":\n",
    "    try:\n",
    "        import pyreadstat\n",
    "        mort, _ = pyreadstat.read_xport(mort_path)\n",
    "    except Exception:\n",
    "        mort = pd.read_sas(mort_path, format=\"xport\")\n",
    "elif suffix == \".sas7bdat\":\n",
    "    try:\n",
    "        import pyreadstat\n",
    "        mort, _ = pyreadstat.read_sas7bdat(mort_path)\n",
    "    except Exception:\n",
    "        mort = pd.read_sas(mort_path, format=\"sas7bdat\")\n",
    "elif suffix == \".csv\":\n",
    "    mort = pd.read_csv(mort_path)\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported mortality file type: {suffix}\")\n",
    "\n",
    "mort.columns = mort.columns.str.upper()\n",
    "\n",
    "# --- Keep relevant columns if present ---------------------------\n",
    "keep_cols = [c for c in [\n",
    "    \"SEQN\",\"ELIGSTAT\",\"MORTSTAT\",\"PERMTH_EXM\",\"PERMTH_INT\",\n",
    "    \"UCOD_LEADING\",\"DIABETES\",\"HYPERTEN\",\"DODQTR\",\"DODYEAR\"\n",
    "] if c in mort.columns]\n",
    "mort = mort[keep_cols].copy()\n",
    "\n",
    "# --- Eligibility and time/event construction --------------------\n",
    "if \"ELIGSTAT\" in mort.columns:\n",
    "    mort = mort[mort[\"ELIGSTAT\"] == 1].copy()\n",
    "\n",
    "# default: exam-based time; switch to interview by changing TIME_COL\n",
    "TIME_COL = \"PERMTH_EXM\" if \"PERMTH_EXM\" in mort.columns else \"PERMTH_INT\"\n",
    "if TIME_COL not in mort.columns:\n",
    "    raise ValueError(\"Neither PERMTH_EXM nor PERMTH_INT found in mortality file.\")\n",
    "\n",
    "mort[\"TIME_Y\"] = pd.to_numeric(mort[TIME_COL], errors=\"coerce\") / 12.0\n",
    "mort[\"EVENT\"]  = (mort[\"MORTSTAT\"] == 1).astype(int)\n",
    "mort = mort[(mort[\"TIME_Y\"].notna()) & (mort[\"TIME_Y\"] >= 0)].copy()\n",
    "\n",
    "# --- Ensure DEMO columns & types (demo9923) ---------------------\n",
    "demo9923.columns = demo9923.columns.str.upper()\n",
    "if \"SDDSRVYR\" not in demo9923.columns:\n",
    "    raise ValueError(\"SDDSRVYR not found in demo9923. Ensure your DEMO build carries SDDSRVYR.\")\n",
    "demo9923[\"SDDSRVYR\"] = pd.to_numeric(demo9923[\"SDDSRVYR\"], errors=\"coerce\")\n",
    "\n",
    "# --- Merge age + cycle into mortality ---------------------------\n",
    "cols_to_merge = [c for c in [\"SEQN\",\"RIDAGEYR\",\"SDDSRVYR\"] if c in demo9923.columns]\n",
    "mort_with_demo = mort.merge(demo9923[cols_to_merge], on=\"SEQN\", how=\"left\")\n",
    "\n",
    "# Optional: adults (≥20)\n",
    "# mort_with_demo = mort_with_demo.dropna(subset=[\"RIDAGEYR\"])\n",
    "# mort_with_demo = mort_with_demo[mort_with_demo[\"RIDAGEYR\"] >= 20]\n",
    "\n",
    "# --- Cycle counts and summary -----------------------------------\n",
    "cycle_counts = mort_with_demo[\"SDDSRVYR\"].value_counts(dropna=False).sort_index()\n",
    "\n",
    "cycle_map = {\n",
    "    1: \"1999–2000\",  2: \"2001–2002\",  3: \"2003–2004\",\n",
    "    4: \"2005–2006\",  5: \"2007–2008\",  6: \"2009–2010\",\n",
    "    7: \"2011–2012\",  8: \"2013–2014\",  9: \"2015–2016\",\n",
    "    10: \"2017–2018\", 11: \"2017–Mar 2020 (pre-pandemic)\", 12: \"Aug 2021–Aug 2023\"\n",
    "}\n",
    "\n",
    "print(\"\\n📊 NHANES cycles in mortality-linked data:\")\n",
    "print(cycle_counts.rename(index=cycle_map))\n",
    "print(f\"\\nTotal records: {int(cycle_counts.sum())}\")\n",
    "\n",
    "print(\"\\n⏱️  Survival summary:\")\n",
    "print(\"  N (unique SEQN):\", mort_with_demo[\"SEQN\"].nunique())\n",
    "print(\"  Events (%):\", round(100 * mort_with_demo[\"EVENT\"].mean(), 2))\n",
    "print(\"  TIME_Y (min / median / max):\",\n",
    "      np.nanmin(mort_with_demo[\"TIME_Y\"]),\n",
    "      np.nanmedian(mort_with_demo[\"TIME_Y\"]),\n",
    "      np.nanmax(mort_with_demo[\"TIME_Y\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df6670d-1b16-413f-a629-93b05ac9befb",
   "metadata": {},
   "source": [
    "<h2>Step 3: merge demo_mort with sodh</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf215fc8-2203-4b3b-8627-6cd85b7df2c7",
   "metadata": {},
   "source": [
    "\n",
    "<h3> OCQ / HOQ / HIQ / FSQ — early (1999–2002) + main (2003–2018), adult filter, tidy outputs </h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fafb19b5-bf54-4410-bf49-c7478074760c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Prereqs & helpers\n",
    "import pandas as pd, numpy as np, os\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path(\"/Users/dengshuyue/Desktop/SDOH/analysis\")\n",
    "DATA = ROOT / \"data\"\n",
    "MOD  = DATA / \"nhanes_by_module\"\n",
    "\n",
    "def read_any(p: Path) -> pd.DataFrame:\n",
    "    s = p.suffix.lower()\n",
    "    if s == \".xpt\":\n",
    "        import pyreadstat\n",
    "        df, _ = pyreadstat.read_xport(str(p))\n",
    "    elif s == \".sas7bdat\":\n",
    "        df = pd.read_sas(str(p), format=\"sas7bdat\", encoding=\"latin1\")\n",
    "    elif s == \".csv\":\n",
    "        df = pd.read_csv(p)\n",
    "    elif s == \".parquet\":\n",
    "        df = pd.read_parquet(p)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file: {p}\")\n",
    "    df.columns = df.columns.str.upper()\n",
    "    return df\n",
    "\n",
    "def filter_adults(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Adults ≥20 using demo9923 already in memory (SEQN, RIDAGEYR).\"\"\"\n",
    "    if \"demo9923\" not in globals():\n",
    "        raise RuntimeError(\"demo9923 not found in memory; load/build it first.\")\n",
    "    demo = demo9923.copy()\n",
    "    demo.columns = demo.columns.str.upper()\n",
    "    age = demo[[\"SEQN\",\"RIDAGEYR\"]].dropna()\n",
    "    age = age[age[\"RIDAGEYR\"] >= 20]\n",
    "    return df.merge(age, on=\"SEQN\", how=\"inner\")\n",
    "\n",
    "def s_or_false(df: pd.DataFrame, col: str):\n",
    "    return df[col] if col in df.columns else pd.Series(False, index=df.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "45c69f37-41a6-4c07-b2f9-e74b3fd0da9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCQ: (55081, 4)\n"
     ]
    }
   ],
   "source": [
    "# %% OCQ: 1999–2018 (employment recode)\n",
    "OCQ = MOD / \"ocq\"\n",
    "ocq_main = OCQ / \"ocq.sas7bdat\"\n",
    "ocq_early_files = [(OCQ/\"OCQ.xpt\",1), (OCQ/\"OCQ_B.xpt\",2)]\n",
    "\n",
    "def recode_employment(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"EMPLOY\"] = np.nan\n",
    "    if \"OCD150\" in df:\n",
    "        df.loc[df[\"OCD150\"] == 1, \"EMPLOY\"] = 1\n",
    "        df.loc[df[\"OCD150\"] == 3, \"EMPLOY\"] = 2\n",
    "    if \"OCQ380\" in df:\n",
    "        df.loc[df[\"OCQ380\"] == 5, \"EMPLOY\"] = 2\n",
    "        df.loc[df[\"OCQ380\"] == 3, \"EMPLOY\"] = 3\n",
    "        df.loc[df[\"OCQ380\"].isin([4,6]), \"EMPLOY\"] = 4\n",
    "        df.loc[df[\"OCQ380\"].isin([1,2,7]), \"EMPLOY\"] = 5\n",
    "    df[\"UNEMPLOYMENT\"] = (df[\"EMPLOY\"] == 2).astype(\"Int64\")\n",
    "    keep = [c for c in [\"SEQN\",\"EMPLOY\",\"UNEMPLOYMENT\",\"SDDSRVYR\"] if c in df.columns]\n",
    "    return df[keep]\n",
    "\n",
    "# early (adult filter applied)\n",
    "parts = []\n",
    "for p, cyc in ocq_early_files:\n",
    "    if p.exists():\n",
    "        df = read_any(p)\n",
    "        df[\"SDDSRVYR\"] = cyc\n",
    "        df = filter_adults(df)\n",
    "        parts.append(recode_employment(df))\n",
    "ocq_early = pd.concat(parts, ignore_index=True) if parts else pd.DataFrame()\n",
    "\n",
    "# main (adult filter applied)\n",
    "if not ocq_main.exists():\n",
    "    raise FileNotFoundError(f\"Missing {ocq_main}\")\n",
    "ocq_main_df = filter_adults(read_any(ocq_main))\n",
    "ocq_main_df = recode_employment(ocq_main_df)\n",
    "\n",
    "# combine + hygiene\n",
    "ocq = pd.concat([ocq_early, ocq_main_df], ignore_index=True)\n",
    "ocq[\"SEQN\"] = pd.to_numeric(ocq[\"SEQN\"], errors=\"coerce\").astype(\"Int64\")\n",
    "if \"SDDSRVYR\" in ocq: ocq[\"SDDSRVYR\"] = pd.to_numeric(ocq[\"SDDSRVYR\"], errors=\"coerce\").astype(\"Int64\")\n",
    "ocq = ocq.dropna(subset=[\"SEQN\"]).drop_duplicates(subset=[\"SEQN\"])\n",
    "print(\"OCQ:\", ocq.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2ddc2a1-84de-4fe3-bde2-a1e34e5f69a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HOQ: (55081, 4)\n"
     ]
    }
   ],
   "source": [
    "# %% HOQ: 1999–2018 (housing subset)\n",
    "HOQ = MOD / \"hoq\"\n",
    "hoq_main = HOQ / \"hoq.sas7bdat\"\n",
    "hoq_early_files = [(HOQ/\"HOQ.xpt\",1), (HOQ/\"HOQ_B.xpt\",2), (HOQ/\"hoq.sas7bdat\",1), (HOQ/\"hoq_b.sas7bdat\",2)]\n",
    "\n",
    "def preprocess_hoq(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    if \"HOQ065\" in df: df.loc[df[\"HOQ065\"].isin([7,9]), \"HOQ065\"] = np.nan\n",
    "    keep = [c for c in [\"SEQN\",\"HOD050\",\"HOQ065\",\"SDDSRVYR\"] if c in df.columns]\n",
    "    return df[keep]\n",
    "\n",
    "parts = []\n",
    "seen = set()\n",
    "for p, cyc in hoq_early_files:\n",
    "    if p.exists() and cyc not in seen:\n",
    "        df = read_any(p); df[\"SDDSRVYR\"] = cyc\n",
    "        df = filter_adults(df)\n",
    "        parts.append(preprocess_hoq(df)); seen.add(cyc)\n",
    "hoq_early = pd.concat(parts, ignore_index=True) if parts else pd.DataFrame()\n",
    "\n",
    "if not hoq_main.exists():\n",
    "    raise FileNotFoundError(f\"Missing {hoq_main}\")\n",
    "hoq_main_df = preprocess_hoq(filter_adults(read_any(hoq_main)))\n",
    "\n",
    "hoq_all = pd.concat([hoq_early, hoq_main_df], ignore_index=True)\n",
    "for c in (\"SEQN\",\"SDDSRVYR\"):\n",
    "    if c in hoq_all: hoq_all[c] = pd.to_numeric(hoq_all[c], errors=\"coerce\").astype(\"Int64\")\n",
    "hoq_all = hoq_all.dropna(subset=[\"SEQN\"]).drop_duplicates(subset=[\"SEQN\"])\n",
    "print(\"HOQ:\", hoq_all.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c2d62239-e5d0-4ab7-9239-72247529cc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INS: (55081, 3)\n"
     ]
    }
   ],
   "source": [
    "# %% HIQ/HIQS: 1999–2018 (insurance category INS)\n",
    "HIQ = MOD / \"hiq\"\n",
    "hiq_main = HIQ / \"hiqs.sas7bdat\"\n",
    "hiq_early_files = [(HIQ/\"HIQ.xpt\",1), (HIQ/\"HIQ_B.xpt\",2), (HIQ/\"hiq.sas7bdat\",1), (HIQ/\"hiq_b.sas7bdat\",2)]\n",
    "\n",
    "# stack early + main, adult filter\n",
    "parts, seen = [], set()\n",
    "for p, cyc in hiq_early_files:\n",
    "    if p.exists() and cyc not in seen:\n",
    "        df = read_any(p); df[\"SDDSRVYR\"] = cyc\n",
    "        parts.append(filter_adults(df)); seen.add(cyc)\n",
    "hiq_early = pd.concat(parts, ignore_index=True) if parts else pd.DataFrame()\n",
    "\n",
    "if not hiq_main.exists():\n",
    "    raise FileNotFoundError(f\"Missing {hiq_main}\")\n",
    "hiqs = filter_adults(read_any(hiq_main))\n",
    "\n",
    "hiq_all = pd.concat([hiq_early, hiqs], ignore_index=True)\n",
    "hiq_all.columns = hiq_all.columns.str.upper()\n",
    "\n",
    "# build INS (0 none, 1 private, 2 Medicare, 3 Medicaid/both, 5 other)\n",
    "ins = pd.DataFrame({\"SEQN\": hiq_all[\"SEQN\"]})\n",
    "if \"SDDSRVYR\" in hiq_all: ins[\"SDDSRVYR\"] = hiq_all[\"SDDSRVYR\"]\n",
    "ins[\"INS\"] = np.nan\n",
    "\n",
    "cond_private = (s_or_false(hiq_all,\"HIQ031A\") == 14) | (s_or_false(hiq_all,\"HID030A\") == 1)\n",
    "ins.loc[cond_private, \"INS\"] = 1\n",
    "\n",
    "cond_med = (\n",
    "    (s_or_false(hiq_all,\"HIQ031B\") == 15) &\n",
    "    (s_or_false(hiq_all,\"HIQ031D\") != 17) &\n",
    "    (s_or_false(hiq_all,\"HIQ031E\") != 18)\n",
    ") | (\n",
    "    (s_or_false(hiq_all,\"HID030B\") == 1) &\n",
    "    (s_or_false(hiq_all,\"HID030C\") != 1)\n",
    ")\n",
    "ins.loc[cond_med, \"INS\"] = 2\n",
    "\n",
    "cond_mcaid_only = (\n",
    "    ((s_or_false(hiq_all,\"HIQ031D\") == 17) | (s_or_false(hiq_all,\"HIQ031E\") == 18)) &\n",
    "    (s_or_false(hiq_all,\"HIQ031B\") != 15)\n",
    ") | (\n",
    "    (s_or_false(hiq_all,\"HID030B\") != 1) &\n",
    "    (s_or_false(hiq_all,\"HID030C\") == 1)\n",
    ")\n",
    "cond_both = (\n",
    "    (s_or_false(hiq_all,\"HIQ031B\") == 15) & (s_or_false(hiq_all,\"HIQ031D\") == 17)\n",
    ") | (\n",
    "    (s_or_false(hiq_all,\"HID030B\") == 1) & (s_or_false(hiq_all,\"HID030C\") == 1)\n",
    ")\n",
    "ins.loc[cond_mcaid_only | cond_both, \"INS\"] = 3\n",
    "\n",
    "other_cols = [c for c in [\"HIQ031C\",\"HIQ031F\",\"HIQ031G\",\"HIQ031H\",\"HIQ031I\"] if c in hiq_all.columns]\n",
    "cond_other = hiq_all[other_cols].eq(1).any(axis=1) if other_cols else pd.Series(False, index=hiq_all.index)\n",
    "cond_other = cond_other | (s_or_false(hiq_all,\"HID030D\") == 1)\n",
    "ins.loc[cond_other, \"INS\"] = 5\n",
    "\n",
    "none_conds = []\n",
    "if \"HIQ011\" in hiq_all: none_conds.append(hiq_all[\"HIQ011\"] == 2)\n",
    "if \"HID010\" in hiq_all: none_conds.append(hiq_all[\"HID010\"] == 2)\n",
    "if none_conds: ins.loc[np.logical_or.reduce(none_conds), \"INS\"] = 0\n",
    "\n",
    "for c in (\"SEQN\",\"SDDSRVYR\"):\n",
    "    if c in ins: ins[c] = pd.to_numeric(ins[c], errors=\"coerce\").astype(\"Int64\")\n",
    "ins = ins.dropna(subset=[\"SEQN\"]).drop_duplicates(subset=[\"SEQN\"])\n",
    "print(\"INS:\", ins.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b6cbb417-9a50-4bc7-a6e5-956d4e310f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNAP/FS: (55081, 5)\n"
     ]
    }
   ],
   "source": [
    "# %% FSQ/FSQS: 1999–2018 (SNAP & FS)\n",
    "FSQ = MOD / \"fsq\"\n",
    "fsq_main = FSQ / \"fsqs.sas7bdat\"\n",
    "fsq_early_files = [(FSQ/\"FSQ.xpt\",1), (FSQ/\"FSQ_B.xpt\",2), (FSQ/\"fsq.sas7bdat\",1), (FSQ/\"fsq_b.sas7bdat\",2)]\n",
    "\n",
    "parts, seen = [], set()\n",
    "for p, cyc in fsq_early_files:\n",
    "    if p.exists() and cyc not in seen:\n",
    "        df = read_any(p); df[\"SDDSRVYR\"] = cyc\n",
    "        parts.append(filter_adults(df)); seen.add(cyc)\n",
    "fsq_early = pd.concat(parts, ignore_index=True) if parts else pd.DataFrame()\n",
    "\n",
    "if not fsq_main.exists():\n",
    "    raise FileNotFoundError(f\"Missing {fsq_main}\")\n",
    "fsqs = filter_adults(read_any(fsq_main))\n",
    "\n",
    "fsq_all = pd.concat([fsq_early, fsqs], ignore_index=True)\n",
    "fsq_all.columns = fsq_all.columns.str.upper()\n",
    "\n",
    "snap = pd.DataFrame({\"SEQN\": fsq_all[\"SEQN\"]})\n",
    "if \"SDDSRVYR\" in fsq_all: snap[\"SDDSRVYR\"] = fsq_all[\"SDDSRVYR\"]\n",
    "if \"FSDHH\" in fsq_all:   snap[\"FSDHH\"] = fsq_all[\"FSDHH\"]\n",
    "\n",
    "snap[\"SNAP\"] = np.nan\n",
    "if \"FSQ165\" in fsq_all: snap.loc[fsq_all[\"FSQ165\"] == 2, \"SNAP\"] = 0\n",
    "if \"FSQ012\" in fsq_all:\n",
    "    snap.loc[fsq_all[\"FSQ012\"] == 1, \"SNAP\"] = 1\n",
    "    snap.loc[fsq_all[\"FSQ012\"] == 2, \"SNAP\"] = 0\n",
    "if \"FSQ171\" in fsq_all:\n",
    "    snap.loc[fsq_all[\"FSQ171\"] == 1, \"SNAP\"] = 1\n",
    "    snap.loc[fsq_all[\"FSQ171\"] == 2, \"SNAP\"] = 0\n",
    "if \"FSD170N\" in fsq_all: snap.loc[fsq_all[\"FSD170N\"] >= 1, \"SNAP\"] = 1\n",
    "if \"FSQ170\" in fsq_all:\n",
    "    snap.loc[fsq_all[\"FSQ170\"] == 1, \"SNAP\"] = 1\n",
    "    snap.loc[(fsq_all[\"FSQ170\"] == 2) & (fsq_all.get(\"FSD170N\", pd.Series(index=fsq_all.index)) < 1), \"SNAP\"] = 0\n",
    "if \"FSD200\" in fsq_all: snap.loc[fsq_all[\"FSD200\"] == 1, \"SNAP\"] = 1\n",
    "\n",
    "snap[\"FS\"] = np.nan\n",
    "if \"FSDHH\" in fsq_all:\n",
    "    snap.loc[fsq_all[\"FSDHH\"].isin([1,2]), \"FS\"] = 1\n",
    "    snap.loc[fsq_all[\"FSDHH\"] > 2,        \"FS\"] = 0\n",
    "\n",
    "for c in (\"SEQN\",\"SDDSRVYR\"):\n",
    "    if c in snap: snap[c] = pd.to_numeric(snap[c], errors=\"coerce\").astype(\"Int64\")\n",
    "snap = snap[[c for c in [\"SEQN\",\"SNAP\",\"FSDHH\",\"FS\",\"SDDSRVYR\"] if c in snap.columns]]\n",
    "snap = snap.dropna(subset=[\"SEQN\"]).drop_duplicates(subset=[\"SEQN\"])\n",
    "print(\"SNAP/FS:\", snap.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8d26ca-28f2-4757-a289-e0f49eeddc92",
   "metadata": {},
   "source": [
    "<h3> Audit — cycles present & inner vs left-merge coverage </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "25bbe1e7-f589-40bc-ab50-48471b5a2d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCQ cycles: [np.int64(1), np.int64(2)]\n",
      "  OCQ | base=56,253 | inner keeps=52,287 | left keeps=56,253\n",
      "HOQ cycles: [np.int64(1), np.int64(2)]\n",
      "  HOQ | base=56,253 | inner keeps=52,287 | left keeps=56,253\n",
      "HIQ cycles: [np.int64(1), np.int64(2)]\n",
      "  HIQ | base=56,253 | inner keeps=52,287 | left keeps=56,253\n",
      "FSQ cycles: [np.int64(1), np.int64(2)]\n",
      "  FSQ | base=56,253 | inner keeps=52,287 | left keeps=56,253\n"
     ]
    }
   ],
   "source": [
    "# %% Audit helpers + run\n",
    "def cycles(df, name):\n",
    "    if \"SDDSRVYR\" in df.columns and df[\"SDDSRVYR\"].notna().any():\n",
    "        print(f\"{name} cycles:\", sorted(df[\"SDDSRVYR\"].dropna().astype(int).unique()))\n",
    "    else:\n",
    "        tmp = df.merge(demo9923[[\"SEQN\",\"SDDSRVYR\"]], on=\"SEQN\", how=\"left\")\n",
    "        if tmp[\"SDDSRVYR\"].notna().any():\n",
    "            print(f\"{name} cycles (via DEMO):\", sorted(tmp[\"SDDSRVYR\"].dropna().astype(int).unique()))\n",
    "        else:\n",
    "            print(f\"{name} cycles: (none found)\")\n",
    "\n",
    "def coverage(base, addon, name):\n",
    "    base_u = base[\"SEQN\"].dropna().nunique()\n",
    "    inner_u = base.merge(addon[[\"SEQN\"]].drop_duplicates(), on=\"SEQN\", how=\"inner\")[\"SEQN\"].nunique()\n",
    "    left_u  = base.merge(addon[[\"SEQN\"]].drop_duplicates(),  on=\"SEQN\", how=\"left\")[\"SEQN\"].nunique()\n",
    "    print(f\"{name:>5} | base={base_u:,} | inner keeps={inner_u:,} | left keeps={left_u:,}\")\n",
    "\n",
    "# Choose a base for audit (prefers mortality, else diet)\n",
    "if \"mort_with_demo\" in globals():\n",
    "    base_df = mort_with_demo\n",
    "elif \"SODH_diet_mort\" in globals():\n",
    "    base_df = SODH_diet_mort\n",
    "else:\n",
    "    base_df = demo9923\n",
    "\n",
    "for (n, df) in [(\"OCQ\", ocq), (\"HOQ\", hoq_all), (\"HIQ\", ins), (\"FSQ\", snap)]:\n",
    "    cycles(df, n)\n",
    "    coverage(base_df, df, n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d1787b-6ade-4b30-b68d-a0541415c66a",
   "metadata": {},
   "source": [
    "<h3> Left-merge onto mortality base and save </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eea6dd35-0057-47fa-84dd-f7ac60d033ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT = /Users/dengshuyue/Desktop/SDOH/analysis\n",
      "OUT  = /Users/dengshuyue/Desktop/SDOH/analysis/output\n",
      "SDOH_ROOT env = None\n",
      "OUT (fixed) = /Users/dengshuyue/Desktop/SDOH/analysis/output\n"
     ]
    }
   ],
   "source": [
    "# check if output path is correct \n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"ROOT =\", ROOT)\n",
    "print(\"OUT  =\", OUT)\n",
    "print(\"SDOH_ROOT env =\", os.environ.get(\"SDOH_ROOT\"))\n",
    "\n",
    "# ✅ Force the correct OUT (one-time reassignment in this kernel)\n",
    "OUT = ROOT / \"output\"\n",
    "print(\"OUT (fixed) =\", OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "17d6efd7-ae4c-4335-bbcf-8f6c4721e9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ master shape: (56253, 20)\n",
      "\n",
      "Non-missing coverage (%):\n",
      "  UNEMPLOYMENT: 92.9%\n",
      "  HOD050: 91.6%\n",
      "  HOQ065: 91.5%\n",
      "  INS: 87.7%\n",
      "  EMPLOY: 86.8%\n",
      "  SNAP: 76.3%\n",
      "  FS: 74.7%\n",
      "\n",
      "Saved:\n",
      "  /Users/dengshuyue/Desktop/SDOH/analysis/output/nhanes_mort_demo_sdoh_1999_2018.parquet \n",
      "  /Users/dengshuyue/Desktop/SDOH/analysis/output/nhanes_mort_demo_sdoh_1999_2018.csv\n"
     ]
    }
   ],
   "source": [
    "# %% Build master (LEFT merges onto mort_with_demo) and save — refined\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "if \"mort_with_demo\" not in globals():\n",
    "    print(\"⚠️ mort_with_demo not found — skip merge/save.\")\n",
    "else:\n",
    "    def _key(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        d = df.copy()\n",
    "        d.columns = d.columns.str.upper()\n",
    "        d[\"SEQN\"] = pd.to_numeric(d[\"SEQN\"], errors=\"coerce\").astype(\"Int64\")\n",
    "        return d.dropna(subset=[\"SEQN\"]).drop_duplicates(subset=[\"SEQN\"])\n",
    "\n",
    "    base = _key(mort_with_demo)\n",
    "\n",
    "    # Pick only needed columns if present\n",
    "    ocq_m  = _key(ocq)      [[c for c in (\"SEQN\",\"EMPLOY\",\"UNEMPLOYMENT\") if c in ocq.columns]]      if \"ocq\"      in globals() else None\n",
    "    hoq_m  = _key(hoq_all)  [[c for c in (\"SEQN\",\"HOD050\",\"HOQ065\")        if c in hoq_all.columns]]  if \"hoq_all\"  in globals() else None\n",
    "    ins_m  = _key(ins)      [[c for c in (\"SEQN\",\"INS\")                    if c in ins.columns]]      if \"ins\"      in globals() else None\n",
    "    snap_m = _key(snap)     [[c for c in (\"SEQN\",\"SNAP\",\"FSDHH\",\"FS\")      if c in snap.columns]]     if \"snap\"     in globals() else None\n",
    "\n",
    "    # Merge in sequence with one-to-one validation\n",
    "    master = base\n",
    "    for name, piece in [(\"OCQ\", ocq_m), (\"HOQ\", hoq_m), (\"INS\", ins_m), (\"FSQ\", snap_m)]:\n",
    "        if piece is None:\n",
    "            print(f\"ℹ️  {name}: not available — skipped\")\n",
    "            continue\n",
    "        try:\n",
    "            master = master.merge(piece, on=\"SEQN\", how=\"left\", validate=\"one_to_one\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Merge failed for {name} (check duplicate SEQN).\") from e\n",
    "\n",
    "    print(\"✅ master shape:\", master.shape)\n",
    "\n",
    "    # Quick audit (non-missing coverage for key SDOH fields)\n",
    "    audit_cols = [c for c in [\"EMPLOY\",\"UNEMPLOYMENT\",\"HOD050\",\"HOQ065\",\"INS\",\"SNAP\",\"FS\"] if c in master.columns]\n",
    "    if audit_cols:\n",
    "        cov = (master[audit_cols].notna().mean()*100).round(1).sort_values(ascending=False)\n",
    "        print(\"\\nNon-missing coverage (%):\")\n",
    "        for k, v in cov.items(): print(f\"  {k}: {v}%\")\n",
    "\n",
    "    # Save (use predefined OUT)\n",
    "    OUT.mkdir(parents=True, exist_ok=True)\n",
    "    base_name = \"nhanes_mort_demo_sdoh_1999_2018\"\n",
    "    master.to_parquet(OUT / f\"{base_name}.parquet\", index=False)\n",
    "    master.to_csv(    OUT / f\"{base_name}.csv\",     index=False)\n",
    "    print(\"\\nSaved:\\n \", OUT / f\"{base_name}.parquet\", \"\\n \", OUT / f\"{base_name}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "da359655-d392-4f88-9443-c1f3dd2a21c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ master shape: (56253, 20)\n",
      "\n",
      "Non-missing coverage (%):\n",
      "  UNEMPLOYMENT: 92.9%\n",
      "  HOD050: 91.6%\n",
      "  HOQ065: 91.5%\n",
      "  INS: 87.7%\n",
      "  EMPLOY: 86.8%\n",
      "  SNAP: 76.3%\n",
      "  FS: 74.7%\n",
      "\n",
      "Saved:\n",
      "  /Users/dengshuyue/Desktop/SDOH/analysis/out/nhanes_mort_demo_sdoh_1999_2018.parquet \n",
      "  /Users/dengshuyue/Desktop/SDOH/analysis/out/nhanes_mort_demo_sdoh_1999_2018.csv\n"
     ]
    }
   ],
   "source": [
    "# %% Build master (LEFT merges onto mort_with_demo) and save — refined\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# If OUT wasn't defined earlier, set it here\n",
    "ROOT = Path(\"/Users/dengshuyue/Desktop/SDOH/analysis\")\n",
    "OUT = ROOT / \"out\"\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if \"mort_with_demo\" not in globals():\n",
    "    print(\"⚠️ mort_with_demo not found — skip merge/save.\")\n",
    "else:\n",
    "    def _key(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        d = df.copy()\n",
    "        d.columns = d.columns.str.upper()\n",
    "        # normalize SEQN\n",
    "        d[\"SEQN\"] = pd.to_numeric(d[\"SEQN\"], errors=\"coerce\")\n",
    "        # if it's float but all .0, move to Int64\n",
    "        if pd.api.types.is_float_dtype(d[\"SEQN\"]) and ((d[\"SEQN\"] % 1) == 0).all():\n",
    "            d[\"SEQN\"] = d[\"SEQN\"].astype(\"Int64\")\n",
    "        else:\n",
    "            d[\"SEQN\"] = d[\"SEQN\"].astype(\"Int64\")\n",
    "        return d.dropna(subset=[\"SEQN\"]).drop_duplicates(subset=[\"SEQN\"])\n",
    "\n",
    "    base = _key(mort_with_demo)\n",
    "\n",
    "    # helper to pick columns AFTER uppercasing\n",
    "    def _pick_cols(df_up: pd.DataFrame, candidates: tuple[str, ...]) -> pd.DataFrame:\n",
    "        keep = [c for c in candidates if c in df_up.columns]\n",
    "        return df_up[keep] if keep else df_up[[\"SEQN\"]]\n",
    "\n",
    "    ocq_m  = _pick_cols(_key(ocq),      (\"SEQN\",\"EMPLOY\",\"UNEMPLOYMENT\")) if \"ocq\"      in globals() else None\n",
    "    hoq_m  = _pick_cols(_key(hoq_all),  (\"SEQN\",\"HOD050\",\"HOQ065\"))        if \"hoq_all\"  in globals() else None\n",
    "    ins_m  = _pick_cols(_key(ins),      (\"SEQN\",\"INS\"))                    if \"ins\"      in globals() else None\n",
    "    snap_m = _pick_cols(_key(snap),     (\"SEQN\",\"SNAP\",\"FSDHH\",\"FS\"))      if \"snap\"     in globals() else None\n",
    "\n",
    "    # Merge in sequence with one-to-one validation\n",
    "    master = base\n",
    "    for name, piece in [(\"OCQ\", ocq_m), (\"HOQ\", hoq_m), (\"INS\", ins_m), (\"FSQ\", snap_m)]:\n",
    "        if piece is None:\n",
    "            print(f\"ℹ️  {name}: not available — skipped\")\n",
    "            continue\n",
    "        # make sure no dupes in piece (already handled by _key → but safe)\n",
    "        if piece[\"SEQN\"].duplicated().any():\n",
    "            raise RuntimeError(f\"{name}: duplicate SEQN detected after cleaning.\")\n",
    "        try:\n",
    "            master = master.merge(piece, on=\"SEQN\", how=\"left\", validate=\"one_to_one\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Merge failed for {name} (check duplicate SEQN).\") from e\n",
    "\n",
    "    print(\"✅ master shape:\", master.shape)\n",
    "\n",
    "    # Quick audit (non-missing coverage for key SDOH fields)\n",
    "    audit_cols = [c for c in [\"EMPLOY\",\"UNEMPLOYMENT\",\"HOD050\",\"HOQ065\",\"INS\",\"SNAP\",\"FS\"] if c in master.columns]\n",
    "    if audit_cols:\n",
    "        cov = (master[audit_cols].notna().mean()*100).round(1).sort_values(ascending=False)\n",
    "        print(\"\\nNon-missing coverage (%):\")\n",
    "        for k, v in cov.items():\n",
    "            print(f\"  {k}: {v}%\")\n",
    "    else:\n",
    "        print(\"ℹ️ No audit columns present among EMPLOY/UNEMPLOYMENT/HOD050/HOQ065/INS/SNAP/FS.\")\n",
    "\n",
    "    # Save\n",
    "    base_name = \"nhanes_mort_demo_sdoh_1999_2018\"\n",
    "    master.to_parquet(OUT / f\"{base_name}.parquet\", index=False)\n",
    "    master.to_csv(    OUT / f\"{base_name}.csv\",     index=False)\n",
    "    print(\"\\nSaved:\\n \", OUT / f\"{base_name}.parquet\", \"\\n \", OUT / f\"{base_name}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7326a1f1-6b6a-4391-aace-6d2c8fcf670e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep a clear in-memory handle for downstream cells\n",
    "nhanes_mort_demo_sdoh = master.copy()\n",
    "\n",
    "# TEMP compatibility alias for older notebooks\n",
    "nhanes_mort_demo_soc_9918 = nhanes_mort_demo_sdoh\n",
    "df = nhanes_mort_demo_sdoh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b1712ce8-a21b-43e4-ac3e-a2236e85d935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 56,253 | Unique SEQN: 56,253 | Missing SEQN: 0 | Duplicates: 0\n",
      "SEQN range: 2 → 102956\n",
      "Cycles:\n",
      " SDDSRVYR\n",
      "1.0     4973\n",
      "2.0     5586\n",
      "3.0     5293\n",
      "4.0     5332\n",
      "5.0     5989\n",
      "6.0     6346\n",
      "7.0     5603\n",
      "8.0     5913\n",
      "9.0     5720\n",
      "10.0    5498\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Use in-memory object if available; else load from disk\n",
    "try:\n",
    "    df = nhanes_mort_demo_soc_9918\n",
    "except NameError:\n",
    "    ROOT = Path(\"/Users/dengshuyue/Desktop/SDOH/analysis\")\n",
    "    candidates = [\n",
    "        ROOT / \"analysis\" / \"output\" / \"nhanes_mort_demo_soc_1999_2018.parquet\",\n",
    "        ROOT / \"analysis\" / \"output\" / \"mort_with_demo_plus_soc.parquet\",\n",
    "    ]\n",
    "    for p in candidates:\n",
    "        if p.exists():\n",
    "            df = pd.read_parquet(p)\n",
    "            print(f\"Loaded from: {p}\")\n",
    "            break\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Couldn’t find the saved table in expected locations.\")\n",
    "\n",
    "# Ensure SEQN exists and is numeric\n",
    "if \"SEQN\" not in df.columns:\n",
    "    raise KeyError(\"SEQN column not found.\")\n",
    "s = pd.to_numeric(df[\"SEQN\"], errors=\"coerce\")\n",
    "\n",
    "print(f\"Rows: {len(df):,} | Unique SEQN: {s.nunique(dropna=True):,} | \"\n",
    "      f\"Missing SEQN: {s.isna().sum():,} | Duplicates: {df.duplicated('SEQN').sum():,}\")\n",
    "print(f\"SEQN range: {int(s.min())} → {int(s.max())}\")\n",
    "\n",
    "# Optional: cycles present (if available)\n",
    "if \"SDDSRVYR\" in df.columns:\n",
    "    print(\"Cycles:\\n\", df[\"SDDSRVYR\"].value_counts(dropna=False).sort_index())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c78a15-4795-4edc-8809-7bb045eea1c8",
   "metadata": {},
   "source": [
    "<h3> Optional Check for old files and moved to old </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "601af0d1-57a1-44af-8504-946fcc80752f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in: /Users/dengshuyue/Desktop/SDOH/analysis/output\n",
      "  ✓ nhanes_mort_demo_sdoh_1999_2018.parquet  | modified: 2025-09-09 13:20:01 | size: 774,526 bytes\n",
      "  ✓ nhanes_mort_demo_sdoh_1999_2018.csv      | modified: 2025-09-09 13:20:01 | size: 4,562,121 bytes\n",
      "  - nhanes_mort_demo_soc_1999_2018.parquet   (missing)\n",
      "  - nhanes_mort_demo_soc_1999_2018.csv       (missing)\n",
      "\n",
      "Loaded: /Users/dengshuyue/Desktop/SDOH/analysis/output/nhanes_mort_demo_sdoh_1999_2018.parquet\n",
      "Shape: (56253, 20)\n",
      "Cols (first 10): ['SEQN', 'ELIGSTAT', 'MORTSTAT', 'PERMTH_EXM', 'PERMTH_INT', 'UCOD_LEADING', 'DIABETES', 'HYPERTEN', 'TIME_Y', 'EVENT']\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Correct output folder (no double \"analysis\")\n",
    "out = Path(\"/Users/dengshuyue/Desktop/SDOH/analysis/output\")\n",
    "\n",
    "# Try new names first, then legacy\n",
    "candidates = [\n",
    "    out / \"nhanes_mort_demo_sdoh_1999_2018.parquet\",\n",
    "    out / \"nhanes_mort_demo_sdoh_1999_2018.csv\",\n",
    "    out / \"nhanes_mort_demo_soc_1999_2018.parquet\",  # legacy\n",
    "    out / \"nhanes_mort_demo_soc_1999_2018.csv\",      # legacy\n",
    "]\n",
    "\n",
    "# Show which of the candidates exist, with timestamps and sizes\n",
    "print(\"Looking in:\", out)\n",
    "found_any = False\n",
    "for p in candidates:\n",
    "    if p.exists():\n",
    "        ts = datetime.fromtimestamp(p.stat().st_mtime).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        print(f\"  ✓ {p.name:40s} | modified: {ts} | size: {p.stat().st_size:,} bytes\")\n",
    "        found_any = True\n",
    "    else:\n",
    "        print(f\"  - {p.name:40s} (missing)\")\n",
    "\n",
    "# Load the first existing file (prefer Parquet)\n",
    "df = None\n",
    "for p in candidates:\n",
    "    if p.exists():\n",
    "        df = pd.read_parquet(p) if p.suffix == \".parquet\" else pd.read_csv(p)\n",
    "        print(\"\\nLoaded:\", p)\n",
    "        break\n",
    "\n",
    "if df is None:\n",
    "    print(\"\\n⚠️ No saved table found in expected locations. \"\n",
    "          \"Re-run the merge/save cell in 00_demo_mort_sdoh.ipynb.\")\n",
    "else:\n",
    "    print(\"Shape:\", df.shape)\n",
    "    print(\"Cols (first 10):\", df.columns[:10].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "245bb8a9-44b2-4e51-8aff-573fb33ad195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Keeping (newest per extension):\n",
      "  ✓ /Users/dengshuyue/Desktop/SDOH/analysis/data/SODH_diet_mort.pkl  |  2025-07-18 10:39:20  |  27 MB\n",
      "  ✓ /Users/dengshuyue/Desktop/SDOH/analysis/data/SODH_diet_mort_depr2.csv  |  2025-09-08 16:16:55  |  21 MB\n",
      "  ↳ total size kept: 48 MB\n",
      "\n",
      "🗑️ Candidates to delete (older versions):\n",
      "  - /Users/dengshuyue/Desktop/SDOH/analysis/data/SODH_diet_mort6.csv  |  2025-08-10 11:39:07  |  14 MB\n",
      "  ↳ total size to delete: 14 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "ROOT = Path(\"/Users/dengshuyue/Desktop/SDOH/analysis\")\n",
    "OUT  = ROOT / \"analysis\" / \"output\"\n",
    "DATA = ROOT / \"data\"\n",
    "\n",
    "patterns = {\n",
    "    OUT:  [\"mort_with_demo_plus_soc.*\",\n",
    "           \"nhanes_mort_demo_soc_*.parquet\",\n",
    "           \"nhanes_mort_demo_soc_*.csv\"],\n",
    "    DATA: [\"SODH_diet_mort*\"],\n",
    "}\n",
    "\n",
    "def keep_newest_per_ext(paths):\n",
    "    by_ext = {}\n",
    "    for p in paths:\n",
    "        by_ext.setdefault(p.suffix.lower(), []).append(p)\n",
    "    keep = set()\n",
    "    for ext, files in by_ext.items():\n",
    "        keep.add(max(files, key=lambda x: x.stat().st_mtime))\n",
    "    return keep\n",
    "\n",
    "def fmt_size(num_bytes: int) -> str:\n",
    "    for unit in (\"B\",\"KB\",\"MB\",\"GB\",\"TB\"):\n",
    "        if num_bytes < 1024 or unit == \"TB\":\n",
    "            return f\"{num_bytes:,.0f} {unit}\"\n",
    "        num_bytes /= 1024\n",
    "\n",
    "# gather matches\n",
    "all_matches = []\n",
    "for base, globs in patterns.items():\n",
    "    for pat in globs:\n",
    "        all_matches.extend(sorted(base.glob(pat)))\n",
    "\n",
    "# decide which to keep/delete\n",
    "TO_KEEP = keep_newest_per_ext(all_matches)\n",
    "TO_DELETE = [p for p in all_matches if p not in TO_KEEP]\n",
    "\n",
    "print(\"✅ Keeping (newest per extension):\")\n",
    "keep_total = 0\n",
    "for p in sorted(TO_KEEP):\n",
    "    stat = p.stat()\n",
    "    ts = datetime.fromtimestamp(stat.st_mtime).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    sz = fmt_size(stat.st_size); keep_total += stat.st_size\n",
    "    print(f\"  ✓ {p}  |  {ts}  |  {sz}\")\n",
    "print(f\"  ↳ total size kept: {fmt_size(keep_total)}\")\n",
    "\n",
    "print(\"\\n🗑️ Candidates to delete (older versions):\")\n",
    "del_total = 0\n",
    "for p in sorted(TO_DELETE):\n",
    "    stat = p.stat()\n",
    "    ts = datetime.fromtimestamp(stat.st_mtime).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    sz = fmt_size(stat.st_size); del_total += stat.st_size\n",
    "    print(f\"  - {p}  |  {ts}  |  {sz}\")\n",
    "print(f\"  ↳ total size to delete: {fmt_size(del_total)}\")\n",
    "\n",
    "(len(TO_KEEP), len(TO_DELETE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fe424ecf-2cda-43aa-a185-ffa5b259ee5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nothing to move.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "\n",
    "ROOT = Path(\"/Users/dengshuyue/Desktop/SDOH/analysis\")\n",
    "DATA = ROOT / \"data\"\n",
    "OLD  = DATA / \"old\"\n",
    "OLD.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1) Find the SODH diet CSVs under data/\n",
    "cands = sorted(DATA.glob(\"SODH_diet_mort*.csv\"))\n",
    "\n",
    "# 2) Exclude the two you want to keep\n",
    "exclude_stems = {\"SODH_diet_mort_depr2\", \"SODH_diet_mort6\"}\n",
    "to_move = [p for p in cands if p.stem not in exclude_stems]\n",
    "\n",
    "# --- Preview ---\n",
    "def fmt_size(n):\n",
    "    for u in (\"B\",\"KB\",\"MB\",\"GB\",\"TB\"):\n",
    "        if n < 1024: return f\"{n:,.0f} {u}\"\n",
    "        n /= 1024\n",
    "\n",
    "if not to_move:\n",
    "    print(\"Nothing to move.\")\n",
    "else:\n",
    "    total = 0\n",
    "    print(\"Will move to /data/old:\")\n",
    "    for p in to_move:\n",
    "        st = p.stat()\n",
    "        total += st.st_size\n",
    "        ts = datetime.fromtimestamp(st.st_mtime).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        print(f\"  • {p.name}  |  {fmt_size(st.st_size)}  |  {ts}\")\n",
    "    print(f\"Total: {fmt_size(total)}\")\n",
    "\n",
    "# --- Move (flip to True to execute) ---\n",
    "CONFIRM_MOVE = False  # <-- set True to actually move\n",
    "\n",
    "if CONFIRM_MOVE and to_move:\n",
    "    for p in to_move:\n",
    "        target = OLD / p.name\n",
    "        if target.exists():  # avoid overwrite if a same-named file already there\n",
    "            stamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "            target = OLD / f\"{p.stem}_{stamp}{p.suffix}\"\n",
    "        shutil.move(str(p), str(target))\n",
    "        print(f\"📦 moved: {p.name} -> {target.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "70e5c9d8-f638-4366-90ee-3656a92a0903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Now in /data/old:\n",
      " - SODH_diet_mort.csv\n",
      " - SODH_diet_mort2.csv\n",
      " - SODH_diet_mort3.csv\n",
      " - SODH_diet_mort4.csv\n",
      " - SODH_diet_mort5.csv\n",
      " - SODH_diet_mort_depr.csv\n"
     ]
    }
   ],
   "source": [
    "# Recompute to_move safely\n",
    "cands = sorted(DATA.glob(\"SODH_diet_mort*.csv\"))\n",
    "exclude_stems = {\"SODH_diet_mort_depr2\", \"SODH_diet_mort6\"}\n",
    "to_move = [p for p in cands if p.stem not in exclude_stems]\n",
    "\n",
    "# Execute move\n",
    "for p in to_move:\n",
    "    target = OLD / p.name\n",
    "    if target.exists():  # avoid overwrite\n",
    "        stamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        target = OLD / f\"{p.stem}_{stamp}{p.suffix}\"\n",
    "    shutil.move(str(p), str(target))\n",
    "    print(f\"📦 moved: {p.name} -> {target}\")\n",
    "\n",
    "# Quick verify\n",
    "print(\"\\nNow in /data/old:\")\n",
    "for p in sorted(OLD.glob(\"SODH_diet_mort*.csv\")):\n",
    "    print(\" -\", p.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000354ec-72a5-4cc2-af26-0b39f60ca7f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eafba0-9b2a-4717-8567-39b39106ecd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0586e5-1589-4a78-b8a4-831ef52a5134",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb01e1e-0a5c-4d06-a00f-9a826a8b8560",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477948b0-0cd8-410d-9ece-8b0353e78ca2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6028ba-d85d-4795-ad13-5252b8432f36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110c3011-3051-435e-9318-9e23a7cc943d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
